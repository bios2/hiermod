[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course contents",
    "section": "",
    "text": "Daily schedule\nLink to Shared notes document"
  },
  {
    "objectID": "index.html#day-1-introduction-data-simulation-discrete-models",
    "href": "index.html#day-1-introduction-data-simulation-discrete-models",
    "title": "Course contents",
    "section": "Day 1: Introduction, data simulation, discrete models",
    "text": "Day 1: Introduction, data simulation, discrete models\nCourse Syllabus\n\n\n\nSlides\n\nIntroduction to the course and location\nProbability Distributions\n\n\n\n\nExercises – data simulation & discrete models\n\nCatch a wild Distribution\n\n\n\n\n\n\n\n\n\n\nMonday night Stan installation session!\n\n\n\nTo use Stan in R, the rstan R package is needed. To install it, follow the steps in this vignette. Make sure you can run the example in the Verifying Installation\nIf you have not yet managed to install Stan and rstan, Andrew and Guillaume will be available on Monday evening. From Tuesday onwards we won’t have time to pause to fix installation issues!"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#basic-regression-model",
    "href": "slides/04_Linear_model/index.html#basic-regression-model",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\n\nHierarchical models are a generalized version of the classic regression models you have seen in your undergraduate courses.\n\n\n\nIn its simplest form, a regression model is usually presented as\n\n\n\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n\\]\n\n\n\nIt is known as a simple linear model, where :\n\n\n\n\n\\(y_i\\) is the value of a response variable for observation \\(i\\)\n\n\n\n\n\\(x_i\\) is the value of an explanatory variable for observation \\(i\\)\n\n\n\n\n\\(\\beta_0\\) is the model intercept\n\n\n\n\n\\(\\beta_1\\) is the model slope\n\n\n\n\n\\(\\varepsilon\\) is the error term"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#basic-regression-model-1",
    "href": "slides/04_Linear_model/index.html#basic-regression-model-1",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model\nThe cool thing about the simple linear model is that it can be studied visually quite easily.\n\nFor example if we are interested in knowing how a newly discovered plant species (Bidonia exemplaris) reacts to humidity, we can relate the biomass of B. exemplaris sampled at 100 sites with the soil humidity content and readily visual the data and the trend."
  },
  {
    "objectID": "slides/04_Linear_model/index.html#basic-regression-model-2",
    "href": "slides/04_Linear_model/index.html#basic-regression-model-2",
    "title": "Linear models",
    "section": "Basic regression model",
    "text": "Basic regression model"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#regression-parameters",
    "href": "slides/04_Linear_model/index.html#regression-parameters",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nGenerally, the slope and the intercept are the regression parameters we focus on when studying the simple linear model, but there is another one that is very important to consider, especially for this course.\n\nAny ideas which one it is ?"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#regression-parameters-1",
    "href": "slides/04_Linear_model/index.html#regression-parameters-1",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\n\nIf we go back to the mathematical description of the model\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n\\]\n\n\n\nwe can see that in the simple linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\n\n\n\n\\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]\n\n\nwhere \\(\\sigma^2\\) is an estimated variance.\n\n\n\n\nIn words, it means that the error in a simple linear regression follows a Gaussian distribution with a variance that is estimated.\n\n\n\n\nFor most of the course, we will play with the variance parameter \\(\\sigma^2\\) in a bunch of different ways.\n\n\n\n\nBut before we do this, we need to understand a bit more about how this parameter influence the model."
  },
  {
    "objectID": "slides/04_Linear_model/index.html#regression-parameters-2",
    "href": "slides/04_Linear_model/index.html#regression-parameters-2",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nA first way to do this is to think about the simple linear regression in a slightly different way. Specifically, based on what we learned in the previous slide the simple linear regression can be rewritten as \\[\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{i}, \\sigma^2)\n\\]\n\nAs we will see later in this course, this writting style will become particularly useful."
  },
  {
    "objectID": "slides/04_Linear_model/index.html#regression-parameters-3",
    "href": "slides/04_Linear_model/index.html#regression-parameters-3",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\nIn essence, \\(\\sigma^2\\) tells us about what the model could not account for.\n\n\n\nFor example, let’s compare the biomass of Bidonia exemplaris with that of Ilovea chicktighii, another species (a carnivorous plant)"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#regression-parameters-4",
    "href": "slides/04_Linear_model/index.html#regression-parameters-4",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\nBy regressing humidity on the biomass of both plants, we can obtain the estimated parameters for each regression (which are all available using summary.lm)\n\n\n# Regression model\nregBexemplaris &lt;- lm(b.exemplaris ~ humidity)\nregIchicktighii &lt;- lm(i.chicktighii ~ humidity)\n\n# Summary\nsummaryRegBexemplaris &lt;- summary(regBexemplaris)\nsummaryRegIchicktighii &lt;- summary(regIchicktighii)"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#regression-parameters-5",
    "href": "slides/04_Linear_model/index.html#regression-parameters-5",
    "title": "Linear models",
    "section": "Regression parameters",
    "text": "Regression parameters\nVariance of the model (\\(\\sigma^2\\))\n\n\nFor Bidonia exemplaris\n\n# Estimated coefficients\nsummaryRegBexemplaris$coefficients[,1:2]\n\n            Estimate Std. Error\n(Intercept) 4.015999 0.05235200\nhumidity    1.313897 0.08845811\n\n# Estimated variance\nsummaryRegBexemplaris$sigma\n\n[1] 0.5232624\n\n\n\nFor Ilovea chicktighii\n\n# Estimated coefficients\nsummaryRegIchicktighii$coefficients[,1:2]\n\n            Estimate  Std. Error\n(Intercept) 3.991785 0.008928294\nhumidity    1.271250 0.015085956\n\n# Estimated variance\nsummaryRegIchicktighii$sigma\n\n[1] 0.089239"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#limits-of-the-simple-linear-regression",
    "href": "slides/04_Linear_model/index.html#limits-of-the-simple-linear-regression",
    "title": "Linear models",
    "section": "Limits of the simple linear regression",
    "text": "Limits of the simple linear regression\nThere are two major pitfalls of the simple linear model for problems in the life sciences\n\n\nOne explanatory is almost never enough to approach biological questions nowadays.\n\n\n\n\nThe simple linear model assumes that the error follows a Gaussian distribution."
  },
  {
    "objectID": "slides/04_Linear_model/index.html#multiple-linear-regression",
    "href": "slides/04_Linear_model/index.html#multiple-linear-regression",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nSimple linear regression can be extended to account for multiple explanatory variables to study more complexe problems. This type of regression model is known as a multiple linear regression.\n\n\n\nMathematically, a multiple linear regression can be defined as\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\n\n\n\n\nTheoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression. However, technically, matrix algebra is quite practical to use in this context and especially for this course.\n\n\n\n\nIn this respect, let’s take a bit of time to get acquinted with different basic (and maybe not so basic!) knowledge of matrix algebra."
  },
  {
    "objectID": "slides/04_Linear_model/index.html#a-general-way-to-write-matrices",
    "href": "slides/04_Linear_model/index.html#a-general-way-to-write-matrices",
    "title": "Linear models",
    "section": "A general way to write matrices",
    "text": "A general way to write matrices\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n                A_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n                A_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n        \\end{bmatrix}\n\\] \\[A = \\left[a_{ij}\\right]=\\left[a_{ij}\\right]_{m\\times n}\\]"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#the-transpose-of-a-matrix",
    "href": "slides/04_Linear_model/index.html#the-transpose-of-a-matrix",
    "title": "Linear models",
    "section": "The transpose of a matrix",
    "text": "The transpose of a matrix\n\n\n\n\\[A = \\begin{bmatrix}\n          5 & -6 & 4 & -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B = \\begin{bmatrix}\n          -8\\\\\n          9\\\\\n          -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C = \\begin{bmatrix}\n          -4 & 1\\\\\n          2 & -5\\\\\n        \\end{bmatrix}\n\\]\n\n\\[A^t=\\begin{bmatrix}\n          5\\\\\n          -6\\\\\n          4\\\\\n          -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B^t =\\begin{bmatrix}\n          -8 & 9 & -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C^t =\\begin{bmatrix}\n          -4 & 2\\\\\n          1 & -5\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#the-transpose-of-a-matrix-1",
    "href": "slides/04_Linear_model/index.html#the-transpose-of-a-matrix-1",
    "title": "Linear models",
    "section": "The transpose of a matrix",
    "text": "The transpose of a matrix\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    1   -2\n\nt(A)\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    5   -2"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#multiplying-a-matrix-by-a-scalar",
    "href": "slides/04_Linear_model/index.html#multiplying-a-matrix-by-a-scalar",
    "title": "Linear models",
    "section": "Multiplying a matrix by a scalar",
    "text": "Multiplying a matrix by a scalar\n\\[\\mathbf{B} = c\\mathbf{A}\\] \\[B_{ij} = cA_{ij}\\]\n\n\\[\n            0.3 \\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} =  \n            \\begin{bmatrix}\n                0.9 & 1.5\\\\\n                0.3 & -0.6\\\\\n            \\end{bmatrix}\n\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nc &lt;- 0.3\nc*A \n\n     [,1] [,2]\n[1,]  0.9  1.5\n[2,]  0.3 -0.6"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#matrix-multiplications-not-divisions",
    "href": "slides/04_Linear_model/index.html#matrix-multiplications-not-divisions",
    "title": "Linear models",
    "section": "Matrix multiplications (not divisions!)",
    "text": "Matrix multiplications (not divisions!)\n\\[\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}\\]\n\\[C_{ik} = \\sum^{n}_{j=1}A_{ij}B_{jk}\\]\nRules\nAssociative: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nDistributive: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B}+\\mathbf{A}\\mathbf{C}\\)\nNot commutative: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#inner-product",
    "href": "slides/04_Linear_model/index.html#inner-product",
    "title": "Linear models",
    "section": "Inner product",
    "text": "Inner product\n\\[(\\mathbf{Ax})_i=\\sum_{j=1}^{n}A_{ij}x_j\\]\n\n\\[\n  \\begin{bmatrix}\n    3 & 5\\\\\n    1 & -2\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2\\\\ 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    (3, 5) \\cdot (2, 5)\\\\\n    (1, -2) \\cdot (2, 5) \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    3 \\times 2 + 5 \\times 5\\\\\n    1 \\times 2 -2 \\times 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    31\\\\\n    -8\\\\\n  \\end{bmatrix}\n\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nx &lt;- matrix(c(2,5), nrow = 2, ncol = 1)\nA %*% x\n\n     [,1]\n[1,]   31\n[2,]   -8"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#identity-matrix",
    "href": "slides/04_Linear_model/index.html#identity-matrix",
    "title": "Linear models",
    "section": "Identity matrix",
    "text": "Identity matrix\nThe identity matrix is a square matrix where all values of its diagonal are 0 except the diagonal values which are all 1s.\n\n\\[\n\\mathbf{I}=\\begin{bmatrix}\n                1 & 0 & 0\\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\\\\\n        \\end{bmatrix}\n\\]\n\n\n\nThe identity matrix is important because\n\\[\\mathbf{A} \\cdot \\mathbf{I}_n = \\mathbf{A}\\] or\n\\[\\mathbf{I}_m \\cdot \\mathbf{A} = \\mathbf{A}\\]"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#diagonal-matrix",
    "href": "slides/04_Linear_model/index.html#diagonal-matrix",
    "title": "Linear models",
    "section": "Diagonal matrix",
    "text": "Diagonal matrix\nThe diagonal matrix is a square matrix where all values of its diagonal are 0 except the ones on the diagonal.\n\n\\[D=\n      \\begin{bmatrix}\n        d_1 & 0 & \\dots & 0\\\\\n        0 & d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots & d_n\\\\\n\\end{bmatrix}\\]\nAn example\n\n\\[\n\\begin{bmatrix}\n                -1 & 0 & 0\\\\\n                0 & 0 & 0\\\\\n                0 & 0 & 6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#multiple-linear-regression-1",
    "href": "slides/04_Linear_model/index.html#multiple-linear-regression-1",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nIn this course, we will rely heavily on multiple linear regression and expand on it by studying how some of the parameters (the \\(\\beta\\)s) can depend on other other data and parameters.\n\n\nAs we saw earlier, a classic way to write multiple linear regression is\n\\[\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n\\]\n\n\nHowever, we can rewrite this using matrix notation has\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#multiple-linear-regression-2",
    "href": "slides/04_Linear_model/index.html#multiple-linear-regression-2",
    "title": "Linear models",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nMatrix notation\n\\[\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\]\nUsing the matrix notation, we assume that\n\n\n\n\\(\\mathbf{y}\\) is a vector of length \\(n\\) (samples)\n\n\n\n\n\n\n\\(\\mathbf{X}\\) is a matrix with \\(n\\) samples (rows) and representing \\(p\\) explanatory variables (columns)\n\n\n\n\n\n\n\\(\\boldsymbol{\\beta}\\) is a vector of \\(p\\) regression coefficients\n\n\n\n\n\n\n\\(\\boldsymbol{\\varepsilon}\\) is a vector of residuals of length \\(n\\)"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#error-in-linear-models",
    "href": "slides/04_Linear_model/index.html#error-in-linear-models",
    "title": "Linear models",
    "section": "Error in linear models",
    "text": "Error in linear models\nAs previously mentioned, in (simple and multiple!) linear regression the error term (\\(\\varepsilon\\)) has actually a very precise definition:\n\\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\] where \\(\\sigma^2\\) is an estimated variance\n\nwhich means that the error in a linear regression follows a Gaussian distribution with an estimated variance.\n\n\nNote The model can be written using matrix notation as\n\\[\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)\\] where \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#generalized-linear-models",
    "href": "slides/04_Linear_model/index.html#generalized-linear-models",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\n\nIf for some reason we do not want our model to have a Gaussian error, generalized linear models (GLMs) have been proposed. In essence, GLMs use link functions to adapt models for them to be used on non-Gaussian data.\n\n\nMathematically, the generic way to write generalized linear model is\n\\[\n\\widehat{y}_i = g^{-1}(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip})\n\\]\n\n\nor in matrix notation\n\\[\n\\widehat{\\mathbf{y}} = g^{-1}(\\mathbf{X}\\boldsymbol{\\beta})\n\\]\nwhere \\(g\\) is the link function and \\(g^{-1}\\) the inverse link function."
  },
  {
    "objectID": "slides/04_Linear_model/index.html#generalized-linear-models-1",
    "href": "slides/04_Linear_model/index.html#generalized-linear-models-1",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\n\nThere are many types of link functions and they are usually directly associated to the underlying data the analysis is carried out on.\n\n\n\nArguably the most common link function in ecology is\n\n\n\nlogit link function\n\nIt is commonly used for modelling binary (0-1) data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\frac{\\widehat{\\mathbf{y}}}{1 - \\widehat{\\mathbf{y}}}\\right)\n\\]\n\n\n\n\nThe inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\frac{\\exp(\\mathbf{X}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}\\boldsymbol{\\beta})} = \\frac{1}{1 + \\exp(-\\mathbf{X}\\boldsymbol{\\beta})}\n\\]"
  },
  {
    "objectID": "slides/04_Linear_model/index.html#generalized-linear-models-2",
    "href": "slides/04_Linear_model/index.html#generalized-linear-models-2",
    "title": "Linear models",
    "section": "Generalized linear models",
    "text": "Generalized linear models\nLink functions\nAnother commonly used link function is\n\nlog link function\nIt is commonly used for modelling count data.\n\\[\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\widehat{\\mathbf{y}}\\right)\n\\]\n\n\nThe inverse logit link function is\n\\[\n\\widehat{\\mathbf{y}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta})\n\\]"
  },
  {
    "objectID": "slides/template/index.html#un-beau-titre",
    "href": "slides/template/index.html#un-beau-titre",
    "title": "Template for presentations",
    "section": "Un beau titre",
    "text": "Un beau titre\nthis is the first slide"
  },
  {
    "objectID": "slides/template/index.html#second",
    "href": "slides/template/index.html#second",
    "title": "Template for presentations",
    "section": "second",
    "text": "second\n\nTest your model"
  },
  {
    "objectID": "slides/template/index.html#section",
    "href": "slides/template/index.html#section",
    "title": "Template for presentations",
    "section": "",
    "text": "img:\n\nit is a landscape"
  },
  {
    "objectID": "slides/template/index.html#section-1",
    "href": "slides/template/index.html#section-1",
    "title": "Template for presentations",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/template/index.html#section-2",
    "href": "slides/template/index.html#section-2",
    "title": "Template for presentations",
    "section": "",
    "text": "check with simulations"
  },
  {
    "objectID": "slides/template/index.html#choose-parameters",
    "href": "slides/template/index.html#choose-parameters",
    "title": "Template for presentations",
    "section": "choose parameters",
    "text": "choose parameters\nyintercept &lt;- 4\nslope &lt;- 1.3\nobs_error &lt;- .5"
  },
  {
    "objectID": "slides/template/index.html#make-up-an-x-variable",
    "href": "slides/template/index.html#make-up-an-x-variable",
    "title": "Template for presentations",
    "section": "make up an X variable",
    "text": "make up an X variable\nyintercept &lt;- 4\nslope &lt;- 1.3\nobs_error &lt;- .5\nx &lt;- runif(200, min = -1, max = 1)"
  },
  {
    "objectID": "slides/template/index.html#calculate-the-average",
    "href": "slides/template/index.html#calculate-the-average",
    "title": "Template for presentations",
    "section": "calculate the average",
    "text": "calculate the average\nyintercept &lt;- 4\nslope &lt;- 1.3\nobs_error &lt;- .5\nx &lt;- runif(200, min = -1, max = 1)\ny_mean &lt;- yintercept + slope * x"
  },
  {
    "objectID": "slides/template/index.html#simulate-some-observations",
    "href": "slides/template/index.html#simulate-some-observations",
    "title": "Template for presentations",
    "section": "simulate some observations",
    "text": "simulate some observations\n\nyintercept &lt;- 4\nslope &lt;- 1.3\nobs_error &lt;- .5\nx &lt;- runif(200, min = -1, max = 1)\ny_mean &lt;- yintercept + slope * x\ny_obs &lt;- rnorm(200, mean = y_mean, sd = obs_error)"
  },
  {
    "objectID": "slides/template/index.html#finally-visualize",
    "href": "slides/template/index.html#finally-visualize",
    "title": "Template for presentations",
    "section": "finally, visualize",
    "text": "finally, visualize\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/template/index.html#here-it-is-all-on-one-slide",
    "href": "slides/template/index.html#here-it-is-all-on-one-slide",
    "title": "Template for presentations",
    "section": "here it is all on one slide",
    "text": "here it is all on one slide\n\n\nyintercept &lt;- 4\nslope &lt;- 1.3\nobs_error &lt;- .5\nx &lt;- runif(200, min = -1, max = 1)\ny_mean &lt;- yintercept + slope * x\ny_obs &lt;- rnorm(200, mean = y_mean, sd = obs_error)\nplot(y_obs, x)"
  },
  {
    "objectID": "slides/template/index.html#or-we-can-present-the-code-and-results-separately",
    "href": "slides/template/index.html#or-we-can-present-the-code-and-results-separately",
    "title": "Template for presentations",
    "section": "Or we can present the code and results separately",
    "text": "Or we can present the code and results separately\n\nThe codeThe figure\n\n\n\nyintercept &lt;- 4\nslope &lt;- 1.3\nobs_error &lt;- .5\nx &lt;- runif(200, min = -1, max = 1)\ny_mean &lt;- yintercept + slope * x\ny_obs &lt;- rnorm(200, mean = y_mean, sd = obs_error)\n\n\n\n\nplot(y_obs ~ x)"
  },
  {
    "objectID": "slides/template/index.html#another-equation",
    "href": "slides/template/index.html#another-equation",
    "title": "Template for presentations",
    "section": "another equation",
    "text": "another equation\n\\[\n2 + 4 = 6\n\\]"
  },
  {
    "objectID": "slides/template/index.html#the-equation",
    "href": "slides/template/index.html#the-equation",
    "title": "Template for presentations",
    "section": "The equation",
    "text": "The equation\n\\[\n\\begin{align}\ny  &\\sim \\text{N}(\\mu, \\sigma_{obs}) \\\\\n\\mu &= a + bx \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/template/index.html#the-model",
    "href": "slides/template/index.html#the-model",
    "title": "Template for presentations",
    "section": "The model",
    "text": "The model\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#declare-the-data",
    "href": "slides/template/index.html#declare-the-data",
    "title": "Template for presentations",
    "section": "Declare the data",
    "text": "Declare the data\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#state-parameters",
    "href": "slides/template/index.html#state-parameters",
    "title": "Template for presentations",
    "section": "State parameters",
    "text": "State parameters\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/template/index.html#write-the-likelihood-and-priors",
    "href": "slides/template/index.html#write-the-likelihood-and-priors",
    "title": "Template for presentations",
    "section": "Write the likelihood and priors",
    "text": "Write the likelihood and priors\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(mu, sigma);\n  mu ~ normal(0, 1);\n  sigma ~ exponential(1);\n}"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#frequentist",
    "href": "slides/02_Bayesian/index.html#frequentist",
    "title": "Bayesian modelling",
    "section": "Frequentist",
    "text": "Frequentist\n\n\nIn introductory statistics course, it is common to rely on the frequentist paradigm when inferring results from data.\n\n\n\n\nFrequentists want to find the best model parameter(s) for the data at hand.\n\n\n\n\n\\[\\text{Likelihood}\\hspace{1.5cm}P(\\text{Data}|\\text{Model})\\]\n\n\n\n\nThey are interested in maximizing the likelihood\nThey need data\n\n\n\nEstimating model parameters\n\n\nMinimizing the sums of squares\nSimulated annealing\nNelder-Mead Simplex\n…"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#bayesian",
    "href": "slides/02_Bayesian/index.html#bayesian",
    "title": "Bayesian modelling",
    "section": "Bayesian",
    "text": "Bayesian\n\n\nBayesians want to find how good the model parameter(s) are given some data\n\n\n\n\n\\[\\text{Posterior}\\hspace{1.5cm}P(\\text{Model}|\\text{Data})\\]\n\n\n\n\nThey are interested in the posterior distribution\n\n\n\n\nThey need data and prior information\n\n\n\n\nThe general framework used in Bayesian modelling is\n\n\n\\[\\underbrace{P(\\text{Model}|\\text{Data})}_\\text{Posterior}\\propto \\underbrace{P(\\text{Data}|\\text{Model})}_\\text{Likelihood}\\underbrace{P(\\text{Model})}_\\text{Prior}\\]\n\n\n\nEstimating model parameters\n\n\nMarkov Chain Monte Carlo\nHamiltonian Monte Carlo\n…"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#our-way-of-thinking-is-bayesian",
    "href": "slides/02_Bayesian/index.html#our-way-of-thinking-is-bayesian",
    "title": "Bayesian modelling",
    "section": "Our way of thinking is Bayesian",
    "text": "Our way of thinking is Bayesian"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#a-few-words-about-the-prior",
    "href": "slides/02_Bayesian/index.html#a-few-words-about-the-prior",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\n\nDefinition of prior probability\n\n\n\nThe prior probability informes us about the probability of the model being true before considering any available data\n\n\n\nTypes of priors\n\n\n“Uninformative”\n\nThese priors are meant to bring very little information about the model\n\n\n\nInformative\n\nThese priors bring information about the model that is available"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#a-few-words-about-the-prior-1",
    "href": "slides/02_Bayesian/index.html#a-few-words-about-the-prior-1",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\n“Uninformative” priors\n\n\nExample If we have no idea of how elevation influence sugar maple\n\n\n\nGaussian distribution\n\n\n\n\\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\]\n\n\n\n\\(\\mu = 0\\)\n\\(\\sigma = \\text{Large say 100}\\)"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#a-few-words-about-the-prior-2",
    "href": "slides/02_Bayesian/index.html#a-few-words-about-the-prior-2",
    "title": "Bayesian modelling",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nInformative priors\n\n\nExample If we know that\n\nThere are less sugar maples the higher we go\nThe influence of elevation on sugar maple cannot be more than two folds\n\nUniform distribution\n\n\n\\[f(x)=\\left\\{\n  \\begin{array}{cl}\n    \\frac{1}{b-a} & \\text{for } x\\in [a,b]\\\\\n    0 &\\text{otherwise}\\\\\n  \\end{array}\n\\right.\\]\n\n\\(a &gt; -2\\)\n\\(b &lt; 0\\)"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#estimating-bayesian-model",
    "href": "slides/02_Bayesian/index.html#estimating-bayesian-model",
    "title": "Bayesian modelling",
    "section": "Estimating Bayesian model",
    "text": "Estimating Bayesian model\n\n\nAs I hinted earlier, there are a number of ways to estimate the parameters of a Bayesian model. A common way to estimate Bayesian models is to rely on Markov Chain Monte Carlo (MCMC) or variants of it, including Hamiltonian Monte Carlo (HMC), which is used in Stan\n\n\n\nTypical reasons to favour MCMC\n\n\n\n\nIt is flexible\nIt can be applied to complex models such as models with multiple levels of hierarchy\n\n\n\n\nWhy should we learn about MCMC ?\n\n\n\nThe goal of this course is not to learn the intricacies of MCMC or HMC, but since we will play a lot with Stan, it is important to learn at least conceptually how MCMC and HMC work."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc",
    "href": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc",
    "title": "Bayesian modelling",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nHistorically, the developments of MCMC have been intimately linked with the arrival of computers. Specifically, the first developments and applications of MCMC were made during the Los Alamos projects.\nTo explain what is an MCMC, let’s imagine that we are interested in understanding how the mallard (Anas platyrhynchos) grows from hatchling to adult."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-1",
    "href": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-1",
    "title": "Bayesian modelling",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nA simplistic statistical example\n\n\nLet’s say that we are interested in modelling how male Mallard weight changes as they grow from hatching to adult. Here are some data obtained from a local Mallard duck farm."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-2",
    "href": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-2",
    "title": "Bayesian modelling",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nA simplistic statistical example\n\n\nA growth model can be constructed using a simple linear regression\n\\[y = \\beta_0 + \\beta x + \\varepsilon\\] From this model we can infer that the average weight of a Mallard duck when it hatches is 107 grams (intercept), and the average daily growth of the Mallard is 13.5 grams (slope)."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-3",
    "href": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-3",
    "title": "Bayesian modelling",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nA simplistic statistical example\n\nEstimating the intercept and slope parameters of the simple linear regression can be done with an MCMC. This amount to sampling\n\n\n\\(\\beta_0\\) as\n\n\n\n\\[\\beta_0 \\sim \\mathcal{D}(\\text{mean}, \\text{variance}, \\text{skewness}, \\text{kurtosis}, \\dots)\\]\n\n\n\n\nand \\(\\beta\\) as\n\n\n\n\n\\[\\beta \\sim \\mathcal{D}(\\text{mean}, \\text{variance}, \\text{skewness}, \\text{kurtosis}, \\dots)\\]\n\n\nIn doing so, we are not focusing on finding the ‘best’ parameter values. Rather, we are focused on finding the distribution of best parameter values."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-4",
    "href": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-4",
    "title": "Bayesian modelling",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nWhen using an MCMC, we are interested in sampling distributions to estimate the parameters of the model.\n\n\nTechnically, MCMC and HMC rely on different approaches to assess the structure of the distributions."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-5",
    "href": "slides/02_Bayesian/index.html#markov-chain-monte-carlo-mcmc-5",
    "title": "Bayesian modelling",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nMCMC relies on using many random samples to assess the structure of the distribution."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#hamiltonian-monte-carlo",
    "href": "slides/02_Bayesian/index.html#hamiltonian-monte-carlo",
    "title": "Bayesian modelling",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\nHamiltonial Monte Carlo relies on Hamiltonian dynamics to assess the structure of the distribution."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#sampling-the-parameters",
    "href": "slides/02_Bayesian/index.html#sampling-the-parameters",
    "title": "Bayesian modelling",
    "section": "Sampling the parameters",
    "text": "Sampling the parameters\n\n\nIn MCMC and HMC, a lot of iterations need to be carried out to assess the distribution of parameters. But how many is enough ?\n\n\n\n\nHere is a rough procedure to follow:\n\nPerform a pilot run with a reduced number of iterations (e.g. 10) and measure the time it takes\nDecide on a number of steps to use to obtain a result in a reasonable amount of time\nRun the algorithm again !\nStudy the chain visually"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#studying-convergence",
    "href": "slides/02_Bayesian/index.html#studying-convergence",
    "title": "Bayesian modelling",
    "section": "Studying convergence",
    "text": "Studying convergence"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#studying-convergence-1",
    "href": "slides/02_Bayesian/index.html#studying-convergence-1",
    "title": "Bayesian modelling",
    "section": "Studying convergence",
    "text": "Studying convergence"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#studying-convergence-2",
    "href": "slides/02_Bayesian/index.html#studying-convergence-2",
    "title": "Bayesian modelling",
    "section": "Studying convergence",
    "text": "Studying convergence\n\nIf we ran the same MCMC as above but instead for 50000 steps, we obtain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote It is also possible to thin (record only the \\(n^{\\text{th}}\\) iteration), but it is better to keep all iterations when possible. Actually, thinning should only be carried out when there are too many iterations for you to keep them and manipulate them in the memory of your computer."
  },
  {
    "objectID": "slides/02_Bayesian/index.html#studying-convergence-3",
    "href": "slides/02_Bayesian/index.html#studying-convergence-3",
    "title": "Bayesian modelling",
    "section": "Studying convergence",
    "text": "Studying convergence\nBurn-in\nBurn-in is throwing away some iterations at the beginning of the MCMC run"
  },
  {
    "objectID": "slides/02_Bayesian/index.html#studying-convergence-4",
    "href": "slides/02_Bayesian/index.html#studying-convergence-4",
    "title": "Bayesian modelling",
    "section": "Studying convergence",
    "text": "Studying convergence\nBurn-in\nAfter burn-in, we obtain"
  },
  {
    "objectID": "slides/01_Data/index.html#illustrative-datasets",
    "href": "slides/01_Data/index.html#illustrative-datasets",
    "title": "Data used for this course",
    "section": "Illustrative datasets",
    "text": "Illustrative datasets\nTo illustrate the different models and methods we will discuss in this course, we will rely on a few data sets, which are directly available in different R packages\n\n\nmite, mite.env and mite.xy available in the vegan R package\n\n\n\n\npenguins available in the palmerpenguins R package\n\n\n\nThese datasets are practical because they are manageable in size and will allow you to see how to work out the different example presented in this course.\n\n\nLet’s look at them in more details…"
  },
  {
    "objectID": "slides/01_Data/index.html#oribatid-mite-data",
    "href": "slides/01_Data/index.html#oribatid-mite-data",
    "title": "Data used for this course",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\nAside from being very interesting, this dataset has been sampled at the Station biologique des Laurentides, so ~200 km north-west from here.\n\nSampling was carried out in June 1989 on the partially floating vegetation mat surrounding a lake, from the forest border to the free water by Daniel Borcard."
  },
  {
    "objectID": "slides/01_Data/index.html#oribatid-mite-data-1",
    "href": "slides/01_Data/index.html#oribatid-mite-data-1",
    "title": "Data used for this course",
    "section": "Oribatid mite data",
    "text": "Oribatid mite data\n\nOribatid mites are small (usually ranging in size from 0.2 to 1.4 mm) invertebrates that are part of the Arachnida class (so they have 8 legs).\n\n\n\n\n\n\n\nIn the mite data, 35 morphospecies were identified and counted across 70 samples."
  },
  {
    "objectID": "slides/01_Data/index.html#sites-coordinates",
    "href": "slides/01_Data/index.html#sites-coordinates",
    "title": "Data used for this course",
    "section": "Sites coordinates",
    "text": "Sites coordinates\nmite.xy"
  },
  {
    "objectID": "slides/01_Data/index.html#vegetation-cover",
    "href": "slides/01_Data/index.html#vegetation-cover",
    "title": "Data used for this course",
    "section": "Vegetation cover",
    "text": "Vegetation cover\nmite.env"
  },
  {
    "objectID": "slides/01_Data/index.html#microtopography-and-shrub-cover",
    "href": "slides/01_Data/index.html#microtopography-and-shrub-cover",
    "title": "Data used for this course",
    "section": "Microtopography and shrub cover",
    "text": "Microtopography and shrub cover\nmite.env"
  },
  {
    "objectID": "slides/01_Data/index.html#substrate-density-and-water-content",
    "href": "slides/01_Data/index.html#substrate-density-and-water-content",
    "title": "Data used for this course",
    "section": "Substrate density and water content",
    "text": "Substrate density and water content\nmite.env"
  },
  {
    "objectID": "slides/01_Data/index.html#getting-the-data",
    "href": "slides/01_Data/index.html#getting-the-data",
    "title": "Data used for this course",
    "section": "Getting the data",
    "text": "Getting the data\n\n\nlibrary(vegan)\ndata(mite)\ndata(mite.env)\nView(mite)\nView(mite.env)"
  },
  {
    "objectID": "slides/01_Data/index.html#palmer-penguins",
    "href": "slides/01_Data/index.html#palmer-penguins",
    "title": "Data used for this course",
    "section": "Palmer penguins",
    "text": "Palmer penguins\n\nThe Palmer Archipelago penguins. Artwork by @allison_horst\nThese data were collected from 2007 to 2009 by Dr. Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network.\n\n\n\nThe data were imported directly from the Environmental Data Initiative (EDI) Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the Palmer Station Data Policy. (Gorman, Williams, and Fraser 2014; Horst, Hill, and Gorman 2020)"
  },
  {
    "objectID": "slides/01_Data/index.html#a-handy-dataset-of-three-groups",
    "href": "slides/01_Data/index.html#a-handy-dataset-of-three-groups",
    "title": "Data used for this course",
    "section": "A handy dataset of three groups",
    "text": "A handy dataset of three groups"
  },
  {
    "objectID": "slides/01_Data/index.html#behold-simpsons-paradox",
    "href": "slides/01_Data/index.html#behold-simpsons-paradox",
    "title": "Data used for this course",
    "section": "Behold: Simpson’s Paradox!",
    "text": "Behold: Simpson’s Paradox!"
  },
  {
    "objectID": "slides/01_Data/index.html#behold-simpsons-paradox-1",
    "href": "slides/01_Data/index.html#behold-simpsons-paradox-1",
    "title": "Data used for this course",
    "section": "Behold: Simpson’s Paradox!",
    "text": "Behold: Simpson’s Paradox!"
  },
  {
    "objectID": "slides/01_Data/index.html#theres-lots-more",
    "href": "slides/01_Data/index.html#theres-lots-more",
    "title": "Data used for this course",
    "section": "There’s lots more!",
    "text": "There’s lots more!\n\n\nlibrary(palmerpenguins)\nView(penguins)\nView(penguins_raw)\n\n\nand also see the official site: https://allisonhorst.github.io/palmerpenguins/\n\n\n\n\n\n\n\n\n\nGorman, Kristen B., Tony D. Williams, and William R. Fraser. 2014. “Ecological Sexual Dimorphism and Environmental Variability Within a Community of Antarctic Penguins (Genus Pygoscelis).” PLOS ONE 9 (3): e90081. https://doi.org/10.1371/journal.pone.0090081.\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. Palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchical-models",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchical models",
    "text": "Hierarchical models\n\nWith a hierarchical model, our interest lies not in finding a single value for a parameter of interest but rather to estimate the distribution of a parameter of interest and, specifically, the variance of this distribution.\n\n\nIf we want to estimate the variance of a distribution, we need to gather some samples from this distribution.\n\n\nIn the context of a hierarchical model, the “samples” are the levels of a factor, which will be used to estimate the variance of the distribution."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#simple-hierarchical-model",
    "href": "slides/06_Simple_hierarchical_model/index.html#simple-hierarchical-model",
    "title": "‘Simple’ hierarchical models",
    "section": "“Simple” hierarchical model",
    "text": "“Simple” hierarchical model\n\nHere, we use the term “simple” in a rather loose way to discuss hierarchical models where the hierarchy is on one parameter without any constrains, whether they are spatial, temporal, phylogenetic or others.\n\n\nFuthermore, for most of this lecture, we will focus on models with a Gaussian error term to develop the underlying theory.\n\n\nWhen we will have done this, it will be reasonably straight forward to move to non-Gaussian hierarchical model."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#the",
    "href": "slides/06_Simple_hierarchical_model/index.html#the",
    "title": "‘Simple’ hierarchical models",
    "section": "The “|”",
    "text": "The “|”\n\n\nMost of you have probably already used the packages lme4, brms or glmmTMB to build hierarchical models and so you have used the | to include a hierarchy in your model.\n\n\n\n\nBut do you know what the underlying mathematical structure of the model you built look like ? Does it really answer the question you were asking ?\n\n\n\n\n\n\n\n\n\nLet’s look at different lme4 models to learn about some basic (and not so basic!) hierarchical models."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#a-bit-of-notation",
    "href": "slides/06_Simple_hierarchical_model/index.html#a-bit-of-notation",
    "title": "‘Simple’ hierarchical models",
    "section": "A bit of notation",
    "text": "A bit of notation\n\nMathematically, the basic structure of a hierarchical model is\n\n\n\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\varepsilon}\\]\n\n\nwhere\n\n\n\n\\(\\mathbf{y}\\) : Vector of response variable\n\\(\\mathbf{X}\\) : Matrix of explanatory variables on which no hierarchies are accounted for\n\\(\\mathbf{Z}\\) : Matrix of explanatory variables on which hierarchies are accounted for\n\\(\\boldsymbol{\\beta}\\) : parameter estimated without a hierarchy\n\\(\\mathbf{b}\\) : parameter estimated with a hierarchy\n\\(\\boldsymbol{\\varepsilon}\\) : a vector that follows a Gaussian distribution such that \\({\\cal N}(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#a-bit-of-notation-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#a-bit-of-notation-1",
    "title": "‘Simple’ hierarchical models",
    "section": "A bit of notation",
    "text": "A bit of notation\n\n\nBefore we get into writing math, we need to define a bit of notation in addition of the one we have used so far.\n\n\n\n\nSpecifically, when defining a hierarchy in a model, it is common to do this using at least one factor. Mathematically, we will define the different level of a factor in a model by a subscript.\nWe will use square brackets to define the sample.\n\n\n\nExample\n\n\\[\\mathbf{Z}_{f[i]}\\] This means that, within \\(\\mathbf{Z}\\), we focus on the \\(i^{\\text{th}}\\) sample within factor \\(f\\).\n\n\n\n\nNote The \\(i^{\\text{th}}\\) sample of factor \\(f\\) maybe associated to any level of factor \\(f\\)."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept",
    "text": "Hierarchy on the intercept\n\n\nlme4 notation used y ~ (1 | f) or y ~ 1 + (1 | f)\n\n\n\n\nThis model assumes there is a hierarchy solely on the intercept.\n\n\n\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f},\\sigma^2\\mathbf{I})\\]\n\n\n\n\nor\n\\[y_i = b_{{f[i]}} + \\varepsilon \\quad \\forall\\quad i = 1\\dots n\\]\n\n\n\n\nwhere\n\\[\\mathbf{b}_f \\sim \\mathcal{N}(\\mu_f, \\sigma^2_f)\\]"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercept-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept",
    "text": "Hierarchy on the intercept"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slope",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slope",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slope",
    "text": "Hierarchy on the slope\n\nlme4 notation : y ~ 1 + (x | f)\n\n\n\nThis model assumes there is a hierarchy on the parameter associated to variable x.\n\n\n\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\beta_0 + \\mathbf{z}\\mathbf{b}_{f},\\sigma^2\\mathbf{I})\\]\n\n\n\n\nor\n\\[y_i = \\beta_0 + b_{f[i]}z_i + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n\n\n\nwhere\n\\(\\mathbf{z}\\) is an explanatory variable, \\(z_i\\) the \\(i^{\\text{th}}\\) sample of \\(\\mathbf{z}\\) and\n\\[\\mathbf{b}_f \\sim \\mathcal{N}(\\mu_f, \\sigma^2_f)\\]"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slopes",
    "text": "Hierarchy on the slopes"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on intercept and slope",
    "text": "Hierarchy on intercept and slope\n\nMathematically speaking, what are the differences between having a hierarchy on the intercept and a hierarchy on the slope ? Any idea ?"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-intercept-and-slope-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on intercept and slope",
    "text": "Hierarchy on intercept and slope\nAnswer : Very little !\n\nActually, if we return to the way \\(\\mathbf{b}\\) is defined we see that in both case it is defined as\n\\[\\mathbf{b}_f \\sim \\mathcal{N}(\\mu_f, \\sigma^2_f)\\] with the sole difference is that \\(\\mathbf{b}\\) is linked to an explanatory variable when the hierarchy is on the slope, while when the hierarchy is on the intercept it is not linked to any explanatory variable.\n\n\nWell… Actually… When a hierarchy is applied on the intercept it is technically associated to a constant explanatory variable."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\n\nA common question that often gets asked is :\n\n\n“How many level is enough ?”\n\n\nThis is a simple questions that sadly does not have a simple answer."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-1",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nIn these types of models we are interested in estimating the variance parameter \\(\\sigma^2_f\\) in\n\\[\\mathbf{b}_f \\sim \\mathcal{N}(\\mu_f, \\sigma^2_f)\\] to get the best estimation of \\(\\mathbf{b}\\).\n\nSo, another way to ask this question is: “What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?”\n\n\nHowever, in the context of how we defined hierarchical models, a sample is a single level of a factor."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-2",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\n\nIs 3 enough ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.234"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-3",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-3",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nMaybe 5 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.12"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-4",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-4",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 10 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.174"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-5",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-5",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 50 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.331"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-6",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-6",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 100 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.271"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-7",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-7",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nWhat is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?\nOr 1000 ?\n\n\n\n\n\n\n\n\n\n\n\n\nTrue variance : 0.25\nEstimated variance : 0.251"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-8",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-8",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nThere is a consensus among researchers working intimately with hierarchical models that when the interest is to properly estimate the variance parameter \\(\\sigma^2\\), 5 or 6 levels is the extreme minimum.\n\n\nIn the book Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects, James S. Hodges (2016) makes this very thoughtful statement :\n\n“Treating factors with small numbers of levels as random will in the best case lead to very small and/or imprecise estimates of random effects; in the worst case it will lead to various numerical difficulties such as lack of convergence, zero variance estimates, etc.”"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-9",
    "href": "slides/06_Simple_hierarchical_model/index.html#how-many-levels-9",
    "title": "‘Simple’ hierarchical models",
    "section": "How many levels ?",
    "text": "How many levels ?\nSo, what to do if the number of level is not high enough for your comfort ?\n\nYou can still use the hierarchy in your model but focus on the mean of the levels instead of the variance.\n\n\nHow does this translate mathematically with what we have seen so far ?"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol{\\beta}_{f},\\sigma_\\mathbf{y}^2\\mathbf{I})\\] or\n\\[y_i = \\beta_{f[i]} + \\varepsilon \\quad \\forall\\quad i = 1\\dots n\\]\n\n\n\nIn this model, we assume that \\(\\boldsymbol{\\beta}_{f}\\) is distributed as\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{N}(\\mu_{f}, \\sigma^2_{f})\\]\n\n\n\n\nThis means that all samples among the levels of factor \\(f\\) are used to estimate \\(\\boldsymbol{\\beta}_{f}\\).\n\n\n\n\nBy developping our model this way, we focus on estimating the mean of groups in the hierarchy instead of only the variance."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\nLet’s take a deeper look at \\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{N}(\\mu_{f}, \\sigma^2_{f})\\]\n\nWhen we study this way of sampling \\(\\boldsymbol{\\beta}_{f}\\), although our interest is more on \\(\\mu_{f}\\), we also have to acount for the variance term \\(\\sigma^2_{f}\\).\n\n\nNote: This is essentially the same thing as a one-way analysis of variance."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-2",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-intercepts-mean-2",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slope’s mean",
    "text": "Hierarchy on the slope’s mean\n\n\nDevelopping a hierarchy on the slope’s mean translate mathematically in a very similar way as it does for the intercept.\n\n\n\n\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol{\\beta}_0+\\mathbf{X}\\boldsymbol{\\beta}_{f},\\sigma_\\mathbf{y}^2\\mathbf{I})\\] or\n\\[y_i = \\beta_0 + \\beta_{f[i]}x_i + \\varepsilon \\quad \\forall\\quad i = 1\\dots n\\]\n\n\n\n\nIn this model, we assume that \\(\\boldsymbol{\\beta}_{f}\\) is distributed as\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{N}(\\mu_{f}, \\sigma^2_{f})\\]\n\n\n\n\nThis means that all the samples among the levels of factor \\(f\\) are used to estimate \\(\\boldsymbol{\\beta}_{f}\\)\nBy developping our model this way, we focus on estimating the average slope for each group in the hierarchy instead of only the variance."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean-1",
    "href": "slides/06_Simple_hierarchical_model/index.html#hierarchy-on-the-slopes-mean-1",
    "title": "‘Simple’ hierarchical models",
    "section": "Hierarchy on the slope’s mean",
    "text": "Hierarchy on the slope’s mean"
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#tracking-the-estimated-parameters",
    "href": "slides/06_Simple_hierarchical_model/index.html#tracking-the-estimated-parameters",
    "title": "‘Simple’ hierarchical models",
    "section": "Tracking the estimated parameters",
    "text": "Tracking the estimated parameters\n\n\nAs can be seen, it is important in hierarchical models to track the different parameters that are estimated to ensure we can make proper inferences with our model.\n\n\n\n\nHowever, we need to be careful because the notation used can play tricks on us. This is especially true when using matrix notation.\n\n\n\n\nFor example, in\n\\[\\mathbf{b}_{f}\\sim \\mathcal{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})\\] the number of levels are not explicitly defined and it is not clear if \\(\\mathbf{\\Sigma}\\) includes the same variance value on the diagonal or different ones and whether the off diagonal elements are 0 or not.\n\n\n\n\nIn any case, make sure to keep track of the estimated parameters so that you can better understand the limits of the model you are building and using."
  },
  {
    "objectID": "slides/06_Simple_hierarchical_model/index.html#choosing-the-right-model",
    "href": "slides/06_Simple_hierarchical_model/index.html#choosing-the-right-model",
    "title": "‘Simple’ hierarchical models",
    "section": "Choosing the right model",
    "text": "Choosing the right model\n\nAlthough these different models are mathematically quite similar, they approach very different biological questions.\n\n\nA comparison of the different figures caricaturizing how each model works should give a good insight of what each model can do.\n\n\nIt is thus important to make sure you design your biological question well so that deciding on which model to use is reasonably straight forward."
  },
  {
    "objectID": "slides/00_Introduction/index.html#internet",
    "href": "slides/00_Introduction/index.html#internet",
    "title": "Introduction",
    "section": "Internet",
    "text": "Internet\nNetwork : jouvence \nPassword : Orford2021"
  },
  {
    "objectID": "slides/00_Introduction/index.html#dos-and-donts-at-jouvence",
    "href": "slides/00_Introduction/index.html#dos-and-donts-at-jouvence",
    "title": "Introduction",
    "section": "Dos and don’ts at Jouvence",
    "text": "Dos and don’ts at Jouvence\n\nBe respectful of the environment\n\n\nMeals will be served at La Petite Auberge\n\n\nAlcohol needs to be bought on site.\n\n\nGlass containers are not allowed\n\n\nIt is forbidden to smoke cannabis on site\n\n\nMore details can be found in this Google Doc."
  },
  {
    "objectID": "slides/00_Introduction/index.html#course-material",
    "href": "slides/00_Introduction/index.html#course-material",
    "title": "Introduction",
    "section": "Course material",
    "text": "Course material\nWebsite : https://bios2.github.io/hiermod/\n\nOn it, you will find the slides but also practical examples.\n\n\nYou will also find the course syllabus\n\n\nHere is a brief overview of the course"
  },
  {
    "objectID": "slides/00_Introduction/index.html#statistiques-avancées-en-sciences-de-la-vie-bio709bio809",
    "href": "slides/00_Introduction/index.html#statistiques-avancées-en-sciences-de-la-vie-bio709bio809",
    "title": "Introduction",
    "section": "Statistiques avancées en sciences de la vie (BIO709/BIO809)",
    "text": "Statistiques avancées en sciences de la vie (BIO709/BIO809)\n3 credits course at Université de Sherbrooke\n\nIt is a pass or fail course…\n\n\nSo, if you get involved, ask questions, try, work… you will pass."
  },
  {
    "objectID": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course",
    "href": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course",
    "title": "Introduction",
    "section": "What we want to do during this course",
    "text": "What we want to do during this course\n\nBasic knowldege of statistics\n\nGeneral introduction\nGeneral overview of probability theory… for the life sciences\nProbability distribution\nFrequentist and Bayesian modelling\nData simulation (how and when)\nData simulation using different probability distribution"
  },
  {
    "objectID": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-1",
    "href": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-1",
    "title": "Introduction",
    "section": "What we want to do during this course",
    "text": "What we want to do during this course\nStatistical computing\n\nIntroduction to Stan\n\n\n\nWe will not give an introduction to any programming language (e.g. R, Python, Julia).\nWe assume you know one and are conformtable using it… We are more comfortable in R, but can work with many other languages (but please no Fortran !)"
  },
  {
    "objectID": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-2",
    "href": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-2",
    "title": "Introduction",
    "section": "What we want to do during this course",
    "text": "What we want to do during this course\nSimple regression models\n\nRefresher in matrix algebra\nRefresher of linear model\nGeneralized linear model\n\nSimple hierarchical regression models\n\nModel with a single hierarchical component on the intercept\nModel with a single hierarchical component on the slope"
  },
  {
    "objectID": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-3",
    "href": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-3",
    "title": "Introduction",
    "section": "What we want to do during this course",
    "text": "What we want to do during this course\nComplexe hierarchical regression models\n\nModel with multiple hierarchical terms\nModel with multiple hierarchical levels\nHierarchical model with constraints\nGaussian process\nPhylogenetic hierarchical models"
  },
  {
    "objectID": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-4",
    "href": "slides/00_Introduction/index.html#what-we-want-to-do-during-this-course-4",
    "title": "Introduction",
    "section": "What we want to do during this course",
    "text": "What we want to do during this course\n\n\n\n\n\n\n \n\n\n\n\nWe also want to learn about your research… and this might influence what we see during the course !"
  },
  {
    "objectID": "slides/00_Introduction/index.html#course-general-structure",
    "href": "slides/00_Introduction/index.html#course-general-structure",
    "title": "Introduction",
    "section": "Course general structure",
    "text": "Course general structure"
  },
  {
    "objectID": "slides/00_Introduction/index.html#general-daily-schedule",
    "href": "slides/00_Introduction/index.html#general-daily-schedule",
    "title": "Introduction",
    "section": "General daily schedule",
    "text": "General daily schedule\n\n\n7h30 to 8h30 - Breakfast\n\n8h30 to 10h00 - Lecture/Practice\n\n10h00 to 10h30 - Break\n\n10h30 to 12h30 - Lecture/Practice\n\n12h00 to 13h00 - Lunch\n\n13h30 to 15h00 - Lecture/Practice\n\n15h30 to 16h00 - Break\n\n16h00 to 17h00 - Lecture/Practice\n17h00 to 17h30 - Let us know about your research !\n\n17h30 to 18h30 - Break\n\n\n18h30 to 19h00 - Supper"
  },
  {
    "objectID": "slides/00_Introduction/index.html#what-are-hierarchical-models",
    "href": "slides/00_Introduction/index.html#what-are-hierarchical-models",
    "title": "Introduction",
    "section": "What are hierarchical models ?",
    "text": "What are hierarchical models ?\nFor this course, hierarchical models are regression models in which the parameters (the regression coefficients) are not defined by a single value but they are themselves given a probability distribution (Gelman and Hill 2007)."
  },
  {
    "objectID": "slides/00_Introduction/index.html#particularities-of-hierarchical-models",
    "href": "slides/00_Introduction/index.html#particularities-of-hierarchical-models",
    "title": "Introduction",
    "section": "Particularities of hierarchical models",
    "text": "Particularities of hierarchical models\nHierarchical models are\n\n\nA challenging bit of technology (probably more than you might think!)\n\n\n\n\nVery flexible models (in many more ways that are usually expected!)\n\n\n\n\nConstrained to the same particularities as (generalized) (non) linear models (sometimes to a more severe extent)"
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-vocabulary",
    "href": "slides/00_Introduction/index.html#a-bit-of-vocabulary",
    "title": "Introduction",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\n\nHierarchical models, as we will see them in this course, are also known under different names\n\n\n\nRandom effect models\n\n\n\n\nMixed models\n\n\n\n\nMultilevel models\n\n\n\n\nVariance component models\n\n\n\n\nError component models"
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-vocabulary-1",
    "href": "slides/00_Introduction/index.html#a-bit-of-vocabulary-1",
    "title": "Introduction",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\nWe decided to use the term hierachical model to prevent confusions that sometimes arises in the litterature about random and fixed effects, which are terms commonly used when referring to mixed effect models.\n\nActually, random and fixed effects have multiple definitions, which leads to confusion."
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-vocabulary-2",
    "href": "slides/00_Introduction/index.html#a-bit-of-vocabulary-2",
    "title": "Introduction",
    "section": "A bit of vocabulary",
    "text": "A bit of vocabulary\nMultiple Definition of fixed and random effects\n\n\n(Kreft and De Leeuw 1998) Fixed effects are constant and random effect vary\n\n\n\n\n(Searl et al. 1992) Effects are fixed if they are interesting in themselves or random if there is interest in the underlying population\n\n\n\n\n(Green and Tukey 1960) When a sample exhausts the population, the corresponding variable is fixed; when the sample is a small (i.e., negligible) part of the population the corresponding variable is random\n\n\n\n\n(Roy LaMotte 2014) If an effect is assumed to be a realized value of a random variable, it is called a random effect\n\n\n\n\n(Robinson 1991) Fixed effects are estimated using least squares (or, more generally, maximum likelihood) and random effects are estimated with shrinkage."
  },
  {
    "objectID": "slides/00_Introduction/index.html#a-bit-of-history",
    "href": "slides/00_Introduction/index.html#a-bit-of-history",
    "title": "Introduction",
    "section": "A bit of history",
    "text": "A bit of history\n\nBecause of the different name used for hierarchial models, the history of this subfield of statistics is a little murky, but we know that a few important figures in statistics dabbed into this field, most notably\n\n\n\n\n\n\n\n\n\n\n\n\nFisher R.A. (1919). The Correlation between Relatives on the Supposition of Mendelian Inheritance. Transactions of the Royal Society of Edinburgh, 52 399–433."
  },
  {
    "objectID": "slides/00_Introduction/index.html#good-reference",
    "href": "slides/00_Introduction/index.html#good-reference",
    "title": "Introduction",
    "section": "Good reference",
    "text": "Good reference\n\nA good portion of this course material is based on this book."
  },
  {
    "objectID": "slides/00_Introduction/index.html#great-technical-references",
    "href": "slides/00_Introduction/index.html#great-technical-references",
    "title": "Introduction",
    "section": "Great technical references",
    "text": "Great technical references\n\nEverything is there but it can gets technical !"
  },
  {
    "objectID": "slides/00_Introduction/index.html#implementation",
    "href": "slides/00_Introduction/index.html#implementation",
    "title": "Introduction",
    "section": "Implementation",
    "text": "Implementation\n\n\nHierarchical models have been implemented in many software packages,\n\n\n\n\nin R\n\nlme4, brms, nlme, glmmTMB, MCMCglmm, …\n\n\n\n\n\nin SAS\n\nMIXED, HPMIXED, GLMMIX, …\n\n\n\n\n\nin Julia\n\nMixedModels.jl\n\n…"
  },
  {
    "objectID": "slides/00_Introduction/index.html#implementation-1",
    "href": "slides/00_Introduction/index.html#implementation-1",
    "title": "Introduction",
    "section": "Implementation",
    "text": "Implementation\nWhat we will use\n\nWe will not use any of these software packages because under specific circumstances, what may seem like the same implementation of a model may lead to different answers and both can actually be right !\n\n\nThis is because the underlying model implemented in the software package may actually be slightly different.\n\n\nInstead we will implement our own models from scratch using Stan."
  },
  {
    "objectID": "slides/00_Introduction/index.html#r-and-stan",
    "href": "slides/00_Introduction/index.html#r-and-stan",
    "title": "Introduction",
    "section": "R and Stan",
    "text": "R and Stan\nAll of the practical aspect of the course will be done with R and Stan.\n\nRStudio\nWe strongly (!) encourage you to use RStudio and to start a project for the course."
  },
  {
    "objectID": "slides/00_Introduction/index.html#r-package-to-install",
    "href": "slides/00_Introduction/index.html#r-package-to-install",
    "title": "Introduction",
    "section": "R package to install",
    "text": "R package to install\n\n# Package on CRAN\ninstall.packages(c(\"vegan\",\n                   \"palmerpenguins\",\n                   \"tidybayes\",\n                   \"tidyverse\",\n                   \"posterior\"))\n\n# Run the next line if you already have rstan installed\n# remove.packages(c(\"StanHeaders\", \"rstan\"))\n\ninstall.packages(\"rstan\", repos = c('https://stan-dev.r-universe.dev', getOption(\"repos\")))\n\nexample(stan_model, package = \"rstan\", run.dontrun = TRUE)"
  },
  {
    "objectID": "topics/03_one_random_effect/index.html",
    "href": "topics/03_one_random_effect/index.html",
    "title": "Models with one level of hierarchy",
    "section": "",
    "text": "Bayesian workflow\n\n\n\n\nVisualize your data\nDecide on your model structure\nSimulate from the model to understand it\nFit the model to the data\nPlot model predictions to evaluate the fit / draw conclusions\nToday’s goal is to look at a couple of different model structures that we saw yesterday.\nlibrary(tidyverse)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\nlibrary(tidybayes)\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "topics/03_one_random_effect/index.html#gaussian-random-intercepts-penguin-body-mass",
    "href": "topics/03_one_random_effect/index.html#gaussian-random-intercepts-penguin-body-mass",
    "title": "Models with one level of hierarchy",
    "section": "Gaussian random intercepts: Penguin body mass",
    "text": "Gaussian random intercepts: Penguin body mass\nAre populations of penguins on different islands different in their body mass?\nThe Palmer penguins are found on three different islands. Let’s look at the distribution of body mass of each species on each island.\n\nPlot the data\n\npenguin_mass_island &lt;- penguins |&gt; \n  select(species, island, body_mass_g) |&gt; \n  drop_na(body_mass_g) |&gt; \n  unite(sp_island, species, island) |&gt; \n  ## center mass and change the units\n  mutate(mass_kg = (body_mass_g)/1000)\n\n\npenguin_mass_island |&gt; \n  ggplot(aes(y = sp_island,\n             x = mass_kg,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nAre the sample sizes equal among the species-island combinations?\n\npenguin_mass_island |&gt; \n  count(sp_island) |&gt; \n  knitr::kable()\n\n\n\n\nsp_island\nn\n\n\n\n\nAdelie_Biscoe\n44\n\n\nAdelie_Dream\n56\n\n\nAdelie_Torgersen\n51\n\n\nChinstrap_Dream\n68\n\n\nGentoo_Biscoe\n123\n\n\n\n\n\n\n\nDecide on a model structure\nWe’ll begin by fitting a model that assumes that body size for each of these five groups is completely independent:\n\\[\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5)\n\\end{align}\n\\]\n\n\nSimulate to understand this model\nHere’s a little trick to get group indexes (numbers) from a character vector:\n\ngroup_names &lt;- unique(penguin_mass_island$sp_island)\ngroup_numbers &lt;- seq_along(group_names)\nnames(group_numbers) &lt;- group_names\n\ngroup_numbers\n\nAdelie_Torgersen    Adelie_Biscoe     Adelie_Dream    Gentoo_Biscoe \n               1                2                3                4 \n Chinstrap_Dream \n               5 \n\n\n\npenguin_groupid &lt;- penguin_mass_island |&gt; \n  mutate(group_id = group_numbers[sp_island])\n\npenguin_groupid\n\n# A tibble: 342 × 4\n   sp_island        body_mass_g mass_kg group_id\n   &lt;chr&gt;                  &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;\n 1 Adelie_Torgersen        3750    3.75        1\n 2 Adelie_Torgersen        3800    3.8         1\n 3 Adelie_Torgersen        3250    3.25        1\n 4 Adelie_Torgersen        3450    3.45        1\n 5 Adelie_Torgersen        3650    3.65        1\n 6 Adelie_Torgersen        3625    3.62        1\n 7 Adelie_Torgersen        4675    4.68        1\n 8 Adelie_Torgersen        3475    3.48        1\n 9 Adelie_Torgersen        4250    4.25        1\n10 Adelie_Torgersen        3300    3.3         1\n# ℹ 332 more rows\n\n\nAs you can see, we’re set up now with the names and the indexes we need.\nNow we can simulate data and plot it:\n\nngroup &lt;- length(group_numbers)\noverall_mean &lt;- rnorm(1, mean = 5, sd = 2)\ngroup_diffs &lt;- rnorm(n = ngroup, mean = 0, sd = 1)\nsigma_obs &lt;- rexp(1, .5)\n\npenguin_pred_obs &lt;- penguin_groupid |&gt; \n  mutate(fake_mass_avg = overall_mean + group_diffs[group_id],\n         fake_mass_obs = rnorm(length(fake_mass_avg), \n                               mean = fake_mass_avg, \n                               sd = sigma_obs))\n\npenguin_pred_obs |&gt; \n  ggplot(aes(y = sp_island,\n             x = fake_mass_obs,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nRun the above code a few times! if you want, try different prior values.\n\n\n\n\nWrite it in Stan\n\nfixed_groups &lt;- cmdstan_model(stan_file = \"topics/03_one_random_effect/fixed_groups.stan\")\n\nfixed_groups\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  int&lt;lower=0&gt; Ngroup;\n  array[N] int&lt;lower=0, upper=Ngroup&gt; group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma);\n  b_group ~ std_normal();\n  b_avg ~ normal(5, 2);\n  sigma ~ exponential(.5);\n}\ngenerated quantities {\n  \n  vector[Ngroup] group_averages;\n  \n  for (k in 1:Ngroup){\n    group_averages[k] = b_avg + b_group[k];\n  }\n  \n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n  \n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(group_averages[k], sigma);\n  }\n}\n\n\n\n\nFit the model\n\npeng_group_list &lt;- with(penguin_groupid, \n         list(\n           N = length(mass_kg),\n           y = mass_kg,\n           Ngroup = max(group_id),\n           group_id = group_id\n         ))\n\nfixed_groups_samples &lt;- fixed_groups$sample(\n  data = peng_group_list,\n  refresh = 0,\n  parallel_chains = 4\n)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/RtmpH4tcI4/model-4700c1ddd09d5.stan', line 13, column 2 to column 47)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.4 seconds.\n\n\n\n\nPlot predictions to evaluate results\nLet’s begin by plotting the averages for each group.\n\nfixed_groups_samples |&gt; \n  gather_rvars(group_averages[group_id]) |&gt; \n  mutate(sp_island = names(group_numbers)[group_id]) |&gt; \n  ggplot(aes(y = sp_island, dist = .value)) + \n  stat_pointinterval() + \n  geom_jitter(data = penguin_mass_island,\n              aes(y = sp_island,\n                  x = mass_kg,\n                  colour = sp_island), \n              pch = 21, inherit.aes = FALSE,\n              alpha = 0.8, height = 0.1, width = 0) + \n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\nSome things to notice about the code above:\n\nI’m using my named vector group_numbers to re-create the column sp_island. This is my technique for making sure I always use the correct label, but you can do this any way you want.\nWe use tidybayes::stat_pointinterval() to summarize the posterior distribution.\nwe’re adding points from the original data (penguin_mass_island) with geom_jitter(). We’re adding noise vertically to make the visualization better, but not adding any horizontal noise.\n\n\n\n\n\n\n\nEXERCISE: plot posterior predictions of observations\n\n\n\nRepeat the exercise above using the value of one_obs_per_group. Why are the results different? What additional error is included in these predictions?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nfixed_groups_samples |&gt; \n  tidybayes::gather_rvars(one_obs_per_group[group_id]) |&gt; \n  mutate(sp_island = group_names[group_id]) |&gt; \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval(colour = \"black\") + \n  geom_jitter(\n    aes(y = sp_island,\n        x = mass_kg,\n        colour = sp_island), \n    inherit.aes = FALSE,\n    alpha = .2, data = penguin_groupid, height = .2, width = 0) + \n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nMake it hierarchical\n\nMath\n\n\n\n\n\n\\[\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5)\n\\end{align}\n\\]\n\n\n\n\\[\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, \\sigma_{\\text{sp}}) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5) \\\\\n\\sigma_{\\text{sp}} &\\sim \\text{Exponential}(1)\n\\end{align}\n\\]\n\n\n\n\n\n\n\nSimulation of a hierarchical model\n\n\n\n\n\n\nEXERCISE\n\n\n\nSimulate from the model above. Base your approach on the code for simulation the non-hierarchical version. Remember to simulate one additional number: the standard deviation of group differences\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nngroup &lt;- length(group_numbers)\noverall_mean &lt;- rnorm(1, mean = 5, sd = 2)\nsigma_group &lt;- rexp(1, .1)\ngroup_diffs &lt;- rnorm(n = ngroup, mean = 0, sd = sigma_group)\nsigma_obs &lt;- rexp(1, .5)\n\npenguin_pred_obs &lt;- penguin_groupid |&gt; \n  mutate(fake_mass_avg = overall_mean + group_diffs[group_id],\n         fake_mass_obs = rnorm(length(fake_mass_avg), \n                               mean = fake_mass_avg, \n                               sd = sigma_obs))\n\npenguin_pred_obs |&gt; \n  ggplot(aes(y = sp_island,\n             x = fake_mass_obs,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nStan\nBelow I’m comparing the two Stan programs side-by-side. Compare them to the models above!\n\nhierarchical_groups &lt;- cmdstan_model(stan_file = \"topics/03_one_random_effect/hierarchical_groups.stan\")\n\n\n\n\n\n\n\nfixed_groups\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  int&lt;lower=0&gt; Ngroup;\n  array[N] int&lt;lower=0, upper=Ngroup&gt; group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma);\n  b_group ~ std_normal();\n  b_avg ~ normal(5, 2);\n  sigma ~ exponential(.5);\n}\ngenerated quantities {\n  \n  vector[Ngroup] group_averages;\n  \n  for (k in 1:Ngroup){\n    group_averages[k] = b_avg + b_group[k];\n  }\n  \n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n  \n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(group_averages[k], sigma);\n  }\n}\n\n\n\n\n\n\nhierarchical_groups\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] y;\n  int&lt;lower=0&gt; Ngroup;\n  array[N] int&lt;lower=0, upper=Ngroup&gt; group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real&lt;lower=0&gt; sigma_obs;\n  real&lt;lower=0&gt; sigma_grp;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma_obs);\n  b_group ~ normal(0, sigma_grp);\n  b_avg ~ normal(5, 2);\n  sigma_obs ~ exponential(.5);\n  sigma_grp ~ exponential(1);\n}\ngenerated quantities {\n  \n  vector[Ngroup] group_averages;\n  \n  for (k in 1:Ngroup){\n    group_averages[k] = b_avg + b_group[k];\n  }\n  \n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n  \n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(group_averages[k], sigma_obs);\n  }\n  \n  // difference for a new group\n  real new_b_group = normal_rng(0, sigma_grp);\n  \n  // observations from that new group\n  real one_obs_new_group = normal_rng(b_avg + new_b_group, sigma_obs);\n  \n}\n\n\n\n\n\n\n\n\nhierarchical_groups_samples &lt;- hierarchical_groups$sample(\n  data = peng_group_list, refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/RtmpWhWKuf/model-473001618f1b5.stan', line 15, column 2 to column 33)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 finished in 0.4 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.4 seconds.\nTotal execution time: 0.4 seconds.\n\n\n\nhierarchical_groups_samples\n\n          variable  mean median   sd  mad    q5   q95 rhat ess_bulk ess_tail\n lp__              88.39  88.71 2.14 1.99 84.52 91.22 1.00      950     1692\n b_avg              4.02   4.01 0.37 0.32  3.45  4.65 1.01      519      643\n b_group[1]        -0.31  -0.29 0.38 0.33 -0.95  0.27 1.01      539      642\n b_group[2]        -0.31  -0.29 0.38 0.32 -0.96  0.28 1.01      535      632\n b_group[3]        -0.33  -0.32 0.38 0.33 -0.97  0.24 1.01      523      652\n b_group[4]         1.05   1.06 0.37 0.32  0.43  1.62 1.01      530      638\n b_group[5]        -0.29  -0.28 0.38 0.32 -0.92  0.29 1.01      529      660\n sigma_obs          0.47   0.46 0.02 0.02  0.44  0.50 1.00     1808     1534\n sigma_grp          0.79   0.69 0.37 0.25  0.41  1.43 1.00     1167      977\n group_averages[1]  3.71   3.71 0.07 0.07  3.60  3.81 1.00     4589     2590\n\n # showing 10 of 21 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n\nhierarchical_groups_samples |&gt; \n  tidybayes::gather_rvars(b_group[group_id],\n                          new_b_group) |&gt; \n  mutate(sp_island = group_names[group_id],\n         sp_island = if_else(is.na(sp_island),\n                             true = \"New Group\",\n                             false = sp_island)) |&gt; \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval()\n\n\n\n\n\n\n\n\n\nhierarchical_groups_samples |&gt; \n  tidybayes::gather_rvars(one_obs_per_group[group_id],\n                          one_obs_new_group) |&gt; \n  mutate(sp_island = group_names[group_id],\n         sp_island = if_else(is.na(sp_island),\n                             true = \"New Group\",\n                             false = sp_island)) |&gt; \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval() + \n  geom_point(aes(y = sp_island,\n             x = mass_kg,\n             colour = sp_island), \n             inherit.aes = FALSE,\n             alpha = .2, data = penguin_groupid)\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\nTry leaving out a group and refitting the hierarchical model. Are the predictions for the missing group accurate?\nThere are other categorical predictors in the dataset. Try including year as a part of the group-creating factor (i.e. in the call to unite() above). What changes?\nModify the generated quantities block to simulate a fake observation for EVERY row of the dataset. This opens the possibility of using bayesplot to make predictions. Look back at the code from Day 1 and create a posterior predictive check for both models. (e.g. using ppc_dens_overlay)\nWe could perhaps have used sex as a grouping factor, but sex has missing values in it! Why is this a problem for this kind of model? What would it take to address that? (Discussion only; missing values are unfortunately outside the scope of the class!)"
  },
  {
    "objectID": "topics/03_one_random_effect/index.html#observation-level-random-effects-mite-abundance",
    "href": "topics/03_one_random_effect/index.html#observation-level-random-effects-mite-abundance",
    "title": "Models with one level of hierarchy",
    "section": "Observation-level random effects: Mite abundance",
    "text": "Observation-level random effects: Mite abundance\n\nWhat is the question?\nLet’s write a model to answer the question:\nHow does the total abundance of the mite community change as water content increases?\n\n\nExpress this in Math\nHere’s a partially complete model for species richness over time\n\\[\n\\begin{align}\n\\text{S}_i &\\sim \\text{Poisson}(e^a) \\\\\na &= \\bar\\beta + \\beta_{\\text{water}} \\cdot \\text{water}_i \\\\\n\\bar\\beta &\\sim \\text{Normal}(?, ?) \\\\\n\\beta_{\\text{water}} &\\sim \\text{Normal}(?, ?) \\\\\n\\end{align}\n\\]\n\n\n\n\n\n\nEXERCISE\n\n\n\nSimulate from this model, and look at your simulations to decide on a reasonable prior for the data.\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nn &lt;- 30\nwater &lt;- seq(from = -5, to = 5, length.out = n)\n\nb0 &lt;- rnorm(1, mean = log(17), sd = .3)\nb1 &lt;- rnorm(1, mean = 0, sd = .2)\n\nS &lt;- rpois(n, lambda = exp(b0 + b1*water))\nplot(water, S)\n\n\n\n\n\n\n\n\n\n\n\n\n\nData preparation & visualization\nFirst we need to load and prepare the data:\n\ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\n\n# combine data and environment\n\nmite_data_long &lt;- mite |&gt; \n  tibble::rownames_to_column(var = \"site_id\") |&gt; \n  bind_cols(mite.env) |&gt; \n  pivot_longer(Brachy:Trimalc2,\n               names_to = \"spp\", values_to = \"abd\")\n\nFirst let’s transform the mite dataset into a dataframe of total community abundance (N) per site. We’ll also standardize the water content while we’re at it:\n\nmite_community_abd &lt;- mite_data_long |&gt; \n  group_by(site_id, WatrCont) |&gt; \n  summarize(N = sum(abd)) |&gt;\n  ungroup() |&gt; \n  mutate(water_c = (WatrCont - mean(WatrCont))/100)\n\n`summarise()` has grouped output by 'site_id'. You can override using the\n`.groups` argument.\n\nknitr::kable(head(mite_community_abd))\n\n\n\n\nsite_id\nWatrCont\nN\nwater_c\n\n\n\n\n1\n350.15\n140\n-0.6048571\n\n\n10\n220.73\n166\n-1.8990571\n\n\n11\n134.13\n216\n-2.7650571\n\n\n12\n405.91\n213\n-0.0472571\n\n\n13\n243.70\n177\n-1.6693571\n\n\n14\n239.51\n269\n-1.7112571\n\n\n\n\n\nWe get a nice histogram of community abundance, and a clear negative relationship with water volume:\nmite_community_abd |&gt; \n  ggplot(aes(x = N)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nmite_community_abd |&gt; \n  ggplot(aes(x = water_c, y = N)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\nWrite the model in Stan and estimate it\n\npoisson_regression &lt;- cmdstan_model(stan_file = \"topics/03_one_random_effect/poisson_regression.stan\")\n\n\nwater_for_pred &lt;- seq(from = -3, to = 4.5, length.out = 15)\n\nabd_data_list &lt;- list(N = length(mite_community_abd$N),\n              water = mite_community_abd$water_c,\n              y = mite_community_abd$N,\n              Npred = 15,\n              water_pred = water_for_pred)\n\npoisson_regression_sample &lt;- poisson_regression$sample(\n  data = abd_data_list,\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\n\n\n\nPlot the model to see if it fits well\npoisson_regression_sample |&gt; \n  tidybayes::gather_rvars(line_obs[i]) |&gt; \n  mutate(water = water_for_pred) |&gt; \n  ggplot(aes(x = water, dist = .value)) + \n  stat_lineribbon() + \n  geom_point(aes(x = water_c, y = N), \n             data = mite_community_abd, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Greens\")\nfake_obs_S &lt;- poisson_regression_sample$draws(variables = \"fake_obs\")\nfake_obs_S_matrix &lt;- posterior::as_draws_matrix(fake_obs_S)\n\nbayesplot::ppc_dens_overlay(y = mite_community_abd$N,\n                            yrep = head(fake_obs_S_matrix, 50))\n\n\n\n\n\n\n\n\n\n\nRemember, on the left we are plotting the Prediction interval here: it’s showing the distribution of probable observations according to the model. Notice that the the model predicts much narrower variation than we really find!\nOn the right hand side we have the posterior predictive check, which once again shows that the model is overconfident and predicts a range of observations that are far too narrow.\n\n\nEXERCISE\n\nDiscuss with your neighbours: would you trust this model? Would you publish it? The technical name for this phenomenon is “overdisperson”. Have you checked for this in previous count models you’ve done?\nAdd a random effect for every individual observation in the model. Begin by writing the mathematical notation for this new model!\nfit the model and re-create the two figures above. What do you notice?\n\n\nWhich model is more trustworthy?\nlook at the slope in the new model. Is it different?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nMathematical notation\n\\[\n\\begin{align}\n\\text{S}_i &\\sim \\text{Poisson}(e^a) \\\\\na &= \\bar\\beta + \\beta_{\\text{water}} \\cdot \\text{water}_i + \\text{site}_i\\\\\n\\bar\\beta &\\sim \\text{Normal}(?, ?) \\\\\n\\beta_{\\text{water}} &\\sim \\text{Normal}(?, ?) \\\\\n\\text{site} &\\sim \\text{Normal}(?, \\sigma) \\\\\n\\sigma &\\sim \\text{Exponential}(?)\n\\end{align}\n\\]\n\n\nStan code\n\npoisson_regression_overdisp &lt;- cmdstan_model(stan_file = \"topics/03_one_random_effect/poisson_regression_overdisp.stan\")\n\npoisson_regression_overdisp_sample &lt;- poisson_regression_overdisp$sample(\n  data = abd_data_list, refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/RtmpIhvutV/model-471231bcd7800.stan', line 18, column 2 to column 42)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 finished in 0.3 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/RtmpIhvutV/model-471231bcd7800.stan', line 18, column 2 to column 42)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 1.3 seconds.\n\n\nfake_obs_S &lt;- poisson_regression_overdisp_sample$draws(variables = \"fake_obs\")\nfake_obs_S_matrix &lt;- posterior::as_draws_matrix(fake_obs_S)\n\nbayesplot::ppc_dens_overlay(y = mite_community_abd$N,\n                            yrep = head(fake_obs_S_matrix, 50))\npoisson_regression_overdisp_sample |&gt; \n  tidybayes::gather_rvars(line_obs[i]) |&gt; \n  mutate(water = water_for_pred) |&gt; \n  ggplot(aes(x = water, dist = .value)) + \n  stat_lineribbon() + \n  geom_point(aes(x = water_c, y = N), \n             data = mite_community_abd, \n             inherit.aes = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAnother great way to model overdispersion is via the Negative Binomial distribution. Look at the Stan documentation for neg_binomial_2_log and adapt your model to use it (don’t forget to drop the random effect when you do!)."
  },
  {
    "objectID": "topics/correlated_effects/index.html",
    "href": "topics/correlated_effects/index.html",
    "title": "Summarizing many univariate models",
    "section": "",
    "text": "We’ve already looked at univariate models. When we fit the same model to multiple different groups, we don’t expect the same values for all the coefficients. Each thing we are studying will respond to the same variable in different ways.\nHierarchial models represent a way to model this variation, in ways that range from simple to complex.\nBefore we dive in with hierarchical structure, let’s build a bridge between these two approaches.\nThis is useful to help us understand what a hierarchical model does.\nHowever it is also useful from a strict model-building perspective – so useful that Andrew Gelman calls it a “Secret Weapon”\ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\nlibrary(tidyverse)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\n# combine data and environment\nmite_data_long &lt;- bind_cols(mite.env, mite) |&gt; \n  pivot_longer(Brachy:Trimalc2, names_to = \"spp\", values_to = \"abd\")\nTo keep things simple and univariate, let’s consider only water:\nFirst, a quick word about centering and scaling a predictor variable:\nmite_data_long_transformed &lt;- mite_data_long |&gt; \n  mutate(presabs = as.numeric(abd&gt;0),\n         # center predictors\n         water = (WatrCont - mean(WatrCont)) / 100\n         )\n\nmite_data_long_transformed |&gt; \n  ggplot(aes(x = water, y = presabs)) + \n  geom_point() + \n  stat_smooth(method = \"glm\", method.args = list(family = \"binomial\")) + \n  facet_wrap(~spp)\n\n`geom_smooth()` using formula = 'y ~ x'\nsome things to notice about this figure:\nmite_many_glms &lt;- mite_data_long_transformed |&gt; \n  nest_by(spp) |&gt; \n  mutate(logistic_regressions = list(\n    glm(presabs ~ water,\n        family = \"binomial\",\n        data = data))) |&gt; \n  mutate(coefs = list(broom::tidy(logistic_regressions)))\nmite_many_glm_coefs &lt;- mite_many_glms |&gt; \n  select(-data, -logistic_regressions) |&gt; \n  unnest(coefs)\n\nmite_many_glm_coefs |&gt; \n  ggplot(aes(x = estimate, y = spp,\n             xmin = estimate - std.error,\n             xmax = estimate + std.error)) + \n  geom_pointrange() + \n  facet_wrap(~term, scales = \"free\")\nAs you can see, some of these estimates are high, others low. We could also plot these as histograms to see this distribution.\nmite_many_glm_coefs |&gt; \n  ggplot(aes(x = estimate)) + \n  geom_histogram(binwidth = .5) + \n  facet_wrap(~term, scales = \"free\")\nOnce again, the two parameters of this model represent:"
  },
  {
    "objectID": "topics/correlated_effects/index.html#say-it-in-stan",
    "href": "topics/correlated_effects/index.html#say-it-in-stan",
    "title": "Summarizing many univariate models",
    "section": "Say it in Stan",
    "text": "Say it in Stan\nThe above tidyverse approach is very appealing and intuitive, but we can also do the same procedure in Stan.\n\nall_species_unpooled &lt;- cmdstan_model(\n  stan_file = \"topics/correlated_effects/all_species_unpooled.stan\", \n  pedantic = TRUE)\n\nall_species_unpooled\n\ndata {\n  // number of rows in dataset\n  int&lt;lower=0&gt; Nsites;\n  // number of species\n  int&lt;lower=0&gt; S;\n  // one environmental variable to use in prediction\n  vector[Nsites] x;\n  // response site (rows) by species (columns) 2D array\n  array[Nsites,S] int &lt;lower=0,upper=1&gt; y;\n}\nparameters {\n  // parameters are now VECTORS\n  vector[S] intercept;\n  vector[S] slope;\n}\nmodel {\n  for (s in 1:S){\n    y[,s] ~ bernoulli_logit(intercept[s] + slope[s] * x);\n  }\n  // priors don't change because Stan is vectorized:\n  // every element of the vector gets the same prior\n  intercept ~ normal(0, .5);\n  slope ~ normal(0, .5);\n}\n\n\nLet’s fit this model by passing in the data:\n\nmite_bin &lt;- mite\nmite_bin[mite_bin&gt;0] &lt;- 1\n\nmite_pa_list &lt;- list(\n      Nsites = nrow(mite_bin),\n      S = ncol(mite_bin),\n      x = with(mite.env, (WatrCont - mean(WatrCont))/100),\n      y = as.matrix(mite_bin)\n    )\n\nall_species_unpooled_posterior &lt;- \n  all_species_unpooled$sample(\n    data = mite_pa_list, \n    refresh = 1000, parallel_chains = 4\n    )\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 1.5 seconds.\nChain 2 finished in 1.5 seconds.\nChain 3 finished in 1.4 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.5 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.5 seconds.\nTotal execution time: 1.7 seconds.\n\n\nnow let’s try to plot this:\n\n# start by looking at the names of variables\n# get_variables(all_species_unpooled_posterior)\n\npost_pred &lt;- tidybayes::spread_rvars(all_species_unpooled_posterior, \n             intercept[spp_id], slope[spp_id]) |&gt; \n  expand_grid(water = seq(from = -4, to = 4, length.out = 10)) |&gt; \n  mutate(prob = posterior::rfun(plogis)(intercept + slope*water),\n         spp = colnames(mite_bin)[spp_id]) |&gt; \n  ggplot(aes(x = water, dist = prob)) + \n  tidybayes::stat_lineribbon() + \n  facet_wrap(~spp) + \n  scale_fill_brewer(palette = \"Greens\")\n\npost_pred\n\n\n\n\n\n\n\n\nWe can imitate the original figure by adding the observed data in orange:\n\npost_pred + \n  geom_point(aes(x = water, y = presabs), \n             inherit.aes = FALSE, \n             data = mite_data_long_transformed,\n             pch = 21, \n             fill = \"orange\")\n\n\n\n\n\n\n\n\nPlot and compare to frequentist point estimates\n\nlong_rvars &lt;- tidybayes::gather_rvars(\n  all_species_unpooled_posterior, \n             intercept[spp_id], slope[spp_id]) \n\n\nmite_many_glm_coefs |&gt; \n  select(spp, term, estimate)\n\n# A tibble: 70 × 3\n# Groups:   spp [35]\n   spp      term        estimate\n   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n 1 Brachy   (Intercept)   2.43  \n 2 Brachy   water        -0.547 \n 3 Ceratoz1 (Intercept)   0.466 \n 4 Ceratoz1 water         0.0273\n 5 Ceratoz3 (Intercept)  -0.237 \n 6 Ceratoz3 water         0.280 \n 7 Eupelops (Intercept)  -0.464 \n 8 Eupelops water        -0.535 \n 9 FSET     (Intercept)  -0.644 \n10 FSET     water        -1.82  \n# ℹ 60 more rows"
  },
  {
    "objectID": "topics/correlated_effects/index.html#saying-it-another-way-with-matrix-algebra",
    "href": "topics/correlated_effects/index.html#saying-it-another-way-with-matrix-algebra",
    "title": "Summarizing many univariate models",
    "section": "Saying it another way: with matrix algebra",
    "text": "Saying it another way: with matrix algebra\nFirst let’s look at how to scale a univariate distribution:\nz &lt;- rnorm(300, mean = 0, sd = 1)\nhist(z)\nsd(z)\nhist(z*2.7)\nsd(z*2.7)\n\n\n\n\n\n\n[1] 0.9802826\n\n\n\n\n\n\n\n[1] 2.646763\n\n\n\nyou can also scale two things at once with a diagonal matrix:\n\ntwo_sds &lt;- diag(c(.4, 7))\n\nzz &lt;- matrix(rnorm(200), ncol = 2)\n\nrescaled_zz &lt;- zz %*% two_sds\n\nplot(rescaled_zz)\n\n\n\n\n\n\n\napply(rescaled_zz, 2, sd)\n\n[1] 0.4442898 7.1119074\n\n\nWe can apply this approach in Stan to create the SAME model as above:\n\nall_species_unpooled_diag &lt;- cmdstan_model(\n  stan_file = \"topics/correlated_effects/all_species_unpooled_diag.stan\", \n  pedantic = TRUE)\n\nall_species_unpooled_diag\n\ndata {\n  int&lt;lower=0&gt; Nsites;         // num of sites in the dataset\n  // int&lt;lower=1&gt; 2;              // num of site predictors\n  int&lt;lower=1&gt; S;              // num of species\n  matrix[Nsites, 2] x;         // site-level predictors\n  array[Nsites, S] int&lt;lower=0, upper=1&gt; y;  // species presence or absence\n}\ntransformed data {\n vector&lt;lower=0&gt;[2] sd_params = [0.5, 0.5]'; \n}\nparameters {\n  // species departures from the average\n  matrix[2, S] z;               \n  // AVERAGE of slopes and intercepts\n  vector[2] gamma;\n}\ntransformed parameters {\n  matrix[2, S] beta = rep_matrix(gamma, S) + diag_pre_multiply(sd_params, z);\n}\nmodel {\n  matrix[Nsites, S] mu;\n  // calculate the model average\n  mu = x * beta;\n  // Likelihood\n  for (s in 1:S) {\n    y[,s] ~ bernoulli_logit(mu[,s]);\n  }\n  // priors\n  to_vector(z) ~ std_normal();\n  gamma ~ normal(0, 2);\n}\n\n\n\nall_species_unpooled_diag_sample &lt;- all_species_unpooled_diag$sample(\n  data = mite_pa_list, \n  refresh = 1000, parallel_chains = 4\n)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Exception: mismatch in number dimensions declared and found in context; processing stage=data initialization; variable name=x; dims declared=(70,2); dims found=(70) (in '/tmp/RtmpLDZ6cu/model-f7225abe2cdc.stan', line 5, column 2 to column 22)\n\n\nChain 2 Exception: mismatch in number dimensions declared and found in context; processing stage=data initialization; variable name=x; dims declared=(70,2); dims found=(70) (in '/tmp/RtmpLDZ6cu/model-f7225abe2cdc.stan', line 5, column 2 to column 22)\n\n\nChain 3 Exception: mismatch in number dimensions declared and found in context; processing stage=data initialization; variable name=x; dims declared=(70,2); dims found=(70) (in '/tmp/RtmpLDZ6cu/model-f7225abe2cdc.stan', line 5, column 2 to column 22)\n\n\nChain 4 Exception: mismatch in number dimensions declared and found in context; processing stage=data initialization; variable name=x; dims declared=(70,2); dims found=(70) (in '/tmp/RtmpLDZ6cu/model-f7225abe2cdc.stan', line 5, column 2 to column 22)\n\n\nWarning: Chain 1 finished unexpectedly!\n\n\nWarning: Chain 2 finished unexpectedly!\n\n\nWarning: Chain 3 finished unexpectedly!\n\n\nWarning: Chain 4 finished unexpectedly!\n\n\nWarning: All chains finished unexpectedly! Use the $output(chain_id) method for more information.\n\n\nWarning: Use read_cmdstan_csv() to read the results of the failed chains.\n\n\nWarning: No chains finished successfully. Unable to retrieve the fit."
  },
  {
    "objectID": "topics/correlated_effects/index.html#making-it-hierarchical",
    "href": "topics/correlated_effects/index.html#making-it-hierarchical",
    "title": "Summarizing many univariate models",
    "section": "Making it hierarchical",
    "text": "Making it hierarchical\n\nall_species_partpooled_diag &lt;- cmdstan_model(\n  stan_file = \"topics/correlated_effects/all_species_partpooled_diag.stan\", \n  pedantic = TRUE)\n\nall_species_partpooled_diag\n\ndata {\n  int&lt;lower=0&gt; Nsites;         // num of sites in the dataset\n  int&lt;lower=1&gt; S;              // num of species\n  matrix[Nsites, 2] x;         // site-level predictors\n  array[Nsites, S] int&lt;lower=0, upper=1&gt; y;  // species presence or absence\n}\nparameters {\n  matrix[2, S] z;               // species departures from the average\n  vector[2] gamma;              // AVERAGE of slopes and intercepts\n  vector&lt;lower=0&gt;[2] sd_params;          // standard deviations of species departures\n}\ntransformed parameters {\n  matrix[2, S] beta = rep_matrix(gamma, S) + diag_pre_multiply(sd_params, z);\n}\nmodel {\n  matrix[Nsites, S] mu;\n  // calculate the model average\n  mu = x * beta;\n  // Likelihood\n  for (s in 1:S) {\n    y[,s] ~ bernoulli_logit(mu[,s]);\n  }\n  // priors\n  to_vector(z) ~ std_normal();\n  sd_params ~ exponential(2);\n  gamma ~ normal(0, 2);\n}\n\n\n\n\n\n\n\n\nCovariance matrices\n\n\n\nIf you’ve fit a lot of hierarchical models you may know that usually, slopes and intercepts are modelled as correlated. In this course, we’ve decided to stop here, but the material below is available for anyone who wants"
  },
  {
    "objectID": "topics/correlated_effects/index.html#modelling-covariation",
    "href": "topics/correlated_effects/index.html#modelling-covariation",
    "title": "Summarizing many univariate models",
    "section": "Modelling COvariation",
    "text": "Modelling COvariation\n\nall_species_partpooled_diag_corr &lt;- cmdstan_model(\n  stan_file = \"topics/correlated_effects/all_species_partpooled_diag_corr.stan\", \n  pedantic = TRUE)\n\nall_species_partpooled_diag_corr\n\ndata {\n  int&lt;lower=0&gt; Nsites;         // num of sites in the dataset\n  int&lt;lower=1&gt; S;              // num of species\n  matrix[Nsites, 2] x;         // site-level predictors\n  array[Nsites, S] int&lt;lower=0, upper=1&gt; y;  // species presence or absence\n}\nparameters {\n  // species departures from the average\n  matrix[2, S] z;               \n  // AVERAGE of slopes and intercepts\n  vector[2] gamma;              \n  // standard deviations of species departures\n  vector&lt;lower=0&gt;[2] sd_params; \n  // add the correlations between slope and intercept\n  cholesky_factor_corr[2] slope_inter_corr;\n}\ntransformed parameters {\n  matrix[2, S] beta = rep_matrix(gamma, S) + \n    diag_pre_multiply(sd_params, slope_inter_corr) * z;\n}\nmodel {\n  matrix[Nsites, S] mu;\n  // calculate the model average\n  mu = x * beta;\n  // Likelihood\n  for (s in 1:S) {\n    y[,s] ~ bernoulli_logit(mu[,s]);\n  }\n  // priors\n  to_vector(z) ~ std_normal();\n  sd_params ~ exponential(2);\n  gamma ~ normal(0, .5);\n  slope_inter_corr ~ lkj_corr_cholesky(2);\n}\n\n\n\nmite_data_list &lt;- list(\n  Nsites = nrow(mite_bin),\n  S = ncol(mite_bin),\n  x = cbind(1, with(mite.env, (WatrCont - mean(WatrCont))/100)),\n  y = as.matrix(mite_bin))\n\nall_species_partpooled_diag_corr_posterior &lt;- \n  all_species_partpooled_diag_corr$sample(\n  data = mite_data_list,\n  refresh = 0, parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 1 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 1 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: bernoulli_logit_lpmf: Logit transformed probability parameter[1] is -nan, but must be not nan! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 27, column 4 to column 36)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 3 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 3 \n\n\nChain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 4 Exception: lkj_corr_cholesky_lpdf: Random variable[2] is 0, but must be positive! (in '/tmp/RtmpLDZ6cu/model-f72247c0874.stan', line 33, column 2 to column 42)\n\n\nChain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 4 \n\n\nChain 2 finished in 3.3 seconds.\nChain 3 finished in 3.6 seconds.\nChain 4 finished in 3.6 seconds.\nChain 1 finished in 3.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 3.6 seconds.\nTotal execution time: 3.8 seconds.\n\n\nplot these, reproducing the figure from earlier:\n\n# get the unpooled numbers\nunpooled_slopes &lt;- all_species_unpooled_posterior |&gt; \n  tidybayes::spread_rvars(slope[spp])\n\n# tidybayes::get_variables(all_species_partpooled_diag_posterior)\n\npartpooled_slopes &lt;- all_species_partpooled_diag_corr_posterior |&gt; \n  tidybayes::spread_rvars(beta[param, spp]) |&gt; \n  filter(param == 2)\n\nleft_join(unpooled_slopes,\n          partpooled_slopes,\n          by = \"spp\") |&gt; \n  ggplot(aes(x = median(slope), y = median(beta))) + \n  geom_point() + \n  geom_abline(intercept = 0, slope = 1)"
  },
  {
    "objectID": "topics/correlated_effects/index.html#section",
    "href": "topics/correlated_effects/index.html#section",
    "title": "Summarizing many univariate models",
    "section": "",
    "text": "unpooled_params &lt;- all_species_unpooled_posterior |&gt; \n  tidybayes::spread_rvars(slope[spp], intercept[spp]) |&gt; \n  mutate(slope = median(slope), \n         intercept = median(intercept))\n\n\n\nall_species_partpooled_diag_corr_posterior |&gt; \n  tidybayes::spread_rvars(beta[param, spp]) |&gt; \n  mutate(beta = median(beta),\n         param = c(\"intercept\", \"slope\")[param]) |&gt; \n  pivot_wider(names_from = \"param\", values_from = beta) |&gt; \n  ggplot(aes(x = intercept, y = slope)) + \n  geom_point() + \n  geom_point(data = unpooled_params, col = \"red\")"
  },
  {
    "objectID": "topics/correlated_effects/index.html#correlating-numbers-with-a-cholesky-decomposition-of-a-correlation-matrix.",
    "href": "topics/correlated_effects/index.html#correlating-numbers-with-a-cholesky-decomposition-of-a-correlation-matrix.",
    "title": "Summarizing many univariate models",
    "section": "Correlating numbers with a cholesky decomposition of a correlation matrix.",
    "text": "Correlating numbers with a cholesky decomposition of a correlation matrix.\n\neg &lt;- rethinking::rlkjcorr(1, 2, 1)\ncc &lt;- chol(eg)\n\nzz &lt;- matrix(data = rnorm(2000), ncol = 2)\nplot(zz)\n\n\n\n\n\n\n\ncor(zz)\n\n           [,1]       [,2]\n[1,] 1.00000000 0.02183946\n[2,] 0.02183946 1.00000000\n\nrr &lt;- t(cc) %*% t(zz)\nplot(t(rr))\n\n\n\n\n\n\n\ncor(t(rr))[1,2]\n\n[1] 0.1324028\n\n\nThe LKJ prior distribution is a prior over correlation matrices\n\n# rethinking::rlkjcorr(1, 5, .3)"
  },
  {
    "objectID": "topics/00_distributions/index.html",
    "href": "topics/00_distributions/index.html",
    "title": "Catch a wild distribution",
    "section": "",
    "text": "To warm up for this class we will conduct a brief exercise. This exercise serves a few purposes.\nMost importantly, it will help us to get to know each other a bit!"
  },
  {
    "objectID": "topics/00_distributions/index.html#group-exercise",
    "href": "topics/00_distributions/index.html#group-exercise",
    "title": "Catch a wild distribution",
    "section": "Group Exercise",
    "text": "Group Exercise\n\nWhat makes things the “same” from the point of view of this exercise? Please take some time with your group to talk about what we mean by the word “same”!\n\nSeparate yourselves into groups. The groups should be at least 2 people. Suggestion pair with people you’ve only just met.\nEach group should pick a different quantity that they will measure repeatedly. This can be anything that you can measure rooughly 30 times in the next 90 minutes. Your measurements should be of the SAME thing. Here are some ideas to get you started:\n\nHow long can someone hop on one foot ?\nAmount of change in everyone’s pockets ?\nHow many birds can you hear in 2 minutes ?\n\n\nHypothesize\nWhat statiscal distribution might describe the observations you are sampling/measuring? What are its property? Think about the kind of statistical distribution that might represent this phenomenon! Take a moment and write down the name of the distribution, and why it might describe the thing you’re going to measure.\n\n\nSimulate\ncreate a simulation of the data you’re about to collect. Every statistical distribution has four functions in R, and here you can use two of them:\n\nDensity function (starting with d)\nRandom generation function (starting with r).\n\nFor example, dnorm() and rnorm. Simulate about as many number as you hope to collect. There’s a quick example below.\n\n\nCollect\nAfter you have an idea of what your data might look like, go and get some! Try to get a good number (30 or more), this should take roughly 1 hour.\n\n\nVisualize and fit\nEnter the data into R and visualize them. Do they match your hypothesized distribution? How do you know? If it doesn’t match, why might that be? For this step you have a couple of choices:\n\nCompare histograms\nhist() (base R) or geom_hist (ggplot2 R package)\nfitdistr (MASS R package)\noptim (base R)\n\nStan (we’ll do this one tomorrow!)\n\n\n\nDiscuss\nWe’ll go around the room and each group will do a brief show-and-tell of their chosen distribution, their measurement methods, their results and what they might mean!"
  },
  {
    "objectID": "topics/04_gp/index.html",
    "href": "topics/04_gp/index.html",
    "title": "Gaussian Processes in Stan",
    "section": "",
    "text": "Goals of this lesson\n\n\n\n\nLet’s appreciate together the power of online community resources\nGaussian Processes are families of smooth functions we learn from data\nWhen used for prediction, a GP is both a “prior” and a “likelihood”"
  },
  {
    "objectID": "topics/04_gp/index.html#background-reading",
    "href": "topics/04_gp/index.html#background-reading",
    "title": "Gaussian Processes in Stan",
    "section": "Background reading",
    "text": "Background reading\nGaussian processes are very common, and there are lots of resources on the topic:\n\nThe Stan manual has a chapter on it\nThe Stan team gives lots of example models on Github which I adapted for this example.\nMichael Betancourt has an extremely detailed, very rigous tutorial on GPs\nHere’s a complete, worked analysis of human birthdays by world-class statisticians (Gelman, Vehtari, Simpson, et al)\nGPs are related to GAMs and can be represented by a collection of basis functions. This is approximate but much much faster. See this excellent tutorial by Aki Vehtari, and the corresponding paper (citation in the blog post).\nthis blog applies GPs to spatial count data\nHere is a very long and wonderfully detailed post describing a GP approach to occupany modelling\nAnother blog on Gaussian Processes, Hidden Markov Models and more, very clear explanation.\n\n\n\n\nReorganizing the mite data\nLet’s begin by (once again!) loading and reorganizing the mite data. This time we’ll also use mite.xy, which gives the coordinates of each one of the 70 samples.\n\n# today we need to do the \ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\ndata(\"mite.xy\", package = \"vegan\")\nlibrary(tidyverse)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\n\n\n# combine data and environment\nmite_data_long &lt;- bind_cols(mite.env, mite) |&gt; \n  mutate(plot_id = 1:length(WatrCont)) |&gt; \n  pivot_longer(Brachy:Trimalc2, names_to = \"spp\", values_to = \"abd\")\n\n\nmite_data_long_transformed &lt;- mite_data_long |&gt; \n  mutate(presabs = as.numeric(abd&gt;0),\n         # center predictors\n         water = (WatrCont - mean(WatrCont)) / 100\n         )\n\n# pick a species that has about 50/50 chance \n\nmite_data_long_transformed |&gt;\n  group_by(spp) |&gt;\n  summarize(freq = mean(presabs)) |&gt;\n  filter(freq &gt; .4 & freq &lt; .6)\n\n# A tibble: 10 × 2\n   spp       freq\n   &lt;chr&gt;    &lt;dbl&gt;\n 1 Ceratoz3 0.443\n 2 FSET     0.429\n 3 HMIN     0.486\n 4 MEGR     0.543\n 5 NCOR     0.5  \n 6 Oppiminu 0.429\n 7 Oribatl1 0.429\n 8 PWIL     0.486\n 9 TVEL     0.557\n10 Trhypch1 0.457\n\n## how about: PWIL \n\nLet’s choose just one species as an example. I’ve chosen one where the relationship with water is rather strong, and for which presence and absence are roughly balanced. This is just to make the example clear.\n\npwil_data &lt;- mite_data_long_transformed |&gt; \n  filter(spp == \"PWIL\")\n\npwil_data |&gt; \n  ggplot(aes(x = water, y = presabs)) + geom_point() + \n  stat_smooth(method = glm, method.args = list(family = \"binomial\")) + \n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nProbability of occurrance of one mite species, as a fuction of water content of the soil\n\n\n\n\n\n# add the spatial coordinates:\n\npwil_spatial &lt;- bind_cols(pwil_data, mite.xy)\n\npwil_spatial |&gt; \n  ggplot(aes(x = x, y = y, fill = as.factor(presabs))) + \n  geom_point(size = 3, pch = 21, stroke = 1) + \n  scale_fill_brewer(type = \"qual\", palette = \"Dark2\") + \n  theme_minimal() + \n  coord_fixed() + \n  labs(fill = \"Pres/Abs\")\n\n\n\n\nPresence-absence data for mite species “PWIL”, at the spatial location of each point.\n\n\n\n\nWe’ll look at two possibilities in turn:\n\nA nonlinear function of one variable\nA smooth function of distance"
  },
  {
    "objectID": "topics/04_gp/index.html#write-the-model",
    "href": "topics/04_gp/index.html#write-the-model",
    "title": "Gaussian Processes in Stan",
    "section": "Write the model",
    "text": "Write the model\n\\[\n\\begin{align}\n\\mathsf{Pr}(y_i = 1) &\\sim \\mathsf{Bernoulli}(p_i)\\\\\n\\mathsf{logit}(p_i) &= a + f_i\\\\\nf_i &\\sim \\mathsf{multivariate\\ normal}(0, K(x | \\theta)) \\\\\n  K(x | \\alpha, \\rho, \\sigma)_{i, j}\n&= \\alpha^2\n\\exp \\left(\n- \\dfrac{1}{2 \\rho^2} \\sum_{d=1}^D (x_{i,d} - x_{j,d})^2\n\\right)\n+ \\delta_{i, j} \\sigma^2,\n\\end{align}\n\\]\nThat’s the general notation for D dimensions. In our case we’re looking at something much simpler.\n\\[\n\\begin{align}\n\\mathsf{Pr}(y_i = 1) &\\sim \\mathsf{Bernoulli}(p_i)\\\\\n\\mathsf{logit}(p_i) &= a + f_i\\\\\nf_i &\\sim \\mathsf{Multivariate\\ Normal}(0, K(x | \\theta)) \\\\\n  K(x | \\alpha, \\rho, \\sigma)_{i, j}\n&= \\alpha^2\n\\exp \\left(\n- \\frac{(\\text{water}_i - \\text{water}_j)^2}{2 \\rho^2}\n\\right)\n+ \\delta_{i, j} \\sigma^2 \\\\\n\\rho &\\sim \\mathsf{Inverse\\ Gamma}(5, 14) \\\\\n\\alpha &\\sim \\mathsf{Normal}(0, .8) \\\\\na &\\sim \\mathsf{Normal}(0, .2) \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "topics/04_gp/index.html#simulate-to-understand-it",
    "href": "topics/04_gp/index.html#simulate-to-understand-it",
    "title": "Gaussian Processes in Stan",
    "section": "Simulate to understand it",
    "text": "Simulate to understand it\n\ngp_example_sim &lt;- cmdstan_model(stan_file = \"topics/04_gp/gp_example_sim.stan\")\n\ngp_example_sim\n\n// Fit the hyperparameters of a latent-variable Gaussian process with an\n// exponentiated quadratic kernel and a Bernoulli likelihood\n// This code is from https://github.com/stan-dev/example-models/blob/master/misc/gaussian-process/gp-fit-logit.stan\ndata {\n  int&lt;lower=1&gt; N;\n  array[N] real x;\n}\ntransformed data {\n  real delta = 1e-9;\n}\nparameters {\n  real&lt;lower=0&gt; rho;\n  real&lt;lower=0&gt; alpha;\n  real a;\n  vector[N] eta;\n  vector[N] y;\n}\ntransformed parameters {\n  vector[N] f;\n  {\n    matrix[N, N] L_K;\n    matrix[N, N] K = gp_exp_quad_cov(x, alpha, rho);\n    \n    // diagonal elements\n    for (n in 1 : N) {\n      K[n, n] = K[n, n] + delta;\n    }\n    \n    L_K = cholesky_decompose(K);\n    f = L_K * eta;\n  }\n}\nmodel {\n  rho ~ inv_gamma(5, 14);\n  alpha ~ normal(0, .8);\n  a ~ normal(0, .2);\n  eta ~ std_normal();\n}\n\n\n\ngp_example_sim_samples &lt;- gp_example_sim$sample(data = list(\n  N = 20,\n  x = seq(from = -3, to = 5, length.out = 20)),\n  refresh = 200, chains = 1, iter_sampling = 200\n)\n\ngp_example_sim_samples$save_object(\n  file = \"topics/04_gp/gp_example_sim_samples\")\n\n\ngp_example_sim_samples &lt;- read_rds(\"topics/04_gp/gp_example_sim_samples\")\n\nx_value_df &lt;- enframe(x = seq(from = -3, to = 5, length.out = 20),\n                      name = \"i\", value = \"water\")\n\ngp_example_sim_samples |&gt; \n  tidybayes::spread_draws(f[i], a, ndraws = 45) |&gt; \n  left_join(x_value_df) |&gt; \n  ggplot(aes(x = water, y = plogis(f + a), group = .draw)) + \n  geom_line() + \n  coord_cartesian(ylim = c(0, 1))\n\nJoining with `by = join_by(i)`"
  },
  {
    "objectID": "topics/04_gp/index.html#express-that-model-in-code",
    "href": "topics/04_gp/index.html#express-that-model-in-code",
    "title": "Gaussian Processes in Stan",
    "section": "Express that model in code",
    "text": "Express that model in code\n\ngp_example_pred &lt;- cmdstan_model(\n  stan_file = \"topics/04_gp/gp_example_pred.stan\")\n\ngp_example_pred\n\n// Fit the hyperparameters of a latent-variable Gaussian process with an\n// exponentiated quadratic kernel and a Bernoulli likelihood\n// This code is from https://github.com/stan-dev/example-models/blob/master/misc/gaussian-process/gp-fit-logit.stan\ndata {\n  int&lt;lower=1&gt; Nobs;\n  int&lt;lower=1&gt; N;\n  array[N] real x;\n  array[Nobs] int&lt;lower=0, upper=1&gt; z;\n}\ntransformed data {\n  real delta = 1e-9;\n}\nparameters {\n  real&lt;lower=0&gt; rho;\n  real&lt;lower=0&gt; alpha;\n  real a;\n  vector[N] eta;\n}\ntransformed parameters {\n  vector[N] f;\n  {\n    matrix[N, N] L_K;\n    matrix[N, N] K = gp_exp_quad_cov(x, alpha, rho);\n    \n    // diagonal elements\n    for (n in 1 : N) {\n      K[n, n] = K[n, n] + delta;\n    }\n    \n    L_K = cholesky_decompose(K);\n    f = L_K * eta;\n  }\n}\nmodel {\n  rho ~ inv_gamma(5, 14);\n  alpha ~ normal(0, .8);\n  a ~ normal(0, .2);\n  eta ~ std_normal();\n  \n  z ~ bernoulli_logit(a + f[1:Nobs]);\n}\n\n\nWe need to generate data for making predictions! I’ll create a new vector of observations called new_x that cover the range of the water variable in our dataset.\n\n# sample N values on the range of x\nnew_x &lt;- seq(from = -3, to = 5, length.out = 15)\n\n# put them on the dataframe\ngp_example_samp &lt;- gp_example_pred$sample(\n  data = list(N = length(pwil_spatial$presabs) + length(new_x),\n              Nobs = length(pwil_spatial$presabs),\n              x = c(pwil_spatial$water, new_x),\n              z = pwil_spatial$presabs),\n  chains = 2, parallel_chains = 2, refresh = 1000)\n\ngp_example_samp$save_object(\"topics/04_gp/gp_example_samp_pwil.rds\")\n\n\n\n\n\n\n\nTip\n\n\n\nNote that cmdstanr models have a method called $save_object(), which lets you save the model outputs into an .rds object.\n\n\n\n# sample N values on the range of x\nnew_x &lt;- seq(from = -3, to = 5, length.out = 15)\n\ngp_example_samp_pwil &lt;- read_rds(\n  \"topics/04_gp/gp_example_samp_pwil.rds\")\n\nwater_prediction_points &lt;- gp_example_samp_pwil |&gt; \n  tidybayes::gather_rvars(f[rownum]) |&gt; \n  slice(-(1:length(pwil_spatial$presabs)))\n\nwater_prediction_points |&gt; \n  mutate(water = new_x,\n         presabs = posterior::rfun(plogis)(.value)) |&gt; \n  ggplot(aes(x = water, dist = presabs)) + \n  tidybayes::stat_lineribbon() + \n  # scale_fill_viridis_d(option = \"rocket\") + \n  scale_fill_brewer(palette = \"Reds\", direction  = -1) + \n  geom_jitter(aes(x = water, y = presabs), \n              inherit.aes = FALSE, \n              height = .01, width = 0,\n              data = pwil_spatial)\n\n\n\n\n\n\n\nggsave(\"topics/04_gp/pwil_water.png\")\n\nSaving 7 x 5 in image\n\n\n\nWe can also pull out some specific functions. What I want you to see here is that there are MANY curvy lines that are consistent with this model.\n\nsome_predicted_lines &lt;-  gp_example_samp_pwil |&gt; \n  # take just some draws\n  tidybayes::spread_draws(a, f[rownum], ndraws = 63) |&gt; \n  # remove the rows that match observed data,\n  # and look only at the points for predictions.\n  filter(rownum &gt; length(pwil_spatial$presabs)) |&gt; \n  # convert to probability\n  mutate(prob = plogis(f + a),\n         rownum = rownum - 70) |&gt; \n  ## need a dataframe that says which \"rownum\" from \n  ## above goes with which value of water from the\n  ## new_x vector I made:\n  left_join(tibble::enframe(new_x,\n                            name = \"rownum\", \n                            value = \"water\"))\n\nJoining with `by = join_by(rownum)`\n\nsome_predicted_lines |&gt; \n  ggplot(aes(x = water, y = prob, group = .draw)) + \n  geom_line(alpha = 0.7) + \n  theme_minimal() + \n  coord_cartesian(ylim = c(0, 1))"
  },
  {
    "objectID": "topics/04_gp/index.html#extensions",
    "href": "topics/04_gp/index.html#extensions",
    "title": "Gaussian Processes in Stan",
    "section": "Extensions:",
    "text": "Extensions:\nAdd water to the model. Does the spatial effect disappear, increase, or stay kind of the same?\nNext step: try to model water curve for more than one species. Would it be possible to make the species rho parameters hierarchical?"
  },
  {
    "objectID": "topics/intercept_only/index.html",
    "href": "topics/intercept_only/index.html",
    "title": "Fitting an intercept-only model",
    "section": "",
    "text": "tidybayes is an incredible tool, and the vignette is a great read for visualization approaches (even if you aren’t using rvars)\nthe posterior package is the best place to learn about how to manipulate Stan posterior distributions.\n\n\\[\n\\begin{align}\n\\text{Abundance}_i &\\sim \\text{Poisson}(\\lambda_i) \\\\\n\\log{\\lambda_i} &\\sim \\mu + \\beta_{\\text{sample}[i]} + \\beta_{\\text{species[i]}} + \\beta_i\\\\\n\\mu &\\sim \\text{Normal}(3, 1)\\\\\n\\beta_{\\text{sample}} &\\sim \\text{Normal}(0,  \\sigma_{\\text{samp}})\\\\\n\\beta_{\\text{species}} &\\sim \\text{Normal}(0, \\sigma_{\\text{species}})\\\\\n\\beta_i &\\sim \\text{Normal}(0,                \\sigma_{\\text{obs}}) \\\\\n\\sigma_{\\text{samp}}    &\\sim \\text{Exponential}(3)\\\\\n\\sigma_{\\text{species}} &\\sim \\text{Exponential}(3)\\\\\n\\sigma_{\\text{obs}}     &\\sim \\text{Exponential}(3)\n\\end{align}\n\\]\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\nlibrary(tidybayes)\n\ndata(\"mite\", package = \"vegan\")\n\nspp_names &lt;- colnames(mite)\nspp_names &lt;- setNames(1:ncol(mite), colnames(mite))\n\n\nmite_long &lt;- mite |&gt; \n  mutate(site_id = seq_len(nrow(mite))) |&gt; \n  tidyr::pivot_longer(-site_id,\n                      names_to = \"spp\",\n                      values_to = \"abd\") |&gt; \n  dplyr::mutate(spp_id = spp_names[spp])\n\n\nlibrary(cmdstanr)\n\nspp_site_obs_intercepts &lt;- cmdstan_model(\"topics/intercept_only/spp_site_obs_intercepts.stan\", \n                                  pedantic = TRUE)\nspp_site_obs_intercepts\n\ndata{\n  int N;\n  int N_spp;\n  array[N] int&lt;lower=1,upper=N_spp&gt; spp_id;\n  int N_sites;\n  array[N] int&lt;lower=1,upper=N_sites&gt; site_id;\n  array[N] int abd;\n}\nparameters{\n  vector[N_spp] spp_effects;\n  vector[N_sites] site_effects;\n  vector[N] obs_effects;\n  real mu;\n  real&lt;lower=0&gt; sigma_spp;\n  real&lt;lower=0&gt; sigma_sites;\n  real&lt;lower=0&gt; sigma_obs;\n}\nmodel {\n  abd ~ poisson_log(mu + spp_effects[spp_id] + site_effects[site_id] + obs_effects);\n  spp_effects ~ normal(0, sigma_spp);\n  site_effects ~ normal(0, sigma_sites);\n  obs_effects ~ normal(0, sigma_obs);\n  mu ~ normal(3, 1);\n  sigma_spp ~ exponential(3);\n  sigma_sites ~ exponential(3);\n  sigma_obs ~ exponential(3);\n}\n\n\nNow we can sample this model.\n\n\n\n\n\n\nWarning: irresponsible statistics\n\n\n\nI’m sampling only 2 chains below, for illustration purposes only! use more chains in your research.\n\n\n\nspp_site_obs_intercepts_samp &lt;- spp_site_obs_intercepts$sample(\n  data = list(\n    N = nrow(mite_long),\n    N_spp = max(mite_long$spp_id),\n    spp_id = mite_long$spp_id,\n    N_sites = max(mite_long$site_id),\n    site_id = mite_long$site_id,\n    abd = mite_long$abd\n  ),chains = 2, parallel_chains = 2)\n\nspp_site_obs_intercepts_samp$save_object(\"topics/intercept_only/spp_site_obs_intercepts.rds\")\n\n\nspp_site_obs_intercepts_samp &lt;- read_rds(\"topics/intercept_only/spp_site_obs_intercepts.rds\")"
  },
  {
    "objectID": "topics/intercept_only/index.html#resources",
    "href": "topics/intercept_only/index.html#resources",
    "title": "Fitting an intercept-only model",
    "section": "",
    "text": "tidybayes is an incredible tool, and the vignette is a great read for visualization approaches (even if you aren’t using rvars)\nthe posterior package is the best place to learn about how to manipulate Stan posterior distributions.\n\n\\[\n\\begin{align}\n\\text{Abundance}_i &\\sim \\text{Poisson}(\\lambda_i) \\\\\n\\log{\\lambda_i} &\\sim \\mu + \\beta_{\\text{sample}[i]} + \\beta_{\\text{species[i]}} + \\beta_i\\\\\n\\mu &\\sim \\text{Normal}(3, 1)\\\\\n\\beta_{\\text{sample}} &\\sim \\text{Normal}(0,  \\sigma_{\\text{samp}})\\\\\n\\beta_{\\text{species}} &\\sim \\text{Normal}(0, \\sigma_{\\text{species}})\\\\\n\\beta_i &\\sim \\text{Normal}(0,                \\sigma_{\\text{obs}}) \\\\\n\\sigma_{\\text{samp}}    &\\sim \\text{Exponential}(3)\\\\\n\\sigma_{\\text{species}} &\\sim \\text{Exponential}(3)\\\\\n\\sigma_{\\text{obs}}     &\\sim \\text{Exponential}(3)\n\\end{align}\n\\]\n\nlibrary(tidyverse)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\nlibrary(tidybayes)\n\ndata(\"mite\", package = \"vegan\")\n\nspp_names &lt;- colnames(mite)\nspp_names &lt;- setNames(1:ncol(mite), colnames(mite))\n\n\nmite_long &lt;- mite |&gt; \n  mutate(site_id = seq_len(nrow(mite))) |&gt; \n  tidyr::pivot_longer(-site_id,\n                      names_to = \"spp\",\n                      values_to = \"abd\") |&gt; \n  dplyr::mutate(spp_id = spp_names[spp])\n\n\nlibrary(cmdstanr)\n\nspp_site_obs_intercepts &lt;- cmdstan_model(\"topics/intercept_only/spp_site_obs_intercepts.stan\", \n                                  pedantic = TRUE)\nspp_site_obs_intercepts\n\ndata{\n  int N;\n  int N_spp;\n  array[N] int&lt;lower=1,upper=N_spp&gt; spp_id;\n  int N_sites;\n  array[N] int&lt;lower=1,upper=N_sites&gt; site_id;\n  array[N] int abd;\n}\nparameters{\n  vector[N_spp] spp_effects;\n  vector[N_sites] site_effects;\n  vector[N] obs_effects;\n  real mu;\n  real&lt;lower=0&gt; sigma_spp;\n  real&lt;lower=0&gt; sigma_sites;\n  real&lt;lower=0&gt; sigma_obs;\n}\nmodel {\n  abd ~ poisson_log(mu + spp_effects[spp_id] + site_effects[site_id] + obs_effects);\n  spp_effects ~ normal(0, sigma_spp);\n  site_effects ~ normal(0, sigma_sites);\n  obs_effects ~ normal(0, sigma_obs);\n  mu ~ normal(3, 1);\n  sigma_spp ~ exponential(3);\n  sigma_sites ~ exponential(3);\n  sigma_obs ~ exponential(3);\n}\n\n\nNow we can sample this model.\n\n\n\n\n\n\nWarning: irresponsible statistics\n\n\n\nI’m sampling only 2 chains below, for illustration purposes only! use more chains in your research.\n\n\n\nspp_site_obs_intercepts_samp &lt;- spp_site_obs_intercepts$sample(\n  data = list(\n    N = nrow(mite_long),\n    N_spp = max(mite_long$spp_id),\n    spp_id = mite_long$spp_id,\n    N_sites = max(mite_long$site_id),\n    site_id = mite_long$site_id,\n    abd = mite_long$abd\n  ),chains = 2, parallel_chains = 2)\n\nspp_site_obs_intercepts_samp$save_object(\"topics/intercept_only/spp_site_obs_intercepts.rds\")\n\n\nspp_site_obs_intercepts_samp &lt;- read_rds(\"topics/intercept_only/spp_site_obs_intercepts.rds\")"
  },
  {
    "objectID": "topics/intercept_only/index.html#exploring-the-model-output",
    "href": "topics/intercept_only/index.html#exploring-the-model-output",
    "title": "Fitting an intercept-only model",
    "section": "Exploring the model output",
    "text": "Exploring the model output\n\nspp_site_obs_intercepts_samp\n\n       variable     mean   median    sd   mad       q5      q95 rhat ess_bulk\n lp__           17755.93 17756.60 56.81 55.89 17661.36 17845.10 1.01      236\n spp_effects[1]     1.98     1.98  0.31  0.32     1.46     2.47 1.02      150\n spp_effects[2]    -0.45    -0.44  0.35  0.35    -1.04     0.09 1.01      228\n spp_effects[3]     2.16     2.15  0.32  0.32     1.63     2.68 1.01       99\n spp_effects[4]    -0.69    -0.69  0.35  0.36    -1.26    -0.11 1.01      162\n spp_effects[5]    -1.99    -2.00  0.41  0.41    -2.66    -1.32 1.00      410\n spp_effects[6]    -1.95    -1.94  0.42  0.42    -2.67    -1.26 1.01      247\n spp_effects[7]     0.25     0.24  0.32  0.31    -0.25     0.79 1.01      162\n spp_effects[8]    -2.36    -2.37  0.46  0.45    -3.10    -1.62 1.01      353\n spp_effects[9]    -0.66    -0.65  0.34  0.34    -1.25    -0.11 1.01      222\n ess_tail\n      516\n      342\n      469\n      217\n      506\n      702\n      786\n      383\n     1208\n      866\n\n # showing 10 of 2560 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nThe sampler was just fine! Note that we have just estimated many more parameters than observations."
  },
  {
    "objectID": "topics/intercept_only/index.html#plot-the-standard-deviations",
    "href": "topics/intercept_only/index.html#plot-the-standard-deviations",
    "title": "Fitting an intercept-only model",
    "section": "Plot the standard deviations",
    "text": "Plot the standard deviations\n\nsigma_post &lt;- spp_site_obs_intercepts_samp$draws(\n  variables = c(\"sigma_spp\", \"sigma_sites\", \"sigma_obs\"))\n\nsigma_post\n\n# A draws_array: 1000 iterations, 2 chains, and 3 variables\n, , variable = sigma_spp\n\n         chain\niteration   1   2\n        1 1.8 1.7\n        2 1.3 1.4\n        3 1.4 1.6\n        4 1.3 1.5\n        5 1.5 1.7\n\n, , variable = sigma_sites\n\n         chain\niteration    1    2\n        1 0.73 0.67\n        2 0.58 0.63\n        3 0.50 0.55\n        4 0.56 0.70\n        5 0.61 0.60\n\n, , variable = sigma_obs\n\n         chain\niteration   1   2\n        1 1.5 1.5\n        2 1.5 1.5\n        3 1.5 1.5\n        4 1.5 1.5\n        5 1.5 1.6\n\n# ... with 995 more iterations\n\n\nThe default output is a draws array\n\nsigma_post_df &lt;- spp_site_obs_intercepts_samp$draws(\n  variables = c(\"sigma_spp\", \"sigma_sites\", \"sigma_obs\"),\n  format = \"data.frame\")\n\nsigma_post_df\n\n# A draws_df: 1000 iterations, 2 chains, and 3 variables\n   sigma_spp sigma_sites sigma_obs\n1        1.8        0.73       1.5\n2        1.3        0.58       1.5\n3        1.4        0.50       1.5\n4        1.3        0.56       1.5\n5        1.5        0.61       1.5\n6        1.3        0.51       1.5\n7        1.6        0.48       1.5\n8        1.4        0.58       1.6\n9        1.5        0.61       1.6\n10       1.4        0.69       1.5\n# ... with 1990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nWe can also get a huge data frame. If you are comfortable manipulating data frames, you can use all your regular techniques here. Here I use tidyverse tools to reshape and plot the posterior distribution of the three \\(\\sigma\\) variables:\n\nsigma_post_df |&gt; \n  pivot_longer(starts_with(\"sigma\"), \n               names_to = \"sigma\", \n               values_to = \"value\") |&gt; \n  ggplot(aes(x = value)) + \n  geom_histogram() + \n  facet_wrap(~sigma)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nParameters from a posterior are NOT independent. If you want to combine parameters in any way (to calculate an average for example) you have to do it WITHIN each posterior sample. Work on ROWS of the draws data frame above\n\n\n\nCalculate the posterior distribution of average abundance for each species\nTo do this we need to extract the average \\(mu\\) and add it to the species effects, \\(\\beta_{\\text{species}}\\)\n\nspp_avg_effects_df &lt;- spp_site_obs_intercepts_samp$draws(\n  variables = c(\"mu\", \"spp_effects\"),\n  format = \"data.frame\")\n\n\nspp_avg_effects_df |&gt; \n  select(mu, starts_with(\"spp_effects\")) |&gt; \n  mutate(row_id = seq_along(mu)) |&gt; \n  pivot_longer(-c(\"mu\", \"row_id\"), \n               names_to = \"parname\", \n               values_to = \"spp_effect\") |&gt; \n  mutate(spp_avg = mu + spp_effect) |&gt; \n  ggplot(aes(x = spp_avg, group = parname)) + \n  geom_density()\n\nWarning: Dropping 'draws_df' class as required metadata was removed."
  },
  {
    "objectID": "topics/intercept_only/index.html#with-rvars",
    "href": "topics/intercept_only/index.html#with-rvars",
    "title": "Fitting an intercept-only model",
    "section": "with rvars",
    "text": "with rvars\nWe can do the same operation even more quickly by using some of the tools from tidybayes\n\nspp_mu_rvars &lt;- spp_site_obs_intercepts_samp |&gt; \n  tidybayes::spread_rvars(mu, spp_effects[spp_id])\n\n\nspp_mu_rvars |&gt; \n  mutate(spp_avg = mu + spp_effects) |&gt; \n  ggplot(aes(dist = spp_avg, group = spp_id)) + \n  tidybayes::stat_slab(col = \"black\") + \n  coord_flip() + \n  theme_minimal()"
  },
  {
    "objectID": "topics/discrete_predictor/index.html",
    "href": "topics/discrete_predictor/index.html",
    "title": "Palmer penguins and discrete predictors",
    "section": "",
    "text": "Let’s start by taking a look at the Palmer Penguin dataset. Let’s look at the distribution of observations of bill size.\nlibrary(tidyverse)\nlibrary(palmerpenguins)\npenguins |&gt; \n  ggplot(aes(x=bill_depth_mm)) + \n  geom_histogram(binwidth = .5)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nHistogram of bill depth for all the penguins in the Palmer Penguin dataset.\nThere’s quite a lot of variation in these measurements, with a suggestion of perhaps more than one peak in this distribution."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#a-simple-model",
    "href": "topics/discrete_predictor/index.html#a-simple-model",
    "title": "Palmer penguins and discrete predictors",
    "section": "A simple model",
    "text": "A simple model\n\\[\n\\begin{align}\n\\text{Bill depth} &\\sim \\text{Normal}(\\mu, \\sigma)\\\\\n\\mu &\\sim \\text{Normal}(17.5, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(1) \\\\\n\\end{align}\n\\]\nlet’s express the same model in Stan:\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\nnormal_dist &lt;- cmdstan_model(\"topics/discrete_predictor/normal_dist.stan\")\nnormal_dist\n\ndata {\n  int N;\n  vector[N] measurements;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  // priors\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n  // likelihood\n  measurements ~ normal(mu, sigma);\n}\n\n\nThe model section looks very much like the mathematics shown above. I want you to notice especially how the bottom chunk has three lines, each describing a probability distribution. Models are devices for putting together the probability of all the quantities we are looking for. Again, a Bayesian separates the world into unmeasured or measured quantities – and above we state which are observed (the data block) and which are unobserved (the parameters block).\nWe can fit this model to data and see the result:\n\n# first we drop all NA values\npenguins_nobillNA &lt;- penguins |&gt; \n  #drop NA values\n  filter(!is.na(bill_depth_mm))\n\n## then we assemble the data as a list.\n## I'm using the base function with()\n##  it lets me use the variable name directly \n## without writing penguins_nobillNA$bill_depth_mm\n\nlist_bill_dep &lt;- with(penguins_nobillNA,\n     list(N = length(bill_depth_mm),\n          measurements = bill_depth_mm))\n     \n## sample 4 chains, suppress counting iterations\nsamp_bill_dep &lt;- normal_dist$sample(data = list_bill_dep, \n                                    parallel_chains = 4,\n                                    refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\n## summarize the samples for each parameter into a nice table\nsamp_bill_dep |&gt; \n  posterior::summarise_draws() |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nlp__\n-405.514525\n-405.208500\n0.9977619\n0.6990459\n-407.501000\n-404.566000\n1.001842\n1615.835\n2281.705\n\n\nmu\n17.152647\n17.152050\n0.1060854\n0.1043009\n16.980995\n17.330930\n1.000769\n3300.521\n2599.964\n\n\nsigma\n1.978273\n1.975145\n0.0762528\n0.0761019\n1.857754\n2.105638\n1.001583\n3264.434\n2693.953"
  },
  {
    "objectID": "topics/discrete_predictor/index.html#plotting-parameters.",
    "href": "topics/discrete_predictor/index.html#plotting-parameters.",
    "title": "Palmer penguins and discrete predictors",
    "section": "Plotting parameters.",
    "text": "Plotting parameters.\nWe don’t have one distribution for each of our unknown numbers: we have thousands. We need to get a sense of what these possible values mean scientifically. An excellent way to do this is by making as many pictures as possible. We will start with making plots of specific parameters.\nWe can look at the distributions easily using the bayesplot package.\ndraws &lt;- samp_bill_dep$draws(variables = c(\"mu\", \"sigma\"))\n\nbayesplot::mcmc_hist(draws, pars = \"mu\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\nbayesplot::mcmc_hist(draws, pars = \"sigma\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nNotice that the distributions do not have the same shape as the prior– this is particularly true for \\(\\sigma\\). This shows an important point: the prior distribution does not determine what the posterior looks like. should I sample from the prior and show them that?\n\nlibrary(ggplot2)\nlibrary(ggdist)\n\ndraws |&gt;  \n  posterior::as_draws_df() |&gt; \n  ggplot(aes(x = sigma)) + \n  stat_dotsinterval()\n\n\n\n\n\n\n\n\nFigure 1: the package ggdist has many fun & useful ways to draw pictures of posterior distributions. Here is one called stats_dotsinterval()"
  },
  {
    "objectID": "topics/discrete_predictor/index.html#posterior-predictions-the-easy-way-to-check-your-model",
    "href": "topics/discrete_predictor/index.html#posterior-predictions-the-easy-way-to-check-your-model",
    "title": "Palmer penguins and discrete predictors",
    "section": "Posterior predictions: the easy way to check your model",
    "text": "Posterior predictions: the easy way to check your model\nPeople care so much about model diagnostics. And with good reason: you need to know how much to trust a model before using it to make a scientific claim. One way to find out which model is best would be to use them to make a prediction, and see how right you are. An alternative is to see how well the data fit your sample.\n\nPosterior prediction in R\n\n# just get some draws\ndraws &lt;- samp_bill_dep$draws(variables = c(\"mu\", \"sigma\"))\ndraws_matrix &lt;- posterior::as_draws_matrix(draws)\n\n## set up a matrix. for every posterior sample, \n## (that is, for a value of mu and a value of sigma) \n## draw a whole fake dataset from a normal distribution with that mean and sd. \nnsamples &lt;- 50\nyrep &lt;- matrix(0, ncol = list_bill_dep$N, nrow = nsamples)\n\n# pick some random rows\nset.seed(1234)\nchosen_samples &lt;- sample(1:nrow(draws_matrix), replace = FALSE, size = nsamples)\nsubset_draws &lt;- draws_matrix[chosen_samples,]\n\nfor (r in 1:nsamples){\n yrep[r,] &lt;- rnorm(n = list_bill_dep$N, \n                   mean = subset_draws[r, \"mu\"], \n                   sd = subset_draws[r, \"sigma\"])\n}\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = yrep)\n\n\n\n\n\n\n\n\n\n\nPosterior predictions in Stan\n\n\n\n\n\n\nEXERCISE\n\n\n\nIn the code above I show how to simulate the posterior predictive distribution using the posterior draws for \\(\\mu\\) and \\(\\sigma\\). However, if you want, you can do the same process in Stan.\n\nExtend the Stan code above to simulate new observations using the generated quantities block. (Tip: look back at the Simulation exercise).\n\nplot them using bayesplot, as above.\n\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nnormal_dist_rng &lt;- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng.stan\")\n\nnormal_dist_rng\n\ndata {\n  int N;\n  vector[N] measurements;\n}\nparameters {\n  real mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  measurements ~ normal(mu, sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu, sigma);\n  }\n}\n\n\nHere we have a handy random number generator inside Stan.\n\nsamp_bill_dep_rng &lt;- normal_dist_rng$sample(\n  data = list_bill_dep,\n  refresh = 0,\n  parallel_chains = 4)\n\nRunning MCMC with 4 parallel chains...\n\n\nChain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n\n\nChain 2 Exception: normal_lpdf: Scale parameter is 0, but must be positive! (in '/tmp/RtmpYRTfHe/model-e4812a51a851.stan', line 10, column 2 to column 35)\n\n\nChain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n\n\nChain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n\n\nChain 2 \n\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.3 seconds.\n\ndraws &lt;- samp_bill_dep_rng$draws(variables = c(\"yrep\"))\ndraws_matrix &lt;- posterior::as_draws_matrix(draws)\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = head(draws_matrix, 50))\n\n\n\n\n\n\n\n\nThe code is much shorter, because there is less to do in R.\n\n\n\nBoth of these gives the same outcome: the posterior predictive distribution. This gives us a straightfoward way to test our model’s performance:\n\nwe use the model to generate fake observations.\nplot these on top of the real data\nif the data is a really poor match, we know our model has a distorted view of the world."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#different-groups-are-different",
    "href": "topics/discrete_predictor/index.html#different-groups-are-different",
    "title": "Palmer penguins and discrete predictors",
    "section": "Different groups are different",
    "text": "Different groups are different\nlet’s add in differences among species\n\npenguins |&gt; \n  ggplot(aes(x = bill_depth_mm, fill = species))+ \n  geom_histogram(binwidth = .5) + \n  scale_fill_brewer(palette = \"Dark2\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNow we can see that the distribution is in fact three different shapes, all placed together.\n\n\n\n\n\n\nWarning\n\n\n\nSometimes scientists will plot histograms of data at the beginning of a research project, and use the histogram to decide if their data are “normally distributed” or not. This is not helpful! Instead, decide on a model first, and ask yourself what kind of data you expect."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#stan-code-for-species-differences",
    "href": "topics/discrete_predictor/index.html#stan-code-for-species-differences",
    "title": "Palmer penguins and discrete predictors",
    "section": "Stan code for species differences",
    "text": "Stan code for species differences\n\\[\n\\begin{align}\n\\text{Bill depth}_{i} &\\sim \\text{Normal}(\\mu_{\\text{sp}[i]}, \\sigma) \\\\\n\\mu_{\\text{sp}} &\\sim \\text{Normal}(17, 2) \\\\\n\\sigma &\\sim \\text{Exponential}(2) \\\\\n\\end{align}\n\\]\n\nnormal_dist_rng_spp_forloop &lt;- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng_spp_forloop.stan\")\n\nnormal_dist_rng_spp_forloop\n\ndata {\n  int N;\n  vector[N] measurements;\n  array[N] int&lt;lower=1,upper=3&gt; spp_id;\n}\nparameters {\n  vector[3] mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  for (i in 1:N){\n    measurements[i] ~ normal(mu[spp_id[i]], sigma);\n  }\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu[spp_id[i]], sigma);\n  }\n}\n\n\nThere’s a few differences to notice here:\n\nin the data block: We have a new input! A declaration of the array of integers at the top, saying if this is “species 1”, “species 2”, or “species 3”\nmu is a vector now. why?\nnotice the for-loop."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#quick-detour-vector-indexing",
    "href": "topics/discrete_predictor/index.html#quick-detour-vector-indexing",
    "title": "Palmer penguins and discrete predictors",
    "section": "Quick detour : vector indexing",
    "text": "Quick detour : vector indexing\nA very useful technique, in both R and Stan, is transforming a vector with indexing. Vector indexing requires two vectors: the first contains values we want to select or replicate, the second contains integers giving the positions of the elements we want. For example:\n\nsome_values &lt;- c(\"taco\", \"cat\", \"goat\", \"cheeze\", \"pizza\")\npositions &lt;- c(1,1,2,2,3,1,1,5)\n\nsome_values[positions]\n\n[1] \"taco\"  \"taco\"  \"cat\"   \"cat\"   \"goat\"  \"taco\"  \"taco\"  \"pizza\"\n\n\nThis works for number values as well, and is very useful when you want to do simulations! let’s simulate three groups with different averages.\n\nset.seed(525600)\nsome_means &lt;- c(12, 17, 19)\nsome_labels &lt;- c(\"taco\", \"cat\", \"goat\")\n\ndf_of_means &lt;- data.frame(index = rep(1:3, each = 42)) |&gt; \n  mutate(the_mean = some_means[index],\n         labels = some_labels[index],\n         obs = rnorm(n = length(the_mean),\n                     mean = the_mean,\n                     sd = 1))\n\ndf_of_means |&gt; \n  ggplot(aes(x = obs, fill = labels)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "topics/discrete_predictor/index.html#vector-indexing-in-stan",
    "href": "topics/discrete_predictor/index.html#vector-indexing-in-stan",
    "title": "Palmer penguins and discrete predictors",
    "section": "Vector indexing in Stan",
    "text": "Vector indexing in Stan\nWe can use this precise technique in Stan:\n\nnormal_dist_rng_spp &lt;- cmdstan_model(stan_file = \"topics/discrete_predictor/normal_dist_rng_spp.stan\")\n\nnormal_dist_rng_spp\n\ndata {\n  int N;\n  vector[N] measurements;\n  array[N] int&lt;lower=1,upper=3&gt; spp_id;\n}\nparameters {\n  vector[3] mu;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  measurements ~ normal(mu[spp_id], sigma);\n  mu ~ normal(17,2);\n  sigma ~ exponential(1);\n}\ngenerated quantities{\n  vector[N] yrep;\n  for (i in 1:N){\n    yrep[i] = normal_rng(mu[spp_id[i]], sigma);\n  }\n}\n\n\nThe only difference to the previous model is in the line with the for-loop, which is now replaced with a vectorized expression. This is faster to write and will run faster in Stan. However it’s not possible in every case.\n\nSampling the species model\n\n\n\n\n\n\nEXERCISE\n\n\n\nFit one (or both) of the species-specific models above. 1. What changes do you need to make to the input data? Remember we’ve added a new input: a vector of numbers 1, 2, or 3 that tells us if we are working with the first, second, or third species. There are many ways to do this (e.g. try using as.numeric after as.factor)\n2. Visualize the posterior with bayesplot. Does it look better than the model without species? How can you tell?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nlist_bill_dep_spp &lt;- with(penguins_nobillNA,\n     list(\n       N = length(bill_depth_mm),\n       measurements = bill_depth_mm,\n       spp_id = as.numeric(as.factor(species))\n     )\n)\n     \nsamp_normal_dist_rng_spp &lt;- normal_dist_rng_spp$sample(\n  data = list_bill_dep_spp, \n  parallel_chains = 4,\n  refresh = 0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.3 seconds.\n\nsamp_normal_dist_rng_spp$draws(variables = c(\"mu\", \"sigma\")) |&gt; \n  posterior::summarise_draws() |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nmu[1]\n18.342111\n18.343000\n0.0903448\n0.0902903\n18.193885\n18.49411\n1.0009652\n4500.886\n2992.683\n\n\nmu[2]\n18.414473\n18.414200\n0.1381799\n0.1374370\n18.186955\n18.64132\n1.0007758\n4798.435\n3080.274\n\n\nmu[3]\n14.985503\n14.985300\n0.1013407\n0.1014840\n14.818200\n15.15663\n1.0010026\n4151.403\n3283.383\n\n\nsigma\n1.124076\n1.121725\n0.0434414\n0.0427434\n1.055824\n1.19891\n0.9997731\n4879.822\n3210.395\n\n\n\n\n\nLet’s take a look at this in Shinystan\n\nshinystan::launch_shinystan(samp_normal_dist_rng_spp)\n\nand we can repeat the posterior checking from before:\n\nspp_yrep_draws &lt;- samp_normal_dist_rng_spp$draws(variables = c(\"yrep\"))\nspp_draws_matrix &lt;- posterior::as_draws_matrix(spp_yrep_draws)\n\nbayesplot::ppc_dens_overlay(y = list_bill_dep$measurements,\n                            yrep = head(spp_draws_matrix, 50))\n\n\n\n\n\n\n\n\nThe predicted distribution is now much more like the real data\n\n\n\n\n\nVisualizing species – using tidybayes\nWe can also make figures for each individual species. Here we will move away from using bayesplot and try to visualize our posterior using the handy functions in the tidybayes package add a link\n\nlibrary(tidybayes)\nspp_draws_df &lt;- posterior::as_draws_df(spp_yrep_draws)\n\nnormal_dist_post_samp &lt;- tidybayes::gather_draws(samp_normal_dist_rng_spp,\n                        yrep[row_id], \n                        ndraws = 50)\n\nnormal_dist_post_samp |&gt; \n  mutate(species = penguins_nobillNA$species[row_id]) |&gt; \n  ggplot(aes(x = .value, colour = species)) + \n  geom_density(aes(group = .iteration), alpha = .1) + \n  facet_wrap(~species) + \n  geom_density(aes(x = bill_depth_mm),\n               data = penguins_nobillNA,\n               colour = \"black\") + \n  scale_colour_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n\n\n\nExercises\n\nLevel 1\n\nrepeat this experience for another variable in the dataset. Does the same code work on bill length? What about body size? What would you change about the model (if anything)\nuse bayesplot to examine the fit of body size to these data.\n\n\n\nLevel 2\n\ngenerate some random groups of your own, with known means. How well does the model fit these data\nThe present model is fixed for exactly 3 groups. how would you change it for any number of groups?\n\n\n\nLevel 3\n\nthe function tidybayes::compose_data is a convenient way to set up your data for passing it into R. Try out this function. What does it produce for our dataset? How do you need to modify our Stan program so that it works for the output of tidybayes::compose_data?\nAs you can see, the model assumes the same sigma for all species. what if you relax this?\n\n\n\n\nOptional!\nTry this on your own data!"
  },
  {
    "objectID": "topics/schedule/index.html",
    "href": "topics/schedule/index.html",
    "title": "Modèles hiérarchiques pour les sciences de la vie",
    "section": "",
    "text": "7h30 to 8h30 - Breakfast\n\n8h30 to 10h00 - Lecture/Practice\n\n10h00 to 10h30 - Break\n\n10h30 to 12h30 - Lecture/Practice\n\n12h00 to 13h00 - Lunch\n\n13h30 to 15h00 - Lecture/Practice\n\n15h30 to 16h00 - Break\n\n16h00 to 17h00 - Lecture/Practice\n17h00 to 17h30 - Let us know about your research !\n\n17h30 to 18h30 - Break\n\n\n18h30 to 19h00 - Supper"
  },
  {
    "objectID": "about.html#cibles-de-formation",
    "href": "about.html#cibles-de-formation",
    "title": "Advanced Statistics and hierarchical models",
    "section": "Cibles de formation",
    "text": "Cibles de formation\nApprendre la théorie statistique pour mieux construire, appliquer et interpréter différents modèles statistiques appliqués à des systèmes biologiques. Devenir familier avec la recherche primaire en en modélisation statistique de systèmes biologiques. Développer des connaissances et gagner de l’expérience à travailler de façon collaborative sur des problématiques lié au développement et à l’application de méthodes statisiques.\nThis course will feature hierarchical models and spatial statistics."
  },
  {
    "objectID": "about.html#course-instructors",
    "href": "about.html#course-instructors",
    "title": "Advanced Statistics and hierarchical models",
    "section": "Course Instructors",
    "text": "Course Instructors\n\n\n\n\n\n\n\n\nDr Guillaume Blanchet\n\n\n\n\n\n\n\nDr Andrew MacDonald"
  },
  {
    "objectID": "topics/visualization_priors.html",
    "href": "topics/visualization_priors.html",
    "title": "Visualizing priors",
    "section": "",
    "text": "Bayesian models\n\\[\n\\begin{align}\nF_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\n\\text{log}(\\lambda_i) &= \\alpha + \\beta x \\\\\n\\alpha & \\sim \\text{Normal}(??, ??) \\\\\n\\beta & \\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\n\n\n\nBayesian models\n\\[\n\\begin{align}\nF_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\n\\text{log}(\\lambda_i) &= \\alpha + \\beta x \\\\\n\\alpha & \\sim \\text{Normal}(0, 1000) \\\\\n\\beta & \\sim \\text{Normal}(0, 1000)\n\\end{align}\n\\]\n.footnote[as seen in Kéry & Royle 2016 p 188]\n\n.footnote[ Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. « Visualization in Bayesian Workflow ». Journal of the Royal Statistical Society: Series A (Statistics in Society) 182 (2): 389‑402. https://doi.org/10.1111/rssa.12378.]\n\n\n\nwhat does this prior mean?\n\\[\n\\begin{align}\nF_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\n\\text{log}(\\lambda_i) &= \\alpha + \\beta x \\\\\n\\alpha & \\sim \\text{Normal}(0, 1000) \\\\\n\\beta & \\sim \\text{Normal}(0, 1000)\n\\end{align}\n\\]\n\n\n\nwhat does this prior mean?\nlets do an example about my favourites: fly larvae!\n.pull-left[ ]\n.pull-right[ \\[\n\\begin{align}\nF_i & \\sim \\text{Poisson}(\\lambda_i) \\\\\n\\text{log}(\\lambda_i) &= \\alpha + \\beta x \\\\\n\\alpha & \\sim \\text{Normal}(0, 1000) \\\\\n\\beta & \\sim \\text{Normal}(0, 1000)\n\\end{align}\n\\] where \\(x\\) is some kind of standardized environmental variable]\n\n\n\nA quick look at the math for those curious\n\\[\n\\begin{align}\n\\text{log}(\\lambda) &= \\alpha + \\beta x \\\\\n\\lambda &= e^{\\alpha + \\beta x} \\\\\n\\lambda &= e^{\\alpha}e^{\\beta x} \\\\\n\\end{align}\n\\]\nAnd since \\(N(0,1000)\\) implies that -1000 and +1000 are entirely reasonable..\n.pull-left[ * \\(2.72^{-1000}\\times 2.72^{-1000}\\) * (practically zero)]\n.pull-right[ * \\(2.72^{1000}\\times 2.72^{1000}\\) * (..kind of a lot)]\n\n\n\neither no flies at all.. or a huge planet of maggots\n\nfor comparison, the planet Saturn weighs 5.7 * 10^29 grams\n\n\n\nsyntax: brms\n# define formula\n\ninsects_bf &lt;- bf(\nabundance ~ 1 + env, \nfamily = poisson()\n)\n\n\n\nsyntax: setting priors\nget_prior(insects_bf, data = insect_data)\n\ninsect_priors &lt;- c(\n  prior(normal(0,100), class = \"b\", coef = \"env\"),\n  prior(normal(0,100), class = \"Intercept\")\n)\n\n\n\nsyntax: sampling the model\ninsect_samples &lt;- brm(insects_bf,\n                      data = insect_data,  # does nothing\n                      prior = insect_priors,\n                      sample_prior = \"only\")\n\n\n\nExercise I – Fly larvae\n\nsee the file insects.R on github\non your own or in a small group, experiment with setting priors for insect abundances.\nAND/OR consider an animal or plant which is more relevant to you\nor stay with insects even if that is not your specialty, and set “vague” priors!\n\n\n\n\nexercises – les poissons\nusing data from Kaggle, about fish\nlibrary(readr)\nfish &lt;- read_csv(\"https://raw.githubusercontent.com/aammd/ISEC_stan_course/master/Fish.csv\")\n\nknitr::kable(head(fish), format = 'html')\n\n\n\n\nSpecies\nWeight\nLength1\nLength2\nLength3\nHeight\nWidth\n\n\n\n\nBream\n242\n23.2\n25.4\n30.0\n11.5200\n4.0200\n\n\nBream\n290\n24.0\n26.3\n31.2\n12.4800\n4.3056\n\n\nBream\n340\n23.9\n26.5\n31.1\n12.3778\n4.6961\n\n\nBream\n363\n26.3\n29.0\n33.5\n12.7300\n4.4555\n\n\nBream\n430\n26.5\n29.0\n34.0\n12.4440\n5.1340\n\n\nBream\n450\n26.8\n29.7\n34.7\n13.6024\n4.9274"
  },
  {
    "objectID": "topics/01_simulation/index.html",
    "href": "topics/01_simulation/index.html",
    "title": "Introduction to Stan and simulation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(rstan)\nrstan_options(\"auto_write\" = TRUE)"
  },
  {
    "objectID": "topics/01_simulation/index.html#simple-exercise-in-simulation",
    "href": "topics/01_simulation/index.html#simple-exercise-in-simulation",
    "title": "Introduction to Stan and simulation",
    "section": "Simple exercise in simulation",
    "text": "Simple exercise in simulation\nLet’s imagine we are taking a walk as a group today at this beautiful field site. What is the number of birds (total abundance of ALL species) each of us is going to see on our hike?\n\nSome questions to ask about simulated data\n\nWhat kind of observations are you going to make? Do they have a minimum or maximum value? Are they integers, or are they decimal numbers, or something else?\nWhere do the numbers come from? This could be anything, from simple linear approximations (i.e. the models we’re looking at in this course) to ODEs, mathematical models, GAMs, etc.\nHow many observations will we be making?\n\nOne of the most useful traits of Bayesian models is that they are generative: they can be used to make a simulated dataset. We’ll do that now for our bird example.\nlet’s simulate from a Poisson distribution:\n\nset.seed(525600)\nn_people &lt;- 21\navg_birds_per_person &lt;- runif(1, min = 0, max = 30)\nbird_count &lt;- rpois(n_people, lambda = avg_birds_per_person)\n\nSome things to note in the code above:\nEvery statistical distribution that is in R (which is a lot! almost all! ) has four different functions. If the distribution is called dist, then they are:\n\nrdist = draw random numbers from dist\nqdist = the quantile function – what value gives a certain proportion of the distribution?\npdist = the probability density function – what proportion of the distribution is below a certain value?\nddist the density function = draws the “shape” of a distribution. How probable are specific values?\n\nThe other thing to note is that there are TWO simulation steps here: first, simulating a value of the average (\\(\\lambda\\)) and second, simulating observations. In our model, the Uniform distribution was referred to as the prior, and the Poisson distribution was referred to as a likelihood, but here you can see that they are very nearly the same thing: just statements about what distribution of values might be most consistent with the data.\n\nPlotting the result\nLet’s take a look at our simulated values:\n\nhist(bird_count, col = \"lightblue\", xlim = c(0, 50))\n\n\n\n\nHistogram of simulated counts of birds\n\n\n\n\nThis is pretty great, and represents one possible realization of sampling. However, one sample isn’t enough to tell us about what our \\(\\text{Uniform}(0, 60)\\) prior really means.\n\n\n\n\n\n\nEXERCISE\n\n\n\nTry to make many different simulations (say, 12 simulations). This represents 12 different repeats of the whole process: draw a value from the uniform prior, THEN draw a value from the poisson. Visualize them any way you want! (the worked example below uses ggplot2)\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nset.seed(525600)\n\nsimulate_some_birds &lt;- function() {\n  lambda &lt;- runif(1, min = 0, max = 60)\n  data.frame(obs = rpois(23, lambda = lambda))\n}\n\nrep_list &lt;- replicate(12, simulate_some_birds())\n\ntibble::tibble(simulation = 1:12,\n               obs = rep_list) |&gt; \n  unnest(cols = \"obs\") |&gt; \n  ggplot(aes(x = obs)) + \n  geom_histogram(bins = 28) + \n  facet_wrap(~simulation) + \n  theme_bw() + \n  labs(x = \"Number of birds observed per person\")\n\n\n\n\nTwelve different simulations of a possible bird dataset. Do all of these seem plausible?\n\n\n\n\n\n\n\nThis figure shows different simulations of what, according to our prior, might be reasonable datasets for us to study. Do any of them seem implausible to you? If so, try changing the prior. The goal is to make fake datasets that seem plausible, but which still include the possibility of some surprising observations.\nWhen you have a prior that generates observations that cover a range of scientifically reasonable values, then you are ready to move on to fitting real data.\nHowever before we actually do that, let’s do the whole thing again: this time in Stan."
  },
  {
    "objectID": "topics/01_simulation/index.html#simulating-data-in-stan",
    "href": "topics/01_simulation/index.html#simulating-data-in-stan",
    "title": "Introduction to Stan and simulation",
    "section": "Simulating data in Stan",
    "text": "Simulating data in Stan\nLet’s look back at the equation:\n\\[\n\\begin{align}\n\\text{Number of Birds}_{\\text{seen by person i}} &\\sim \\text{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\text{Uniform}(0, 60)\n\\end{align}\n\\]\nAnd then translate it into Stan:\n\npoisson_simulation &lt;- rstan::stan_model(\n  file = \"topics/01_simulation/poisson_simulation.stan\", \n  model_name = \"poisson_simulation\",\n  save_dso = TRUE\n  )\n\npoisson_simulation\n\nS4 class stanmodel 'poisson_simulation' coded as follows:\ndata {\n  int&lt;lower=0&gt; n_people;\n}\ngenerated quantities {\n  real&lt;lower=0&gt; avg_birds_per_person;\n  // an array -- like a list in R\n  array[n_people] int&lt;lower=0&gt; bird_count;\n\n  // simulate averages\n  avg_birds_per_person = uniform_rng(0, 60);\n  // simulate observations with that average\n  for (i in 1:n_people){\n    bird_count[i] = poisson_rng(avg_birds_per_person);\n  }\n} \n\n\nWhat you see just above is not R, but is the first Stan program we will see in this course. Stan code is written in a text file, which I’ve read in here just to display it for you.\nWhen you sample the model, as we’ll do later, Stan samples the posterior distribution using Hamiltonian Monte Carlo.\nBefore we run it, let’s look at the parts of a Stan model:\n\nParts of a Stan model\nThis Stan program has two parts. Each part is separated with curly braces, {}. The are they data block and the generated quantities block:\ndata {\n  int&lt;lower=0&gt; n_people;\n}\nAnd the generated quantites block.\ngenerated quantities {\n  real&lt;lower=0&gt; avg_observed;\n  // an array -- like a list in R\n  array[n_people] int&lt;lower=0&gt; bird_count;\n  \n  // simulate averages\n  avg_birds_per_person = uniform_rng(0, 60);\n  // simulate observations with that average\n  for (i in 1:n_people){\n    bird_count[i] = poisson_rng(avg_birds_per_person);\n  }\n}\n\n\n\n\n\n\nEXERCISE\n\n\n\nLet’s look at similarities and differences to the procedure in R. Try to find a few similarities and differences between R code and Stan code!\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nsimilarities:\n\nWe have a random number generating function for each of our distributions. In R, these were called runif and rpois, here they are uniform_rng and poisson_rng.\nOnce again, the only thing we need to provide is n_people, the number of observers we have\n\ndifferences:\n\nevery line ends with a semicolon ;\nin Stan, the name of a variable is on the RIGHT of a line, while in R it’s on the left.\nwe need to use a for-loop to generate random variables.\nnote the syntax for creating an array of integers. Arrays in Stan are a little like lists in R: they can hold any other kind of object, and are of a certain length."
  },
  {
    "objectID": "topics/01_simulation/index.html#sampling-in-stan",
    "href": "topics/01_simulation/index.html#sampling-in-stan",
    "title": "Introduction to Stan and simulation",
    "section": "Sampling in Stan",
    "text": "Sampling in Stan\n\n# We connect the data in R to the model in Stan using a \n# named list!\npoisson_simulation_datalist &lt;- list(\n  n_people = 21\n)\n\npoisson_simulation_samp &lt;- rstan::sampling(poisson_simulation,\n  data = poisson_simulation_datalist, \n  refresh = 0,\n  # usually not necessary -- this model has no parameters \n  algorithm = \"Fixed_param\")\n\nThis generates a large number of simulated datasets – the default is 4000 datasets! Each time the model samples, it draws a new value for the unobserved average (avg_birds_per_person) and for the number of birds seen by each person.\nLet’s pull out just a few of these datasets and visualize them.\nWe’ll use a wonderful package called tidybayes to easily extract posterior draws from stanfit objects.\n\nlibrary(tidybayes)\npois_sim &lt;- tidybayes::spread_draws(poisson_simulation_samp, \n                                    avg_birds_per_person,\n                                    bird_count[],\n                                    ndraws = 20,\n                                    seed = 525600)\n\nHere we pass tidybayes::spread_draws() the model name, as well as the names of the parameters that we want to work with. The parameter avg_birds_per_person is a scalar, so we only need to mention it by name. The parameter vector bird_count needs square brackets after its name. This syntax is strange but gives us lots of options for more complex models, as we’ll see!\nLet’s see what we get from tidybayes by looking at the first few rows.\n\npois_sim |&gt; \n  as.data.frame() |&gt; \n  head(9) |&gt; \n  knitr::kable()\n\n\n\n\n.chain\n.iteration\n.draw\navg_birds_per_person\nbird_count\n\n\n\n\n1\n553\n553\n16.80902\n17\n\n\n1\n553\n553\n16.80902\n19\n\n\n1\n553\n553\n16.80902\n18\n\n\n1\n553\n553\n16.80902\n22\n\n\n1\n553\n553\n16.80902\n21\n\n\n1\n553\n553\n16.80902\n13\n\n\n1\n553\n553\n16.80902\n14\n\n\n1\n553\n553\n16.80902\n22\n\n\n1\n553\n553\n16.80902\n18\n\n\n\n\n\nRemember we asked for only 25 of the 4000 posterior samples. Here is one sample, and just a bit of the next. We can see that the value of avg_birds_per_person is the same within each iteration. The model uses this average to sample every bird_count value, one for each person making observations. Then the program takes a new value of avg_birds_per_person and simulates everyone’s bird_count again!\nLet’s take a look at some of these simulations:\n\npois_sim |&gt; \n  ggplot(aes(x = bird_count)) + \n  geom_histogram(fill = \"orange\") + \n  geom_vline(aes(xintercept = avg_birds_per_person), col = \"darkgreen\", lwd = 1) + \n  facet_wrap(~.draw) + \n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nPrior simulations of bird counts. Green bars are the mean for a particular simulation, and orange histograms show the distribution of observations around this mean."
  },
  {
    "objectID": "topics/01_simulation/index.html#parameter-recovery",
    "href": "topics/01_simulation/index.html#parameter-recovery",
    "title": "Introduction to Stan and simulation",
    "section": "Parameter recovery",
    "text": "Parameter recovery\nLet’s go back and look at the fake datasets we created in R\n\navg_birds_per_person\n\n[1] 17.12789\n\nbird_count\n\n [1] 23 10 19 27 20 15 16 18 18 22 14 14 14 18 17 13 26 19 16 13 10\n\n\nand let’s see if we can recapture the only known parameter, avg_birds_per_person, which is equal to 17.127887.\nWe’ll do it first in R, using the function fitdistr from the MASS package:\n\nMASS::fitdistr(bird_count, dpois, start = list(lambda=10))\n\nWarning in stats::optim(x = c(23L, 10L, 19L, 27L, 20L, 15L, 16L, 18L, 18L, : one-dimensional optimization by Nelder-Mead is unreliable:\nuse \"Brent\" or optimize() directly\n\n\n     lambda  \n  17.2382812 \n ( 0.9060239)\n\n\nThis could also be done with glm\n\nbird_glm &lt;- glm(bird_count ~ 1, family = \"poisson\")\nexp(coef(bird_glm))\n\n(Intercept) \n    17.2381 \n\n\nYou can see that in all cases we are getting close to the value of avg_birds_per_person, which in these simulations is the true value.\n\nSampling the posterior distribution in Stan\nTime for the HMC Slides!\nWe will be doing a lot of Stan models this week, and we will begin by replicating the above GLM in Stan.\n\npoisson_model &lt;- rstan::stan_model(\n  file = \"topics/01_simulation/poisson_model.stan\",\n  model_name = \"poisson_model\")\n\npoisson_model\n\nS4 class stanmodel 'poisson_model' coded as follows:\ndata {\n  int&lt;lower=0&gt; n_people;\n  array[n_people] int&lt;lower=0&gt; bird_count_observed;\n}\nparameters {\n  real&lt;lower=0&gt; avg_birds_per_person;\n}\nmodel {\n  bird_count_observed ~ poisson(avg_birds_per_person);\n  avg_birds_per_person ~ uniform(0, 60);\n}\ngenerated quantities {\n  // an array -- like a list in R\n  array[n_people] int&lt;lower=0&gt; bird_count;\n\n  // simulate observations with that average\n  for (i in 1:n_people){\n    bird_count[i] = poisson_rng(avg_birds_per_person);\n  }\n} \n\n\nThis model has all the same code as the previous one, but has two additional parts. Let’s compare them\n\n\n\n\n\n\npoisson_simulation\n\nS4 class stanmodel 'poisson_simulation' coded as follows:\ndata {\n  int&lt;lower=0&gt; n_people;\n}\ngenerated quantities {\n  real&lt;lower=0&gt; avg_birds_per_person;\n  // an array -- like a list in R\n  array[n_people] int&lt;lower=0&gt; bird_count;\n\n  // simulate averages\n  avg_birds_per_person = uniform_rng(0, 60);\n  // simulate observations with that average\n  for (i in 1:n_people){\n    bird_count[i] = poisson_rng(avg_birds_per_person);\n  }\n} \n\n\n\n\n\n\npoisson_model\n\nS4 class stanmodel 'poisson_model' coded as follows:\ndata {\n  int&lt;lower=0&gt; n_people;\n  array[n_people] int&lt;lower=0&gt; bird_count_observed;\n}\nparameters {\n  real&lt;lower=0&gt; avg_birds_per_person;\n}\nmodel {\n  bird_count_observed ~ poisson(avg_birds_per_person);\n  avg_birds_per_person ~ uniform(0, 60);\n}\ngenerated quantities {\n  // an array -- like a list in R\n  array[n_people] int&lt;lower=0&gt; bird_count;\n\n  // simulate observations with that average\n  for (i in 1:n_people){\n    bird_count[i] = poisson_rng(avg_birds_per_person);\n  }\n} \n\n\n\n\n\n\n\nWhat’s different in this second Stan program? There are two new sections:\n\nparameters block : Indicated by parameters {}, this block includes all the unobserved quantities. In this case there is only one: avg_birds_per_person. We have to give this value a name and say what kind of number it is. Here is also the place to declare any constraints. In our example, we state that the parameter is always positive (because it is an average of counts).\nmodel block indicated by model {}, this block contains the model. What this means in a Bayesian model is that it lists the probability distribution for all the observations, and for all the unobserved parameters. In other words, it looks just like our mathematical expressions above. The model block can also contain intermediate calculations, for example combining data and parameters with an equation. We’ll see examples later.\n\nThis Stan program also contains one line that’s been moved.\n  avg_birds_per_person ~ uniform(0, 60);\nhas been moved to the model block. This has two consequences.\nFirst, when the model sees our data, it’s going to try to find values of avg_birds_per_person which make those data probable – in other words, it is going to find the posterior distribution of possible values of the average. Together, the prior and the data constrain what those values can be.\nSecond, the generated quantities block means something different now. Previously, we had no idea what avg_birds_per_person should be, so we had the computer choose a random number from a wide range. We called this “the prior” Now, when the computer draws new values of bird_count, it is going to use the values from the posterior that it is finding in the model {} block . This means that the simulations, rather than being prior predictive checks, are now posterior predictive checks."
  },
  {
    "objectID": "topics/01_simulation/index.html#parameter-recovery-1",
    "href": "topics/01_simulation/index.html#parameter-recovery-1",
    "title": "Introduction to Stan and simulation",
    "section": "Parameter recovery",
    "text": "Parameter recovery\n\n\n\n\n\n\nEXERCISE: parameter recovery in Stan\n\n\n\nUse the Stan code above to fit the model to our simulated data. Do we recover the parameters?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\nOnce again, we connect a dataset to our model with a list:\n\nbird_data_list &lt;- list(bird_count_observed = bird_count,\n                       n_people = length(bird_count))\n\npoisson_model_samp &lt;- rstan::sampling(poisson_model, \n                                      data  = bird_data_list,\n                                      refresh = 0)\n\nsummary(poisson_model_samp)$summary\n\n                          mean    se_mean        sd      2.5%       25%\navg_birds_per_person  17.29133 0.02261672 0.8816431  15.57717  16.69148\nbird_count[1]         17.29300 0.07687152 4.2517844   9.00000  14.00000\nbird_count[2]         17.33650 0.06715999 4.2114964  10.00000  14.00000\nbird_count[3]         17.39075 0.06702556 4.2348610  10.00000  14.00000\nbird_count[4]         17.25250 0.07114388 4.2449085   9.00000  14.00000\nbird_count[5]         17.36050 0.06971805 4.3035648   9.00000  14.00000\nbird_count[6]         17.25850 0.06832552 4.3164031  10.00000  14.00000\nbird_count[7]         17.33325 0.07065510 4.3133335   9.97500  14.00000\nbird_count[8]         17.26750 0.06733192 4.1986130  10.00000  14.00000\nbird_count[9]         17.24650 0.06760074 4.2252457  10.00000  14.00000\nbird_count[10]        17.31600 0.07210748 4.2432470   9.00000  14.00000\nbird_count[11]        17.25300 0.06658456 4.2534708  10.00000  14.00000\nbird_count[12]        17.26600 0.06920602 4.2779457  10.00000  14.00000\nbird_count[13]        17.29650 0.06801648 4.1935050  10.00000  14.00000\nbird_count[14]        17.39875 0.07151847 4.3008572  10.00000  14.00000\nbird_count[15]        17.28850 0.07701954 4.3527582   9.00000  14.00000\nbird_count[16]        17.33800 0.07159828 4.2882226  10.00000  14.00000\nbird_count[17]        17.35300 0.07049617 4.3282877  10.00000  14.00000\nbird_count[18]        17.26150 0.07121162 4.2332136   9.00000  14.00000\nbird_count[19]        17.21725 0.06854881 4.1986855  10.00000  14.00000\nbird_count[20]        17.40175 0.07273169 4.2889912  10.00000  14.00000\nbird_count[21]        17.27975 0.07080281 4.2271098  10.00000  14.00000\nlp__                 671.03420 0.01613589 0.6999904 669.03755 670.89259\n                           50%       75%     97.5%    n_eff      Rhat\navg_birds_per_person  17.28907  17.86921  19.05998 1519.590 1.0013517\nbird_count[1]         17.00000  20.00000  26.00000 3059.226 1.0001325\nbird_count[2]         17.00000  20.00000  26.00000 3932.345 0.9999590\nbird_count[3]         17.00000  20.00000  26.00000 3992.064 0.9995419\nbird_count[4]         17.00000  20.00000  26.00000 3560.095 1.0015001\nbird_count[5]         17.00000  20.00000  26.00000 3810.362 1.0001988\nbird_count[6]         17.00000  20.00000  26.00000 3990.967 0.9999574\nbird_count[7]         17.00000  20.00000  26.00000 3726.826 0.9999260\nbird_count[8]         17.00000  20.00000  26.00000 3888.389 0.9992242\nbird_count[9]         17.00000  20.00000  26.00000 3906.618 1.0006428\nbird_count[10]        17.00000  20.00000  26.00000 3462.868 0.9998367\nbird_count[11]        17.00000  20.00000  26.00000 4080.749 0.9996817\nbird_count[12]        17.00000  20.00000  26.00000 3821.050 0.9995255\nbird_count[13]        17.00000  20.00000  26.00000 3801.246 0.9997909\nbird_count[14]        17.00000  20.00000  26.00000 3616.376 0.9998891\nbird_count[15]        17.00000  20.00000  27.00000 3193.944 0.9995756\nbird_count[16]        17.00000  20.00000  26.00000 3587.149 0.9998720\nbird_count[17]        17.00000  20.00000  26.00000 3769.651 0.9997274\nbird_count[18]        17.00000  20.00000  26.00000 3533.773 0.9998664\nbird_count[19]        17.00000  20.00000  26.00000 3751.689 1.0003756\nbird_count[20]        17.00000  20.00000  26.00000 3477.466 1.0010472\nbird_count[21]        17.00000  20.00000  26.00000 3564.397 1.0007880\nlp__                 671.29515 671.46106 671.50609 1881.908 1.0005204\n\n\n\n\n\nWe can look at a table of coefficients but it is much easier to once again look at posterior samples as a figure.\n\n\n\n\n\n\nVisualize everything!\n\n\n\nBayesian workflows are highly visual. Make as many plots as you can: of your parameters, your predictions, the performance of your chains, etc.\n\n\n\nbayesplot\nAnother essential package for working with posterior samples is called bayesplot. Let’s use it to look at the posterior distribution for avg_birds_per_person.\n\nbayesplot::mcmc_areas(poisson_model_samp, pars = \"avg_birds_per_person\") + \n  geom_vline(xintercept = avg_birds_per_person, col = \"orange\", lwd = 2)\n\n\n\n\nposterior distribution for avg_birds_per_person. The orange line is the true parameter value, which we simulated in R.\n\n\n\n\n\nPosterior predictive checks\nBayesian models MAKE data, which suggests a clear way to validate our models: ask the model to make some data, then see how well these data correspond to biology (e.g. to our real data). Here, we will take 50 fake datasets of bird counts and compare them to the simulation we first did in R.\nThe process involves a bit of fiddling around in R to get the simulated data, but then bayesplot does all the work:\n\nbird_count_draws &lt;- rstan::extract(poisson_model_samp, pars = \"bird_count\")\nbayesplot::ppc_dens_overlay(y = bird_count,\n                            yrep = head(bird_count_draws$bird_count, 50))\n\n\n\n\n\n\n\n\n\n\n\ntidybayes again\nAs we see above, bayesplot offers many out-of-the-box figures. Sometimes however, you’ll want to control exactly what your figures look like, and for this tidybayes is an excellent tool.\nLet’s use the flexibility of tidybayes to show how the prior and posterior differ between our two models\ntidybayes::gather_rvars(poisson_simulation_samp,\n                        avg_birds_per_person) |&gt; \n  ggplot(aes(y = \"avg_birds_per_person\", dist = .value)) + \n  tidybayes::stat_halfeye() + \n  labs(title = \"Prior\")\ntidybayes::gather_rvars(poisson_model_samp, \n                        avg_birds_per_person) |&gt; \n  ggplot(aes(y = \"avg_birds_per_person\", dist = .value)) + \n  tidybayes::stat_halfeye() + \n  labs(title = \"Posterior\") + \n  coord_cartesian(xlim = c(0, 60))\n\n\n\n\n\n\n\n\n\n\n\n\nShinystan\n\nshinystan::launch_shinystan(poisson_model_samp)"
  },
  {
    "objectID": "topics/01_simulation/index.html#exercises",
    "href": "topics/01_simulation/index.html#exercises",
    "title": "Introduction to Stan and simulation",
    "section": "Exercises",
    "text": "Exercises\n\nLevel 1\n\nWhat would you do next to add complexity the bird-counting model above?\nWe plotted histograms to evaluate our model. Experiment with other types of plots. For example, what is the maximum value in each posterior simulation? What is the minimum? How to these compare to the real data? TIP: check out ?ppc_stat.\n\n\n\nLevel 2\n\nTake a closer look at poisson_model_samp$summary(). All the values of bird_count are the same. That’s correct, but why?\nTry to fit YOUR data and your chosen distribution from Monday’s exercise. Check to see if the distribution you chose is implemented in Stan – see, for example, this list.\ncheck the fit using the plots we have already seen today.\n\n\n\nLevel 3\n\nYou would never actually do the analysis in this exercise! If all you want is the average of a Poisson distribution, you can get that without any sampling at all. Start by writing the model with a different prior:\n\n\\[\n\\begin{align}\n\\text{Number of Birds}_{\\text{seen by person i}} &\\sim \\text{Poisson}(\\lambda) \\\\\n\\lambda &\\sim \\text{Gamma}(9, .5)\n\\end{align}\n\\]\nThis lets us calculate the posterior distribution directly. See the equation on Wikipedia and calculate the posterior for our bird data."
  },
  {
    "objectID": "topics/01_simulation/index.html#footnotes",
    "href": "topics/01_simulation/index.html#footnotes",
    "title": "Introduction to Stan and simulation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nin Andrew’s experience anyway!↩︎"
  },
  {
    "objectID": "topics/02_regression/index.html",
    "href": "topics/02_regression/index.html",
    "title": "Univariate regression",
    "section": "",
    "text": "make a histogram of 500 numbers from a distribution!\n\nnormal\npoisson\n** EXTRA** try a new one, like beta, gamma, lognormal\n\nmake a histogram of poisson observations, using the classic log() link function.\n\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(e^a) \\\\\na &\\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\n\n\n\n\n\n\nTIP\n\n\n\n\n\n\n## sample poisson variables like this:\nrpois(500, exp(3))\n\n  [1] 23 26 27 16 25 16 14 21 15 18 26 19 22 21 20 23 22 21 19 15 18 19 18 24 23\n [26] 18 20 14 18 15 21 22 24 23 15 19 18 23 18 22 20 15 22 11 25 28 23 19 30 25\n [51] 16 21 20 24 23 19 14 21 16 31 19 16 19 19 20 26 15 15 15 25 29 12 24 19 26\n [76] 16 18 21 20 22 26 27 24 15 26 33 22 23 26 23 22 23 20 28 17 12 14 20 30 20\n[101] 15 24 21 31 20 13 18 17 20 15 27 14 13 13 16 25 24 21 16 21 19 26 14 18 26\n[126] 14 24 18 20 20 23 16 25 24 19 16 15 23 15 17 21 18 16 11 26 22 28 15 20 21\n[151] 19 23 18 18 22 26 23 22 13 27 24 14 21 22 31 13 24 19 26 18 25 30 12 18 15\n[176] 25 17 22 22 17 26 23 19 24 21 16 21 24 17 20 16 18 25 26 18 12 22 21 13 19\n[201] 20 22 19 21 19 13 20 23 12 26 14 19 16 22 19 21 19 29 20 19 26 21 19 28 19\n[226] 17 15 19 22 12 24 15 13 19 23 17 22 18 16 28 23 21 24 23 19 23 19 24 18 22\n[251] 17 17 24 19 18 21 18 24 17 32 26 27 26 28 22 23 17 23  9 15 23 33 16 15 17\n[276] 35 16 19 13 14 18 19 18 14 28 21 16 19 21 19 17 17 12 21 20 17 20 21 13 20\n[301] 17 13 22 18 21 22 14 18 22 18 21 20 12 16 22 15 19 24 16 19 19 13 24 16 20\n[326] 16 12 16 28 23 24 20 13 15 24 19 18 29 17 18 19 22 15 14 26 20 24 26 22 23\n[351] 18 16 18  9 14 24 17 32 29 19 18 22 19 27 16 26 17 23 25 22 23 22 19 20 23\n[376] 15 28 24 10 19 23 27 18 15 23 20 14 20 19 28 13 30 21 14 14 25 17 21 22 21\n[401] 19 16 19 16 15 31 23 27 20 22 12 24 21 11 28 17 21 19 29 29 23 23 21 15 28\n[426] 16 19 15 25 12 12 20 19 15 18 24 24 23 21 21 29 21 19 26 22 21 17 21 17 17\n[451] 24 19 15 26 23 22 25 21 25 16 16 17 16 20 16 29 25 23 13 19 17 24 15 31 16\n[476] 22 23 19 17 27 17 15 15 15 19 16 14 17 17 25 15 29 22 19 28 19 23 23 18 29\n\n\n\n\n\n\nmake a histogram of Binomial observations, using the inverse logit link function\n\n\\[\n\\begin{align}\ny &\\sim \\text{Binomial}\\left(\\frac{1}{1+e^{-a}}, N \\right) \\\\\na &\\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\nHere’s a plot of the link function, to help you think about it:\n\ncurve(1 / (1 + exp(-x)), xlim = c(-3, 3), ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIP\n\n\n\n\n\n\na &lt;- rnorm(1, mean = 0, 1)\nhist(rbinom(n = 500, size = 50, prob = 1 / (1 + exp(-a))))"
  },
  {
    "objectID": "topics/02_regression/index.html#simulation-workout",
    "href": "topics/02_regression/index.html#simulation-workout",
    "title": "Univariate regression",
    "section": "",
    "text": "make a histogram of 500 numbers from a distribution!\n\nnormal\npoisson\n** EXTRA** try a new one, like beta, gamma, lognormal\n\nmake a histogram of poisson observations, using the classic log() link function.\n\n\\[\n\\begin{align}\ny &\\sim \\text{Poisson}(e^a) \\\\\na &\\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\n\n\n\n\n\n\nTIP\n\n\n\n\n\n\n## sample poisson variables like this:\nrpois(500, exp(3))\n\n  [1] 23 26 27 16 25 16 14 21 15 18 26 19 22 21 20 23 22 21 19 15 18 19 18 24 23\n [26] 18 20 14 18 15 21 22 24 23 15 19 18 23 18 22 20 15 22 11 25 28 23 19 30 25\n [51] 16 21 20 24 23 19 14 21 16 31 19 16 19 19 20 26 15 15 15 25 29 12 24 19 26\n [76] 16 18 21 20 22 26 27 24 15 26 33 22 23 26 23 22 23 20 28 17 12 14 20 30 20\n[101] 15 24 21 31 20 13 18 17 20 15 27 14 13 13 16 25 24 21 16 21 19 26 14 18 26\n[126] 14 24 18 20 20 23 16 25 24 19 16 15 23 15 17 21 18 16 11 26 22 28 15 20 21\n[151] 19 23 18 18 22 26 23 22 13 27 24 14 21 22 31 13 24 19 26 18 25 30 12 18 15\n[176] 25 17 22 22 17 26 23 19 24 21 16 21 24 17 20 16 18 25 26 18 12 22 21 13 19\n[201] 20 22 19 21 19 13 20 23 12 26 14 19 16 22 19 21 19 29 20 19 26 21 19 28 19\n[226] 17 15 19 22 12 24 15 13 19 23 17 22 18 16 28 23 21 24 23 19 23 19 24 18 22\n[251] 17 17 24 19 18 21 18 24 17 32 26 27 26 28 22 23 17 23  9 15 23 33 16 15 17\n[276] 35 16 19 13 14 18 19 18 14 28 21 16 19 21 19 17 17 12 21 20 17 20 21 13 20\n[301] 17 13 22 18 21 22 14 18 22 18 21 20 12 16 22 15 19 24 16 19 19 13 24 16 20\n[326] 16 12 16 28 23 24 20 13 15 24 19 18 29 17 18 19 22 15 14 26 20 24 26 22 23\n[351] 18 16 18  9 14 24 17 32 29 19 18 22 19 27 16 26 17 23 25 22 23 22 19 20 23\n[376] 15 28 24 10 19 23 27 18 15 23 20 14 20 19 28 13 30 21 14 14 25 17 21 22 21\n[401] 19 16 19 16 15 31 23 27 20 22 12 24 21 11 28 17 21 19 29 29 23 23 21 15 28\n[426] 16 19 15 25 12 12 20 19 15 18 24 24 23 21 21 29 21 19 26 22 21 17 21 17 17\n[451] 24 19 15 26 23 22 25 21 25 16 16 17 16 20 16 29 25 23 13 19 17 24 15 31 16\n[476] 22 23 19 17 27 17 15 15 15 19 16 14 17 17 25 15 29 22 19 28 19 23 23 18 29\n\n\n\n\n\n\nmake a histogram of Binomial observations, using the inverse logit link function\n\n\\[\n\\begin{align}\ny &\\sim \\text{Binomial}\\left(\\frac{1}{1+e^{-a}}, N \\right) \\\\\na &\\sim \\text{Normal}(??, ??)\n\\end{align}\n\\]\nHere’s a plot of the link function, to help you think about it:\n\ncurve(1 / (1 + exp(-x)), xlim = c(-3, 3), ylim = c(0, 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIP\n\n\n\n\n\n\na &lt;- rnorm(1, mean = 0, 1)\nhist(rbinom(n = 500, size = 50, prob = 1 / (1 + exp(-a))))"
  },
  {
    "objectID": "topics/02_regression/index.html#statistical-models-of-penguin-bill-morphology.",
    "href": "topics/02_regression/index.html#statistical-models-of-penguin-bill-morphology.",
    "title": "Univariate regression",
    "section": "Statistical models of Penguin bill morphology.",
    "text": "Statistical models of Penguin bill morphology.\nWe’ll be studying the relationship between two numbers about penguin bills. Specifically, we’ll ask “Are longer bills also deeper?”. This question might not be the most interesting ecologically, but it is a great chance to practice some interesting stats.\nLet’s begin with plotting the data:\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\nlibrary(tidybayes)\npenguins |&gt; \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() + \n  stat_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nBill depth (mm) as predicted by bill length (mm) across the entire palmerpenguins dataset.\n\n\n\n\nLet’s write a simple statistical model for these data:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??) \\\\\n\\sigma &\\sim \\text{Exponential}(??)\n\\end{align}\n\\]\nWhat should our priors be? Before we can answer that, we have a more important question:\n\n\n\n\n\n\nWHERE IS ZERO??\n\n\n\nIt has to be somewhere. Does it make sense? take control and choose for yourself.\n\n\nIf we fit a model like this without thinking about the location of zero, we get some pretty silly answers:\n\nbill_line &lt;- coef(lm(bill_depth_mm ~ bill_length_mm, data = penguins))\n\nWhen the value of bill length is 0, the average of the response is the intercept:\n\\[\n\\begin{align}\n\\mu_i &= \\beta_0 + \\beta_1\\times\\text{Bill length}_i \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times0 \\\\\n\\mu_i &= \\beta_0 \\\\\n\\end{align}\n\\]\nBut, if we take the data as we found it, we’re going to be talking about \\(\\beta_0\\) as the depth of a penguin’s bill when the bill has 0 length! Either way it is the same line. However, from the point of view of setting priors and interpreting coefficients, it helps a lot to set a meaningful 0.\nA very common choice is to subtract the average from your independent variable, so that it is equal to 0 at the average:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(??) \\\\\n\\beta_1 &\\sim \\text{Normal}(??)\n\\end{align}\n\\]\nNow \\(\\beta_0\\) means the average bill depth at the average bill length. It becomes easier to think about priors:\n\\[\n\\begin{align}\n\\text{Bill depth}_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n\\mu_i &= \\beta_0 + \\beta_1\\times(\\text{Bill length}_i  - \\overline{\\text{Bill length}})\\\\\n\\beta_0 &\\sim \\text{Normal}(17,2) \\\\\n\\beta_1 &\\sim \\text{Normal}(0,.5) \\\\\n\\sigma &\\sim \\text{Exponential}(0.5)\n\\end{align}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nWhat continuous predictors have you used in your analysis? How would you find a biologically meaningful zero? Think about how you would center time, age, mass, fitness etc."
  },
  {
    "objectID": "topics/02_regression/index.html#prior-predictive-simulations",
    "href": "topics/02_regression/index.html#prior-predictive-simulations",
    "title": "Univariate regression",
    "section": "Prior predictive simulations",
    "text": "Prior predictive simulations\nArmed with this model, it becomes much easier to think about prior predictions.\nWe’ll make a bunch of lines implied by the equation above. There’s two steps:\n\nCenter the predictor\nMake up a vector that goes from the minimum to the maximum of the predictor. This is just for convenience!\n\n\nbill_len_centered &lt;- with(penguins,\n                          bill_length_mm - mean(bill_length_mm,\n                                                na.rm = TRUE))\n\n## make up a short vector\nsome_bill_lengths &lt;- seq(\n  from = min(bill_len_centered, na.rm = TRUE), \n  to = max(bill_len_centered, na.rm = TRUE),\n  length.out = 10\n  )\n\n\n\n\n\n\n\nShortcuts to these common tasks\n\n\n\nThese tasks are so common that they are automated in helper functions.\nFor centering predictors, see the base R function ?scale\nFor creating a short vector over the range of a predictor, see modelr::seq_range. The R package modelr has many different functions to help with modelling.\n\n\nTo simulate, we’ll use some matrix algebra, as we saw in lecture:\n\nslopes &lt;- rnorm(7, 0, .5)\ninters &lt;- rnorm(7, 17, 2)\n\nX &lt;- cbind(1, some_bill_lengths)\nB &lt;- rbind(inters, slopes)\n\nknitr::kable(head(X))\n\n\n\n\n\nsome_bill_lengths\n\n\n\n\n1\n-11.8219298\n\n\n1\n-8.7663743\n\n\n1\n-5.7108187\n\n\n1\n-2.6552632\n\n\n1\n0.4002924\n\n\n1\n3.4558480\n\n\n\n\nknitr::kable(head(B))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninters\n19.3267323\n18.7340270\n16.6558349\n17.814553\n14.0237585\n17.8310158\n14.6174138\n\n\nslopes\n0.2564205\n-0.4651482\n0.2435612\n-0.278236\n-0.1508848\n0.6891655\n-0.5906592\n\n\n\n\nprior_mus &lt;- X %*% B\n\nmatplot(x = some_bill_lengths,\n        y = prior_mus, type = \"l\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCopy the code above. Increase the number of simulations. Which priors are too wide? Which are too narrow?\n\n\n\nSimulating Observations\nThere are always at least TWO kinds of predictions we can be thinking about:\n\nPredicted averages. This is often called a “confidence” interval for a regression line.\nPredicted observations. This is often called a “prediction” interval.\n\nWe can use the full model to simulate observations!\n\nslopes &lt;- rnorm(7, 0, .5)\ninters &lt;- rnorm(7, 17, 2)\nsigmas &lt;- rexp(7, rate = 0.3)\n\nX &lt;- cbind(1, some_bill_lengths)\nB &lt;- rbind(inters, slopes)\n\nprior_mus &lt;- X %*% B\n\nprior_obs &lt;- matrix(0, nrow = nrow(prior_mus), ncol = ncol(prior_mus))\n\nfor (j in 1:ncol(prior_obs)) {\n  prior_obs[,j] &lt;- rnorm(n = nrow(prior_mus),\n                         mean = prior_mus[,j],\n                         sd = sigmas[j])\n}\n\nmatplot(x = some_bill_lengths,\n        y = prior_obs, type = \"p\")\n\n\n\n\n\n\n\n\nTidyverse style for those who indulge:\n\ntibble(\n  sim_id = 1:7,\n  slopes = rnorm(7, 0, .5),\n  inters = rnorm(7, 17, 2),\n  sigmas = rexp(7, rate = 0.2)\n  ) |&gt; \n  mutate(x = list(seq(from = -10, to = 10, length.out = 6))) |&gt; \n  rowwise() |&gt; \n  mutate(avg = list(x * slopes + inters),\n         obs = list(rnorm(length(avg), mean = avg, sd = sigmas)),\n         sim_id = as.factor(sim_id)) |&gt; \n  unnest(cols = c(\"x\", \"avg\", \"obs\")) |&gt; \n  ggplot(aes(x= x, y = avg, group = sim_id, fill = sim_id)) + \n  geom_line(aes(colour = sim_id)) + \n  geom_point(aes(y = obs, fill = sim_id), pch = 21, size = 3) + \n  scale_fill_brewer(type = \"qual\") + \n  scale_colour_brewer(type = \"qual\") + \n  facet_wrap(~sim_id)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nPick one of the two simulations above and modify it. Here are some suggested modifications:\n\nExperiment with priors that are “too narrow” or “too wide”.\nTry a different distribution than the one used\nInstead of bill size, imagine that we are applying this model to YOUR data. What would you change?"
  },
  {
    "objectID": "topics/02_regression/index.html#linear-regression-in-stan",
    "href": "topics/02_regression/index.html#linear-regression-in-stan",
    "title": "Univariate regression",
    "section": "Linear regression in Stan",
    "text": "Linear regression in Stan\nNow we write a Stan program for this model. We’ll begin with a simple model that has no posterior predictions:\n\nnormal_regression_no_prediction &lt;- cmdstan_model(\n  stan_file = \"topics/02_regression/normal_regression_no_prediction.stan\")\n\nnormal_regression_no_prediction\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] bill_len;\n  vector[N] bill_dep;\n}\nparameters {\n  real intercept;\n  real slope;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  bill_dep ~ normal(intercept + slope * bill_len, sigma);\n  intercept ~ normal(17, 2);\n  slope ~ normal(0, 1);\n  sigma ~ exponential(.7);\n}\n\n\nIn order to get the posterior, we need to put our data in Stan. We follow the same steps as previously:\n\nRemember to remove NAs first!\n\narrange the data in a list\npass the data to a Stan model to estimate.\n\n\n## drop NAs\npenguins_no_NA &lt;- penguins |&gt; \n  tidyr::drop_na(bill_depth_mm, bill_length_mm) |&gt; \n  dplyr::mutate(\n    bill_length_center = bill_length_mm - mean(bill_length_mm))\n\n## assemble data list\ndata_list &lt;- with(penguins_no_NA,\n     list(N = length(bill_length_center),\n          bill_len = bill_length_center,\n          bill_dep = bill_depth_mm\n          ))\n\n## run the sampler, using the compiled model.\nnormal_reg_no_pred &lt;- normal_regression_no_prediction$sample(\n  data = data_list, \n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\nnormal_reg_no_pred$summary()\n\n# A tibble: 4 × 10\n  variable       mean    median     sd    mad       q5       q95  rhat ess_bulk\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__      -396.     -395.     1.20   0.998  -398.    -394.      1.00    2081.\n2 intercept   17.2      17.2    0.104  0.103    17.0     17.3     1.00    3849.\n3 slope       -0.0849   -0.0848 0.0192 0.0192   -0.117   -0.0524  1.00    4194.\n4 sigma        1.93      1.93   0.0729 0.0720    1.81     2.06    1.00    4059.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\nnormal_reg_no_pred$draws() |&gt; \n  bayesplot::mcmc_areas(pars = c(\"slope\", \"intercept\", \"sigma\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nDiscussion : Look just at the posterior distribution of the slope right above. Do we have evidence that there’s a relationship between bill length and bill depth."
  },
  {
    "objectID": "topics/02_regression/index.html#posterior-predictions-in-r",
    "href": "topics/02_regression/index.html#posterior-predictions-in-r",
    "title": "Univariate regression",
    "section": "Posterior predictions in R",
    "text": "Posterior predictions in R\nWe can calculate a posterior prediction line directly in R for these data. I’ll show each step in this workflow separately:\n\nnormal_reg_no_pred |&gt; \n  spread_rvars(slope, intercept, sigma)\n\n# A tibble: 1 × 3\n            slope  intercept        sigma\n       &lt;rvar[1d]&gt; &lt;rvar[1d]&gt;   &lt;rvar[1d]&gt;\n1  -0.085 ± 0.019   17 ± 0.1  1.9 ± 0.073\n\n\ntidybayes helps us extract the posterior distribution of the parameters into a convenient object called an rvar. Learn more about tidybayes here and about the rvar datatype here\nNext we combine these posteriors with a vector of observations to make a posterior distribution of LINES:\n\nnormal_reg_predline &lt;- normal_reg_no_pred |&gt; \n  tidybayes::spread_rvars(slope, intercept) |&gt; \n  expand_grid(x = seq(from = -15, to = 15, length.out = 5)) |&gt; \n  mutate(mu = intercept + slope*x)\n\nknitr::kable(normal_reg_predline)\n\n\n\n\nslope\nintercept\nx\nmu\n\n\n\n\n-0.085 ± 0.019\n17 ± 0.1\n-15.0\n18 ± 0.31\n\n\n-0.085 ± 0.019\n17 ± 0.1\n-7.5\n18 ± 0.18\n\n\n-0.085 ± 0.019\n17 ± 0.1\n0.0\n17 ± 0.10\n\n\n-0.085 ± 0.019\n17 ± 0.1\n7.5\n17 ± 0.18\n\n\n-0.085 ± 0.019\n17 ± 0.1\n15.0\n16 ± 0.31\n\n\n\n\n\nFinally we’ll plot these:\n\nnormal_reg_predline |&gt; \n  ggplot(aes(x = x, dist = mu)) + \n  stat_lineribbon() + \n  geom_point(aes(x = bill_length_center, y = bill_depth_mm),\n             inherit.aes = FALSE,\n             data = penguins_no_NA)"
  },
  {
    "objectID": "topics/02_regression/index.html#posterior-predictions-in-stan",
    "href": "topics/02_regression/index.html#posterior-predictions-in-stan",
    "title": "Univariate regression",
    "section": "Posterior predictions in Stan",
    "text": "Posterior predictions in Stan\nWe can also make these posterior predictions in Stan.\n\nnormal_regression &lt;- cmdstan_model(stan_file = \"topics/02_regression/normal_regression.stan\")\n\nnormal_regression\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] bill_len;\n  vector[N] bill_dep;\n  // posterior predictions\n  int&lt;lower=0&gt; npost;\n  vector[npost] pred_values;\n}\nparameters {\n  real intercept;\n  real slope;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  bill_dep ~ normal(intercept + slope * bill_len, sigma);\n  intercept ~ normal(17, 2);\n  slope ~ normal(0, 1);\n}\ngenerated quantities {\n  vector[npost] post_bill_dep_obs;\n  vector[npost] post_bill_dep_average;\n  \n  // calculate expectation\n  post_bill_dep_average = intercept + slope * pred_values;\n  \n  // make fake observations\n  for (i in 1:npost) {\n    post_bill_dep_obs[i] = normal_rng(intercept + slope * pred_values[i], sigma);\n  }  \n  \n}\n\n\n\npenguins_no_NA &lt;- penguins |&gt; \n  tidyr::drop_na(bill_depth_mm, bill_length_mm) |&gt; \n  dplyr::mutate(\n    bill_length_center = bill_length_mm - mean(bill_length_mm))\n\ndata_list &lt;- with(penguins_no_NA,\n     list(N = length(bill_length_center),\n          bill_len = bill_length_center,\n          bill_dep = bill_depth_mm,\n          npost = 6,\n          pred_values = modelr::seq_range(penguins_no_NA$bill_length_center, n = 6)\n          ))\n\nbill_norm_reg &lt;- normal_regression$sample(data = data_list, \n                                          refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n\n\nlibrary(tidyverse)\n\nbill_posterior &lt;- bill_norm_reg |&gt; \n  tidybayes::spread_rvars(post_bill_dep_average[i],\n                          post_bill_dep_obs[i]) |&gt;\n  mutate(bill_length = data_list$pred_values[i]) \n\nbill_posterior |&gt; \n  ggplot(aes(x = bill_length, dist = post_bill_dep_average)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center, y = bill_depth_mm),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1, guide = \"none\") + \n  labs(title = \"Average response\")\nbill_posterior |&gt; \n  ggplot(aes(x = bill_length, dist = post_bill_dep_obs)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center, y = bill_depth_mm),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Greens\", direction = -1, guide = \"none\") +\n  labs(title = \"Predicted observations\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEXERCISE\n\n\n\nExtend this model to include species. Specifically, let each species have its own value of the intercept. This involves combining this regression example with the previous activity on discrete predictors.\nWhen you’re done, look at the resulting summary of coefficients. What do you notice that’s different?\n\n\n\n\n\n\n\n\nSOLUTION\n\n\n\n\n\n\nnormal_regression_spp &lt;- cmdstan_model(stan_file = \"topics/02_regression/normal_regression_spp.stan\")\n\nnormal_regression_spp\n\ndata {\n  int&lt;lower=0&gt; N;\n  vector[N] bill_len;\n  vector[N] bill_dep;\n  // species IDs\n  array[N] int spp_id;\n  // posterior predictions\n  int&lt;lower=0&gt; npost;\n  vector[npost] pred_values;\n  array[npost] int pred_spp_id;\n}\nparameters {\n  vector[3] intercept;\n  real slope;\n  real&lt;lower=0&gt; sigma;\n}\nmodel {\n  intercept ~ normal(17, 2);\n  slope ~ normal(0, 1);\n  sigma ~ exponential(.7);\n  bill_dep ~ normal(intercept[spp_id] + slope * bill_len, sigma);\n}\ngenerated quantities {\n  vector[npost] post_bill_dep_obs;\n  vector[npost] post_bill_dep_average;\n  \n  // calculate expectation\n  post_bill_dep_average = intercept[pred_spp_id] + slope * pred_values;\n  \n  // make fake observations\n  for (i in 1:npost) {\n    post_bill_dep_obs[i] = normal_rng(intercept[pred_spp_id[i]] + slope * pred_values[i], sigma);\n  }  \n  \n}\n\n\nWe set up a list for this model just as we did before. Note that this time we are using TRIPLE the pred_values, because we want to run independent predictions for each species.\n\nbill_vec &lt;- modelr::seq_range(penguins_no_NA$bill_length_center, n = 6)\n\ndata_list_spp &lt;- with(penguins_no_NA,\n     list(N = length(bill_length_center),\n          bill_len = bill_length_center,\n          bill_dep = bill_depth_mm,\n          spp_id = as.numeric(as.factor(species)),\n          npost = 3*6,\n          pred_values = rep(bill_vec, 3),\n          pred_spp_id = rep(1:3, each = 6)\n          ))\n\nnormal_reg_spp_post &lt;- normal_regression_spp$sample(data = data_list_spp, refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.9 seconds.\n\n\nNote that the sign of the slope is different now!\n\nnormal_reg_spp_post$summary()\n\n# A tibble: 42 × 10\n   variable         mean   median     sd    mad       q5      q95  rhat ess_bulk\n   &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 lp__         -157.    -157.    1.56   1.35   -161.    -156.    0.999    1892.\n 2 intercept[1]   19.4     19.4   0.117  0.116    19.2     19.6   1.00     1937.\n 3 intercept[2]   17.4     17.4   0.141  0.141    17.2     17.7   1.00     2143.\n 4 intercept[3]   14.3     14.3   0.105  0.105    14.1     14.4   1.00     1949.\n 5 slope           0.198    0.198 0.0171 0.0173    0.170    0.226 1.00     1686.\n 6 sigma           0.956    0.955 0.0360 0.0350    0.899    1.02  1.00     3169.\n 7 post_bill_d…   17.0     17.0   0.966  0.963    15.4     18.6   0.999    3886.\n 8 post_bill_d…   18.1     18.1   0.945  0.953    16.5     19.6   1.00     3833.\n 9 post_bill_d…   19.2     19.2   0.966  0.954    17.6     20.8   1.00     3755.\n10 post_bill_d…   20.3     20.3   0.960  0.941    18.7     21.9   1.00     3841.\n# ℹ 32 more rows\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\n\n\n\nPlotting posterior predictions\nUsing stat_lineribbon(), let’s plot the average and predicted intervals for this regression.\nbill_posterior &lt;- normal_reg_spp_post |&gt; \n  tidybayes::spread_rvars(post_bill_dep_average[i],\n                          post_bill_dep_obs[i]) |&gt;\n  mutate(bill_length = data_list_spp$pred_values[i],\n         spp = data_list_spp$pred_spp_id) |&gt; \n  mutate(spp = as.factor(levels(penguins$species)[spp]))\n\nbill_posterior |&gt; \n  ggplot(aes(x = bill_length,\n             ydist = post_bill_dep_average,\n             fill = spp, \n             colour = spp)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center,\n                 y = bill_depth_mm,\n                 fill = species, colour = species),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) +   \n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Dark2\") + \n  labs(title = \"Average response\")\nbill_posterior |&gt; \n  ggplot(aes(x = bill_length,\n             dist = post_bill_dep_obs,\n             fill = spp,\n             colour = spp)) + \n  tidybayes::stat_lineribbon() + \n  geom_point(aes(x = bill_length_center,\n                 y = bill_depth_mm,\n                 colour = species),\n             data = penguins_no_NA, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Dark2\") + \n  labs(title = \"Predicted observations\") + \n  facet_wrap(~spp, ncol = 1)"
  },
  {
    "objectID": "topics/02_regression/index.html#exercise-5",
    "href": "topics/02_regression/index.html#exercise-5",
    "title": "Univariate regression",
    "section": "Exercise!",
    "text": "Exercise!\nshow how the \\(\\sigma\\) is different between these two models"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#content",
    "href": "topics/parameter_resampling/day_1.html#content",
    "title": "Day 1",
    "section": "Content",
    "text": "Content\nThe Secret Weapon\nregression with discrete predictors\n\nAfternoon practical exercises"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#course-setup-information",
    "href": "topics/parameter_resampling/day_1.html#course-setup-information",
    "title": "Day 1",
    "section": "Course setup information",
    "text": "Course setup information\n\nsite information\nplagiarism"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#simulation",
    "href": "topics/parameter_resampling/day_1.html#simulation",
    "title": "Day 1",
    "section": "Simulation",
    "text": "Simulation"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#quantifying-uncertainty",
    "href": "topics/parameter_resampling/day_1.html#quantifying-uncertainty",
    "title": "Day 1",
    "section": "Quantifying uncertainty",
    "text": "Quantifying uncertainty"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#resampling",
    "href": "topics/parameter_resampling/day_1.html#resampling",
    "title": "Day 1",
    "section": "Resampling",
    "text": "Resampling\nIn frequentist models, we can use the variance covariance matrix of parameters to resample new parameters values. This lets us propagate uncertainty from the estimated parameters to the predicted relationship.\nLet’s demonstrate this with one specific mite:\n\nlrug_water &lt;- mite_water |&gt; \n  filter(sp == \"LRUG\")\n\nlrug_glm &lt;- glm(pa ~ water, data = lrug_water, family = \"binomial\")\n\nNow, with our model object, we can create the resampling distribution of the model predicitons:\n\n# Set seed\nset.seed(42) # The answer !\n\n# a sequence along the range of water values in the data\npredVal &lt;- seq(from = min(lrug_water$water),\n               to = max(lrug_water$water),\n               length.out = 30)\n\nn_resamp &lt;- 500\n\n# Result object\nresampModel &lt;- array(NA_real_,\n                   dim = c(length(predVal), n_resamp))\n\n# Resample model parameters and calculate model predictions\nparamMean &lt;- summary(lrug_glm)$coefficients[,1]\nparamCov &lt;- summary(lrug_glm)$cov.unscaled\n\n# Resample model parameters\nparamSmpl &lt;- MASS::mvrnorm(n_resamp, paramMean, paramCov)\n\n# Calculate model predictions using the resampled model parameters\nfor(j in 1:n_resamp){\n  resampModel[,j] &lt;- binomial(link = \"logit\")$linkinv(\n    paramSmpl[j,1] + paramSmpl[j,2] * predVal)\n}\n\n# make a plot of these predictions\nmatplot(predVal, resampModel, type = \"l\", col = \"grey\", lty = 1)\n\n\n\n\n\n\n\n\nIf we want to find some kind of confidence interval for this line, we can take the quantiles of this resampling:\n\nlow &lt;- apply(resampModel, 1, quantile, probs = .015)\nhigh &lt;- apply(resampModel, 1, quantile, probs = .985)\n\n# plot\nwith(lrug_water, plot(pa ~ water, pch = 21, bg = \"lightblue\"))\npolygon(c(predVal,rev(predVal)),\n        c(low,rev(high)), col=\"thistle\", border=NA)\nlines(predVal, \n      predict(lrug_glm, newdata = list(water = predVal), type = \"response\")\n      )\n\n\n\n\n\n\n\n\nWe can also do this in a tidyverse style, if you are more comfortable with that:\n\ntibble(predVal) |&gt; \n  rowwise() |&gt; \n  mutate(intercept = list(paramSmpl[,1]),\n         slope = list(paramSmpl[,2]),\n         prediction = list(intercept + slope*predVal),\n         prediction_probability = list(plogis(prediction)),\n         low  = quantile(prediction_probability, .015),\n         high = quantile(prediction_probability, .985)) |&gt; \n  ggplot(aes(x = predVal, ymin = low, ymax = high)) + \n  geom_ribbon(fill = \"thistle\") + \n  theme_bw() + \n  ylim(c(0,1))"
  },
  {
    "objectID": "topics/parameter_resampling/day_1.html#bayesian-approach",
    "href": "topics/parameter_resampling/day_1.html#bayesian-approach",
    "title": "Day 1",
    "section": "Bayesian approach",
    "text": "Bayesian approach\nhere is a simple bayesian model to generate the same inference:\n\\[\n\\begin{align}\ny &\\sim \\text{Bernoulli}(p)\\\\\n\\text{logit}(p) &= \\alpha + X\\beta\\\\\n\\alpha &\\sim \\text{Normal}(-2.5, .5)\\\\\n\\beta &\\sim \\text{Normal}(0, .5)\\\\\n\\end{align}\n\\]\nnormally we would go through a careful process of checking our priors here. At this time we won’t because the point here is to show how the bayesian posterior includes uncertainty, not to demonstrate a full Bayes workflow.\nFirst we compile the model, then we’ll look at the Stan code:\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/andrew/software/cmdstan\n\n\n- CmdStan version: 2.34.1\n\nlogistic_glm_stan &lt;- cmdstan_model(\n  stan_file = \"topics/parameter_resampling/logistic_bern_logit.stan\", \n  pedantic = TRUE)\n\nlogistic_glm_stan\n\ndata {\n  int&lt;lower=0&gt; n;\n  vector[n] x;\n  array[n] int&lt;lower=0,upper=1&gt; y;\n}\nparameters {\n  real intercept;\n  real slope;\n}\nmodel {\n  y ~ bernoulli_logit(intercept + slope * x);\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n}\n\n\nHere we see the same three parts of a Stan model that we have reviewed already:\n\ndata\nparameters\nprobability statements\n\nAs you can see, we are using a handy Stan function called bernoulli_logit. This function expects our prediction for the average to be on the logit scale, then applies the logit link function for us.\n\n\nAs a quick review, the logit equation, or inverse-log-odds, is written as \\[\n\\frac{e^\\mu}{1 + e^\\mu}\n\\] Which is also written as\n\\[\n\\frac{1}{1 + e^{-\\mu}}\n\\]\nStan expects our data as a list.\n\nlogistic_glm_stan_samples &lt;- logistic_glm_stan$sample(\n  data = list(n = nrow(lrug_water),\n              y = lrug_water$pa,\n              x = lrug_water$water),\n  refresh = 0)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.6 seconds.\n\ncoef(lrug_glm)\n\n(Intercept)       water \n-2.48153943  0.00874349 \n\nlibrary(tidybayes)\n\nspread_rvars(logistic_glm_stan_samples, intercept, slope[]) |&gt; \n  bind_cols(predVal = predVal) |&gt; \n  mutate(pred = posterior::rfun(plogis)(predVal * slope + intercept)) |&gt; \n  ggplot(aes(x = predVal, ydist = pred)) + \n  stat_dist_lineribbon() + \n  guides(fill = \"none\") + \n  ylim(c(0,1))\n\n\n\n\n\n\n\n\n\nAlternative parameterization\nStan contains many functions intended to facilitate writing statistical models. Above, we used the function bernoulli_logit so that we could provide the expression for the average on the logit scale.  Stan also provides an even more efficient function that we can use; it is especially good when we have more than one predictor variable and a vector of slopes:This idea is the core concept of a GLM, or generalized linear model. Statistical distributions have parameters, but for most distributions these have constraints – only some values are “allowed”. For example, the only parameter of a Bernoulli distribution is \\(p\\), the probability of success. We respect this constraint by using a link function: we write an expression for the average of a distribution that can be any real number, and put it through a link function to get the value for \\(p\\).\n\n\n\n\n\n\nWarning\n\n\n\nPLEASE NOTE below you will see the relative path to the stan file (stan/logistic.stan). Immediately below you will see the Stan file content. You can copy and paste this to your own computer!\n\n\n\nsuppressPackageStartupMessages(library(cmdstanr))\n\nlogistic_bern_glm &lt;- cmdstan_model(stan_file = \"topics/parameter_resampling/logistic.stan\", \n                               pedantic = TRUE)\n\nlogistic_bern_glm\n\ndata {\n  int&lt;lower=0&gt; N;\n  matrix[N, 1] x;\n  array[N] int&lt;lower=0,upper=1&gt; y;\n}\nparameters {\n  real intercept;\n  vector[1] slope;\n}\nmodel {\n  intercept ~ normal(-2.5, .5);\n  slope ~ normal(0, .5);\n  y ~ bernoulli_logit_glm(x, intercept, slope);\n}"
  },
  {
    "objectID": "topics/morgue/crossed_random_effects.html",
    "href": "topics/morgue/crossed_random_effects.html",
    "title": "Difference of normals is normal",
    "section": "",
    "text": "Taken from the single-random-effect exercise!\nuses two random effects though"
  },
  {
    "objectID": "topics/morgue/crossed_random_effects.html#bernoulli-presence-absence-data-mite-occurrance.",
    "href": "topics/morgue/crossed_random_effects.html#bernoulli-presence-absence-data-mite-occurrance.",
    "title": "Difference of normals is normal",
    "section": "Bernoulli presence-absence data: Mite occurrance.",
    "text": "Bernoulli presence-absence data: Mite occurrance.\nWhich mite species are found in which site?\nLet’s begin by drawing a classic picture: a species by site matrix!\n\nVisualizing\n\nspecies_numbers &lt;- with(mite_data_long,\n                        setNames(seq_along(unique(spp)), unique(spp)))\n\nmite_data_groupID &lt;- mutate(mite_data_long,\n                            group_id = species_numbers[spp])\n\n \nmite_list &lt;- with(mite_data_groupID,\n                  list(\n                    N = length(abd),\n                    y = abd,\n                    Ngroup = dplyr::n_distinct(spp),\n                    group_id = group_id\n                  ))\n\n\nmite_long_pa &lt;- mite_data_groupID |&gt; \n  mutate(pa = as.numeric(abd &gt; 0))\n\nmite_long_pa |&gt; \n  mutate(\n    spp = forcats::fct_reorder(spp, pa),\n    site_id = forcats::fct_reorder(site_id, pa),\n    pa = as.character(pa)) |&gt;\n  ggplot(aes(x = site_id, y = spp, fill = pa)) + \n  geom_tile() + \n  scale_fill_manual(values = c(\"1\" = \"black\", \"0\" = \"white\")) + \n  coord_fixed()\n\n\n\nMathematics\n\\[\n\\begin{align}\n\\text{Pr(y = 1)} &\\sim \\text{Bernoulli}(p) \\\\\n\\text{logit}(p) &= \\bar\\beta + \\beta_{\\text{site}[i]} + \\beta_{\\text{species}[i]} \\\\\n\\bar\\beta &\\sim N(0,.5) \\\\\n\\beta_{\\text{site}} &\\sim N(0, .2) \\\\\n\\beta_{\\text{species}} &\\sim N(0, \\sigma_{\\text{spp}}) \\\\\n\\sigma_{\\text{spp}} &\\sim \\text{Exponential}(2)\n\\end{align}\n\\]\n\nbernoulli_spp_site &lt;- cmdstan_model(stan_file = \"topics/03_one_random_effect/bernoulli_spp_site.stan\")\n\nbernoulli_spp_site\n\n\nbernoulli_mite_spp &lt;- bernoulli_spp_site$sample(data = list(\n  N = nrow(mite_long_pa),\n  y = mite_long_pa$pa,\n  Nsite = max(as.numeric(mite_long_pa$site_id)),\n  site_id = as.numeric(mite_long_pa$site_id),\n  Nspp = max(mite_long_pa$group_id),\n  spp_id = mite_long_pa$group_id\n),parallel_chains = 2, refresh = 0, chains = 2)\n\n\n# tidybayes::get_variables(bernoulli_mite_spp)\n\nThis model can produce a probability that any species occurs in any plot:\n\nmite_occ_prob_logit &lt;- bernoulli_mite_spp |&gt; \n  tidybayes::gather_rvars(prob_occurence[spp_id, site_id])\n\n## transform into probabilities\nmite_occ_prob &lt;- mite_occ_prob_logit |&gt; \n  mutate(prob = posterior::rfun(plogis)(.value))\n\nmite_occ_prob |&gt; \n  mutate(med_prob = median(prob),\n         spp_id  = forcats::fct_reorder(as.factor(spp_id), med_prob),\n         site_id = forcats::fct_reorder(as.factor(site_id), med_prob)) |&gt; \n  ggplot(aes(x = spp_id, y = site_id, fill = med_prob)) + \n  geom_tile()\n\n\n\nExercises:\n\nHow could you assess this model fit to data? What kind of figure would be most interesting?\nThe probability of a species occurring across all sites is given by b_avg + b_spp. How does that compare to the fraction of sites at which any species was observed?\nThe expected species richness of each site is given by b_avg + b_site. How does that compare to the observed species richness?"
  },
  {
    "objectID": "topics/morgue/crossed_random_effects.html#poisson-random-intercepts-mite-abundance",
    "href": "topics/morgue/crossed_random_effects.html#poisson-random-intercepts-mite-abundance",
    "title": "Difference of normals is normal",
    "section": "Poisson random intercepts: Mite abundance",
    "text": "Poisson random intercepts: Mite abundance\nHow does mite abundance vary among sites?\n\nmite_data_long |&gt; \n  mutate(site_id = forcats::fct_reorder(site_id, abd)) |&gt; \n  ggplot(aes(y = site_id, x = abd)) +\n  geom_point() + \n  coord_cartesian(xlim = c(0,100)) + \n  stat_summary(fun = median, col = \"red\", geom = \"point\")\n\n\n\n\n\n\n\nwrite the model in the same notation as the original\n\n\n\nLet’s model the counts of species abundances, using a random effect for each site. Write the model that corresponds to this!\n\n\n\nTrying it with a Normal distribution:\nIt’s actually possible to run the previous model on this one. let’s set up the data and try:\n\nspecies_numbers &lt;- with(mite_data_long,\n     setNames(seq_along(unique(spp)), unique(spp)))\n\nmite_data_groupID &lt;- mutate(mite_data_long,\n                            group_id = species_numbers[spp])\n\nmite_list &lt;- with(mite_data_groupID,\n                  list(\n                    N = length(abd),\n                    y = abd,\n                    Ngroup = dplyr::n_distinct(spp),\n                    group_id = group_id\n                  ))\n\nnormal_samples &lt;- hierarchical_groups$sample(data = mite_list, refresh = 0, parallel_chains = 4)\n\nThis is interesting, but it would probably be better to fit this model with something meant for counts! With this comes the need to include a log link function. Fortunately, Stan makes all this possible with just a few small changes:\n\n\nExercise: translate it into Stan\nModify the program hierarchical_groups.stan to work for poisson data. Some things to keep in mind:\n\ndata {} block: remember that the Poisson distribution needs integers and set up the data inputs accordinly.\nparameters {} block: think about which parameters the poisson does NOT need.\nmodel {} block: remember to remove any unneeded parameters from the likelihood (the model of the data), and their priors too.\nreplace normal with poisson_log. Note that this evaluates its argument on the log scale. That is, it works like a typical GLM done in R. We can keep priors the same as in the last model, though we may decide to change their values.\ngenerated quantities {} block: replace normal_rng() with poisson_log_rng() – where necessary – and delete unused parameters.\nagain, remember that the Poisson needs to be making integers. For example, replace vector[Ngroup] with array[Ngroup] int\n\n\n\nExercises\n\nTry modifying the program again, this time adding a predictor: water content. What happens to sigma_grp in this example?"
  },
  {
    "objectID": "topics/morgue/crossed_random_effects.html#extra-stuff",
    "href": "topics/morgue/crossed_random_effects.html#extra-stuff",
    "title": "Difference of normals is normal",
    "section": "Extra stuff",
    "text": "Extra stuff\nLet’s use simulations to demonstrate that univariate normal distributions are special cases of multivariate normal distributions:\n\nmysigma &lt;- 3\nnsamp &lt;- 1100\n\nhist(rnorm(nsamp, 0, mysigma))\n\nmySigma &lt;- diag(mysigma, nrow = nsamp)\n\nmv_numbers &lt;- MASS::mvrnorm(1, mu = rep(0, nsamp), Sigma = mySigma)\nhist(mv_numbers)"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-the-point-of-this",
    "href": "slides/03_Stan/index.html#what-is-the-point-of-this",
    "title": "Stan",
    "section": "What is the point of this?",
    "text": "What is the point of this?\n\\[\n\\begin{equation}\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-the-point-of-this-1",
    "href": "slides/03_Stan/index.html#what-is-the-point-of-this-1",
    "title": "Stan",
    "section": "What is the point of this",
    "text": "What is the point of this\n\\[\n\\begin{equation}\nP(\\boldsymbol{\\theta}|\\text{data}) = \\frac{P(\\boldsymbol{\\text{data}|\\theta}) \\cdot P(\\boldsymbol{\\theta})}{P(\\text{data})}\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-the-point-of-this-2",
    "href": "slides/03_Stan/index.html#what-is-the-point-of-this-2",
    "title": "Stan",
    "section": "What is the point of this",
    "text": "What is the point of this\n\\[\n\\begin{equation}\nP(\\boldsymbol{\\theta}|\\text{data}) \\propto P(\\boldsymbol{\\text{data}|\\theta}) \\cdot P(\\boldsymbol{\\theta})\n\\end{equation}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\nA concrete example\n\n\n\nLaid\nHatched\n\n\n\n\nEgg 1\nChick 1\n\n\nEgg 2\nChick 2\n\n\nEgg 3\nChick 3"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-1",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-1",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\nWhat’s the probability of this dataset?\nThis is called the likelihood\n\\[\n\\begin{align}\n\\text{hatch} &\\sim \\text{Binomial}(p, 5) \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-2",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-2",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\n\\[\n\\begin{align*}\n    P(3, 4, 5 | p) &= \\text{Binomial}(3 | p, 5) \\\\\n                   &\\quad \\times \\text{Binomial}(4 | p, 5) \\\\\n                   &\\quad \\times \\text{Binomial}(5 | p, 5)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-3",
    "href": "slides/03_Stan/index.html#what-we-talk-about-when-we-talk-about-pboldsymboltextdatatheta-3",
    "title": "Stan",
    "section": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)",
    "text": "What we talk about when we talk about \\(P(\\boldsymbol{\\text{data}|\\theta})\\)\n\\[\n\\begin{align*}\n    \\ln{P(3, 4, 5 | p)} &=  \\log(\\text{Binomial}(3 | p, 5)) \\\\\n                   &+  \\log(\\text{Binomial}(4 | p, 5)) \\\\\n                   &+ \\log(\\text{Binomial}(5 | p, 5))\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#log-likelihood-code-in-r",
    "href": "slides/03_Stan/index.html#log-likelihood-code-in-r",
    "title": "Stan",
    "section": "log-likelihood code in R",
    "text": "log-likelihood code in R\n\n\nsurv &lt;- c(3, 4, 5)\n\ncalc_ll &lt;- function(x) {\n  res &lt;- sum(-dbinom(surv, 5,\n                     prob = x,\n                     log = TRUE))\n  return(res)\n}\n\nprob_val &lt;- seq(from = 0, to = 1,\n                length.out = 30)\nlog_lik &lt;- numeric(30L)\n\nfor (i in 1:length(prob_val)) {\n  log_lik[i] &lt;- calc_ll(prob_val[i])\n}\npar(cex = 3)\nplot(prob_val, log_lik, type = \"b\")"
  },
  {
    "objectID": "slides/03_Stan/index.html#make-it-bayes-add-a-prior",
    "href": "slides/03_Stan/index.html#make-it-bayes-add-a-prior",
    "title": "Stan",
    "section": "Make it Bayes: add a prior",
    "text": "Make it Bayes: add a prior\n\\[\n\\begin{align}\n\\text{hatch} &\\sim \\text{Binomial}(p, 5) \\\\\np &\\sim \\text{Uniform}(0,1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#make-it-bayes-add-a-prior-1",
    "href": "slides/03_Stan/index.html#make-it-bayes-add-a-prior-1",
    "title": "Stan",
    "section": "Make it Bayes: add a prior",
    "text": "Make it Bayes: add a prior\n\\[\n\\begin{align*}\nP(\\boldsymbol{\\theta}|\\text{data}) &\\propto P(\\boldsymbol{\\text{data}|\\theta}) \\cdot P(\\boldsymbol{\\theta}) \\\\[10pt]\n&\\propto \\text{Bin}(3|p,5) \\cdot \\text{Bin}(4|p,5) \\\\\n&\\qquad \\cdot \\text{Bin}(5|p,5)\\cdot \\text{Uniform}(p|0,1) \\\\[10pt]\n\\log(P(\\boldsymbol{\\theta}|\\text{data})) &\\propto \\log(\\text{Bin}(3|p,5)) + \\log(\\text{Bin}(4|p,5)) \\\\\n&\\qquad + \\log(\\text{Bin}(5|p,5)) + \\log(\\text{Uniform}(p|0,1)) \\\\\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/03_Stan/index.html#sampling-the-uncalculable",
    "href": "slides/03_Stan/index.html#sampling-the-uncalculable",
    "title": "Stan",
    "section": "Sampling the uncalculable",
    "text": "Sampling the uncalculable\n\n\n\n\n\n\n\nMANIAC I, 1956 (top) Arianna W. Rosenbluth"
  },
  {
    "objectID": "slides/03_Stan/index.html#sampling-the-uncalculable-1",
    "href": "slides/03_Stan/index.html#sampling-the-uncalculable-1",
    "title": "Stan",
    "section": "Sampling the uncalculable",
    "text": "Sampling the uncalculable"
  },
  {
    "objectID": "slides/03_Stan/index.html#what-is-stan",
    "href": "slides/03_Stan/index.html#what-is-stan",
    "title": "Stan",
    "section": "What is Stan?",
    "text": "What is Stan?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStanisław Ulam"
  },
  {
    "objectID": "slides/03_Stan/index.html#stan",
    "href": "slides/03_Stan/index.html#stan",
    "title": "Stan",
    "section": "Stan",
    "text": "Stan\nhttps://mc-stan.org/\n\nA comprehensive software ecosystem aimed at facilitating the application of Bayesian inference\n\nFull Bayesian statistical inference with MCMC sampling (but not only)\nIntegrated with most data analysis languages (R, Python, MATLAB, Julia, Stata)"
  },
  {
    "objectID": "slides/03_Stan/index.html#why-stan",
    "href": "slides/03_Stan/index.html#why-stan",
    "title": "Stan",
    "section": "Why Stan?",
    "text": "Why Stan?\n\nOpen source\nExtensive documentation\nPowerful sampling algorithm\nLarge and active online community!"
  },
  {
    "objectID": "slides/03_Stan/index.html#hmc",
    "href": "slides/03_Stan/index.html#hmc",
    "title": "Stan",
    "section": "HMC",
    "text": "HMC\n\nMetropolis and Gibbs limitations:\n\nA lot of tuning to find the best spot between large and small steps\nInefficient in high-dimensional spaces\nCan’t travel long distances between isolated local minimums\n\nHamiltonian Monte Carlo:\n\nUses a gradient-based MCMC to reduce the random walk (hence autocorrelation)\nStatic HMC\nNo-U-Turn Sampler (NUTS)\nDon’t get it? Viz it!"
  },
  {
    "objectID": "slides/03_Stan/index.html#how-to-stan",
    "href": "slides/03_Stan/index.html#how-to-stan",
    "title": "Stan",
    "section": "How to Stan",
    "text": "How to Stan"
  },
  {
    "objectID": "slides/03_Stan/index.html#why-to-stan",
    "href": "slides/03_Stan/index.html#why-to-stan",
    "title": "Stan",
    "section": "WHY to Stan",
    "text": "WHY to Stan"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#a-general-way-to-write-matrices",
    "href": "slides/05_Matrix_algebra_advanced/index.html#a-general-way-to-write-matrices",
    "title": "Matrix algebra",
    "section": "A general way to write matrices",
    "text": "A general way to write matrices\n\\[\n\\mathbf{A} = \\begin{bmatrix}\n                A_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n                A_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n        \\end{bmatrix}\n\\] \\[A = \\left[a_{ij}\\right]=\\left[a_{ij}\\right]_{m\\times n}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#the-transpose-of-a-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#the-transpose-of-a-matrix",
    "title": "Matrix algebra",
    "section": "The transpose of a matrix",
    "text": "The transpose of a matrix\n\n\n\n\\[A = \\begin{bmatrix}\n          5 & -6 & 4 & -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B = \\begin{bmatrix}\n          -8\\\\\n          9\\\\\n          -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C = \\begin{bmatrix}\n          -4 & 1\\\\\n          2 & -5\\\\\n        \\end{bmatrix}\n\\]\n\n\\[A^t=\\begin{bmatrix}\n          5\\\\\n          -6\\\\\n          4\\\\\n          -4\\\\\n        \\end{bmatrix}\n\\]\n\\[B^t =\\begin{bmatrix}\n          -8 & 9 & -2\\\\\n        \\end{bmatrix}\n\\]\n\\[C^t =\\begin{bmatrix}\n          -4 & 2\\\\\n          1 & -5\\\\\n        \\end{bmatrix}\n\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nA\n\n     [,1] [,2]\n[1,]    3    5\n[2,]    1   -2\n\nt(A)\n\n     [,1] [,2]\n[1,]    3    1\n[2,]    5   -2"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#addition-and-substraction",
    "href": "slides/05_Matrix_algebra_advanced/index.html#addition-and-substraction",
    "title": "Matrix algebra",
    "section": "Addition and Substraction",
    "text": "Addition and Substraction\n\\[\\mathbf{C} = \\mathbf{A}\\pm \\mathbf{B}\\] \\[C_{ij} = A_{ij} \\pm B_{ij}\\]\n\n\\[\\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} +\n            \\begin{bmatrix}\n                2 & 1\\\\\n                4 & -2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                3+2 & 5+1\\\\\n                1+4 & -2-2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                5 & 6\\\\\n                5 & -4\\\\\n            \\end{bmatrix}\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nB &lt;- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)\nA + B\n\n     [,1] [,2]\n[1,]    5    6\n[2,]    5   -4"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#multiplying-a-matrix-by-a-scalar",
    "href": "slides/05_Matrix_algebra_advanced/index.html#multiplying-a-matrix-by-a-scalar",
    "title": "Matrix algebra",
    "section": "Multiplying a matrix by a scalar",
    "text": "Multiplying a matrix by a scalar\n\\[\\mathbf{B} = c\\mathbf{A}\\] \\[B_{ij} = cA_{ij}\\]\n\n\\[\n            0.3 \\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} =  \n            \\begin{bmatrix}\n                0.9 & 1.5\\\\\n                0.3 & -0.6\\\\\n            \\end{bmatrix}\n\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nc &lt;- 0.3\nc*A \n\n     [,1] [,2]\n[1,]  0.9  1.5\n[2,]  0.3 -0.6"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#matrix-multiplications-not-divisions",
    "href": "slides/05_Matrix_algebra_advanced/index.html#matrix-multiplications-not-divisions",
    "title": "Matrix algebra",
    "section": "Matrix multiplications (not divisions!)",
    "text": "Matrix multiplications (not divisions!)\n\\[\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}\\]\n\\[C_{ik} = \\sum^{n}_{j=1}A_{ij}B_{jk}\\]\nRules\nAssociative: \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\)\nDistributive: \\(\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B}+\\mathbf{A}\\mathbf{C}\\)\nNot commutative: \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\)"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#inner-product",
    "href": "slides/05_Matrix_algebra_advanced/index.html#inner-product",
    "title": "Matrix algebra",
    "section": "Inner product",
    "text": "Inner product\n\\[(\\mathbf{Ax})_i=\\sum_{j=1}^{n}A_{ij}x_j\\]\n\n\\[\n  \\begin{bmatrix}\n    3 & 5\\\\\n    1 & -2\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2\\\\ 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    (3, 5) \\cdot (2, 5)\\\\\n    (1, -2) \\cdot (2, 5) \\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    3 \\times 2 + 5 \\times 5\\\\\n    1 \\times 2 -2 \\times 5\\\\\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n    31\\\\\n    -8\\\\\n  \\end{bmatrix}\n\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nx &lt;- matrix(c(2,5), nrow = 2, ncol = 1)\nA %*% x\n\n     [,1]\n[1,]   31\n[2,]   -8"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#identity-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#identity-matrix",
    "title": "Matrix algebra",
    "section": "Identity matrix",
    "text": "Identity matrix\nThe identity matrix is a square matrix where all values of its diagonal are 0 except the diagonal values which are all 1s.\n\n\n\\[\n\\mathbf{I}=\\begin{bmatrix}\n                1 & 0 & 0\\\\\n                0 & 1 & 0\\\\\n                0 & 0 & 1\\\\\n        \\end{bmatrix}\n\\]\n\nThe identity matrix is important because\n\\[\\mathbf{A} \\cdot \\mathbf{I}_n = \\mathbf{A}\\] or\n\\[\\mathbf{I}_m \\cdot \\mathbf{A} = \\mathbf{A}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#diagonal-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#diagonal-matrix",
    "title": "Matrix algebra",
    "section": "Diagonal matrix",
    "text": "Diagonal matrix\nThe diagonal matrix is a square matrix where all values of its diagonal are 0 except the ones on the diagonal.\n\n\\[D=\n      \\begin{bmatrix}\n        d_1 & 0 & \\dots & 0\\\\\n        0 & d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots & d_n\\\\\n\\end{bmatrix}\\]\nAn example\n\n\\[\n\\begin{bmatrix}\n                -1 & 0 & 0\\\\\n                0 & 0 & 0\\\\\n                0 & 0 & 6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#triangular-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#triangular-matrix",
    "title": "Matrix algebra",
    "section": "Triangular matrix",
    "text": "Triangular matrix\n\n\nLower triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & 0 & 0\\\\\n        3 & 0 & 0 & 0\\\\\n        0 & 4 & 3 & 0\\\\\n        9 & -5 & 4 & 3\\\\\n\\end{bmatrix}\\]\n\nUpper triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & -5 & 0\\\\\n        0 & 0 & 5 & 6\\\\\n        0 & 0 & 3 & 3\\\\\n        0 & 0 & 0 & 3\\\\\n      \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#symmetric-matrix",
    "href": "slides/05_Matrix_algebra_advanced/index.html#symmetric-matrix",
    "title": "Matrix algebra",
    "section": "Symmetric matrix",
    "text": "Symmetric matrix\nThe values on the above and below the diagonal are match so that \\(A = A^t\\)\n\n\\[\n\\begin{bmatrix}\n                3 & 4 & -10\\\\\n                4 & 5 & 7\\\\\n                -10 & 7 & -6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion",
    "href": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion",
    "title": "Matrix algebra",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nIn matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix \\(\\mathbf{A}\\) is defined as \\(\\mathbf{A}^{-1}\\)\nAs such, \\[\\mathbf{A}\\cdot \\mathbf{A}^{-1} = \\mathbf{I}\\] In R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\n(Ainv &lt;- solve(A))\n\n           [,1]       [,2]\n[1,] 0.18181818  0.4545455\n[2,] 0.09090909 -0.2727273\n\nA %*% Ainv\n\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -2.775558e-17    1"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion-1",
    "href": "slides/05_Matrix_algebra_advanced/index.html#matrix-inversion-1",
    "title": "Matrix algebra",
    "section": "Matrix inversion",
    "text": "Matrix inversion\nInverting a diagonal matrix\n\\[D^{-1}=\n      \\begin{bmatrix}\n        1/d_1 & 0 & \\dots & 0\\\\\n        0 &  1/d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots &  1/d_n\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition",
    "href": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition",
    "title": "Matrix algebra",
    "section": "Cholesky decomposition",
    "text": "Cholesky decomposition\n\nThe Cholesky decomposition allows to decompose a matrix in a triangular, which, when multiplied by its transposed will allow us to recover the initial matrix.\nIn coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrice\nIn math terms the Cholesky decomposition is defined as \\[\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t\\] Example\n\\[\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            1 & 5 & 5\\\\\n            1 & 5 & 14\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            1 & 0 & 0\\\\\n            1 & 2 & 0\\\\\n            1 & 2 & 3 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            0 & 2 & 2\\\\\n            0 & 0 & 3 \\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition-1",
    "href": "slides/05_Matrix_algebra_advanced/index.html#cholesky-decomposition-1",
    "title": "Matrix algebra",
    "section": "Cholesky decomposition",
    "text": "Cholesky decomposition\nWhy is it useful ?\nThere are actually two main reasons :\n\nWorking with triangular matrices is computationally more efficient\nIt can be used to rescale matrices and make MCMC algorithms converge more easily"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution",
    "href": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution",
    "title": "Convergence trick",
    "section": "Playing with the Gaussian distribution",
    "text": "Playing with the Gaussian distribution\nWhen estimating regression parameters, the Gaussian distribution is commonly used. Often what we need to do is figure out the mean and/or the variance of the Gaussian distribution that best fit the data given a particular model structure.\nHowever, for technical reasons, it is sometimes (actually, more often than we would care to advertize broadly !) very difficult to reach convergence for a particular parameter. Visually, a trace plot would look like this"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-1",
    "href": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-1",
    "title": "Convergence trick",
    "section": "Playing with the Gaussian distribution",
    "text": "Playing with the Gaussian distribution\nEven if you run the model for many, many (many !) iterations, it never seems to converge.\nWhat should we do ?"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-2",
    "href": "slides/07_Convergence_trick/index.html#playing-with-the-gaussian-distribution-2",
    "title": "Convergence trick",
    "section": "Playing with the Gaussian distribution",
    "text": "Playing with the Gaussian distribution\nThere is a very cool trick that can help us here.\nBefore we start to discuss this trick, it is important to know that sampling a standard Gaussian distribution (\\(\\mathcal{N}(0,1)\\)) is very straight forward computationally. So, the closer we get to a standard Gaussian distribution the better it is.\nThe convergence trick\nIf we think about it, the Gaussian distribution can be translated and scaled. If we can find a way to do this mathematically, we can incorporate this into our estimation procedure.\n\nAny ideas how to do this ?"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-1",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-1",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nTranslation"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-2",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-2",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nTranslation\nMathematically, translation is the equivalent of adding or subtracting a value from the mean of the distribution.\n\nThis means that\n\\[\\mathcal{N}(\\mu, \\sigma^2)\\]\n\n\nis exactly the same as\n\\[\\mathcal{N}(0, \\sigma^2) + \\mu\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-3",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-3",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nScaling"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-4",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-4",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nScaling\nMathematically, scaling amounts to multiplying the Gaussian distribution by a positive number.\n\nThis means that\n\\[\\mathcal{N}(\\mu, \\sigma^2)\\]\n\n\nis exactly the same as\n\\[\\mathcal{N}(\\mu, 1) \\times \\sigma^2\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-5",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-5",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nThe convergence trick amounts to sampling a standard Gaussian distribution and adjusting its mean and variance from outside the distribution\n\\[\\mathcal{N}(0, 1) \\times \\sigma^2 + \\mu\\] When implementing an MCMC in Stan (or any other such software), this trick allows for convergence to be much more efficient."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-6",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-6",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-7",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-7",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\nTo do this, we need to work with a multivariate Gaussian distribution."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-8",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-8",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\nTo do this, we need to work with a multivariate Gaussian distribution.\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-9",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-9",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\n\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\nTo do this, we need to work with a multivariate Gaussian distribution.\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.\nTo show how our convergence trick works for a multivariate Gaussian distribution, let’s first visualize the two dimensional version of this distribution."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#bivariate-gaussian-distribution",
    "href": "slides/07_Convergence_trick/index.html#bivariate-gaussian-distribution",
    "title": "Convergence trick",
    "section": "Bivariate Gaussian distribution",
    "text": "Bivariate Gaussian distribution\n\\[\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  0\\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n  2 & -1\\\\\n  -1 & 2\\\\\n\\end{bmatrix}\\right)\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-10",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-10",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nMultivariate Gaussian distribution\nTranslation\nFor a multivariate distribution, a translation amounts to adding a vector of values to make the translation.\nMathematically, this means that\n\\[\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right)=\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  \\vdots\\\\\n  0\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right) + \\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-11",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-11",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nMultivariate Gaussian distribution\nScaling\nUnlike for the univariate Gaussian distribution, scalling for a multivariate distribution is a little trickier to perform… But mathematician and statistician have worked hard to figure out how to do this properly.\nHowever, we need to delve a little deeper into matrix algebra to understand how to scale a multivariate Gaussian distribution."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#scaling-a-covariance-matrix",
    "href": "slides/07_Convergence_trick/index.html#scaling-a-covariance-matrix",
    "title": "Convergence trick",
    "section": "Scaling a covariance matrix",
    "text": "Scaling a covariance matrix\n\nFirst recall that a covariance matrix \\(\\mathbf{\\Sigma}\\) is a square matrix (i.e. it is an \\(n\\times n\\) matrix).\nTo scale \\(\\mathbf{\\Sigma}\\), we cannot only multiply it by a scalar or even by a single matrix, we need to use the following matrix multiplication\n\\[\\mathbf{L}\\mathbf{\\Sigma}\\mathbf{L}^t\\] where \\(\\mathbf{L}\\) is a \\(p\\times n\\) matrix of weight to be used for the scaling (a “scaling” matrix) and \\(\\mathbf{L}^t\\) is its tranpose.\nThe technical reason why we need to use the equation above is to ensure that the resulting scaled covariance matrix also has an \\(n \\times n\\) dimension.\nIf only \\[\\mathbf{L}\\mathbf{\\Sigma}\\] is used the dimension of the resulting matrix also would be \\(p \\times p\\)."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix",
    "href": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix",
    "title": "Convergence trick",
    "section": "Square-root of a matrix",
    "text": "Square-root of a matrix\n\nBecause in our problem weighting (or scaling) matrices is usually done with other covariance matrices, to apply the matrix scaling operation described previously, we need to find a way to square-root a matrix.\nThis where the genious of André-Louis Cholesky comes to the rescue."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-1",
    "href": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-1",
    "title": "Convergence trick",
    "section": "Square-root of a matrix",
    "text": "Square-root of a matrix\nCholesky decomposition\n\nAndré-Louis Cholesky discovered a matrix decomposition approach probably around 1902 (so when he was 27 years old!), although it was attributed to him a few years after his death.\nThe Cholesky decomposition allows to decompose a square matrix in a triangular matrix, which, when multiplied by its transposed will allow us to recover the initial matrix.\nIn coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrices.\nIn math terms the Cholesky decomposition is defined as \\[\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-2",
    "href": "slides/07_Convergence_trick/index.html#square-root-of-a-matrix-2",
    "title": "Convergence trick",
    "section": "Square-root of a matrix",
    "text": "Square-root of a matrix\nCholesky decomposition\nExample\n\\[\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t\\]\n\\[\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            1 & 5 & 5\\\\\n            1 & 5 & 14\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            1 & 0 & 0\\\\\n            1 & 2 & 0\\\\\n            1 & 2 & 3 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            1 & 1 & 1\\\\\n            0 & 2 & 2\\\\\n            0 & 0 & 3 \\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-12",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-12",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nMultivariate Gaussian distribution\nScaling\n\nTo scale the following multivariate Gaussian distribution \\[\\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{\\Sigma}\\right),\\]\nThe following steps need to be applied\n\nApply the Cholesky decomposition on the scaling matrix, here \\(\\mathbf{\\Sigma}\\) \\[\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^t\\]\nMultiply the \\(\\mathbf{L}\\) matrix to a standard variance multivariate Gaussian distribution\n\n\\[\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t.\\]\nRecall, that \\(\\mathbf{I}\\) is the identity matrix."
  },
  {
    "objectID": "slides/07_Convergence_trick/index.html#the-convergence-trick-13",
    "href": "slides/07_Convergence_trick/index.html#the-convergence-trick-13",
    "title": "Convergence trick",
    "section": "The convergence trick",
    "text": "The convergence trick\nMultivariate Gaussian distribution\nIf we apply translation and scaling together on a multivariate Gaussian distribution, we get\n\\[\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\mathbf{0},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t + \\boldsymbol{\\mu}\\] When implementing in Stan some of the models we will discuss in this course, this convergence trick becomes very practical because it can lead a model to convergence much faster than without using this trick."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities",
    "href": "slides/02_5_Distribution/index.html#probabilities",
    "title": "Probability Distribution",
    "section": "Probabilities",
    "text": "Probabilities\nTo understand distributions, we first need to have a basic understanding of probabilities."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#a-bit-of-history",
    "href": "slides/02_5_Distribution/index.html#a-bit-of-history",
    "title": "Probability Distribution",
    "section": "A bit of history",
    "text": "A bit of history\n\nUnlike many other fields of science, the first contributors in the study of probability were not scholars, they were gamblers !\n\n\n\nFor example, the emperor Claudius (10 BC – 54 AD), who was an avid gambler (he had a carriage built to allow him and his party to gamble while travelling) wrote a treatise on randomness and probability.\n\n\n\n\n\n\n\n\n\n\n\nLanciani (1892) Gambling and Cheating in Ancient Rome. The North American Review 155:97-105"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#a-bit-of-history-1",
    "href": "slides/02_5_Distribution/index.html#a-bit-of-history-1",
    "title": "Probability Distribution",
    "section": "A bit of history",
    "text": "A bit of history\n\nIf you want a fun book to read about probabilities, its history and the difficulty of working with probabilities, I strongly recommend"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#the-basics-of-probabilities",
    "href": "slides/02_5_Distribution/index.html#the-basics-of-probabilities",
    "title": "Probability Distribution",
    "section": "The basics of probabilities",
    "text": "The basics of probabilities\nA probability ALWAYS ranges between 0 and 1\n\nA probability of 0 means that an event is impossible\n\n\n\nExample: The probability that a dog and a cat naturally reproduce is 0\n\n\n\nA probability of 1 means that an event is certain\n\n\n\nExample: The probability that you are in this summer school as we speak is 1"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#the-basics-of-probabilities-1",
    "href": "slides/02_5_Distribution/index.html#the-basics-of-probabilities-1",
    "title": "Probability Distribution",
    "section": "The basics of probabilities",
    "text": "The basics of probabilities\nNotation\n\nA classic way to write the probability of an event is to use the notation \\(P\\).\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nThe probability that it rains as you walk outside after the lecture is written as\n\\[P(r)\\] where \\(r\\) is the event you are interested in. Here, \\(r\\) is a rain as you walk outside after the lecture today"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities-and-events",
    "href": "slides/02_5_Distribution/index.html#probabilities-and-events",
    "title": "Probability Distribution",
    "section": "Probabilities and events",
    "text": "Probabilities and events\n\nWhen dealing with discrete (countable) events, it is very practical to know the number of events that can occur.\n\n\n\nIn the simplest case, we can either measure the probability of an event to occur or not.\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIt can either rain or not. Mathematically, the probability that it rains is written as\n\\[P(r)\\] and the probability that it does not rain would be written as\n\\[1 - P(r)\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities-and-events-1",
    "href": "slides/02_5_Distribution/index.html#probabilities-and-events-1",
    "title": "Probability Distribution",
    "section": "Probabilities and events",
    "text": "Probabilities and events\n\nUsually, these basic notions of probabilities are presented using coin flipping. When a coin is flip, it is usually assumed that\n\\[P(r)=0.5\\]\n\n\n\nHowever, in probability theory, P(r) can have any value ranging between 0 and 1.\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nWhat do you think is the probability that it will rain at the end of the lecture?"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#probabilities-and-events-2",
    "href": "slides/02_5_Distribution/index.html#probabilities-and-events-2",
    "title": "Probability Distribution",
    "section": "Probabilities and events",
    "text": "Probabilities and events\nAt this point we can note that when we add the probabilites of all possible events, the sum will always be 1\n\nExample\n\n\n\n\n\n\n\n\n\n\\[P(r) + (1-P(r)) = 1\\]\n\n\nThis is true only if the events are independent from each other"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#independent",
    "href": "slides/02_5_Distribution/index.html#independent",
    "title": "Probability Distribution",
    "section": "Independent !?",
    "text": "Independent !?\n\nEvents that are independent from each other means that if an event occurs it is in no way related to the occurrence of another event.\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIf we assume that weather events like a rainy day are independent from one another, it means that if it rains today it is unrelated to the weather of yesterday or tomorrow.\n\n\n\n\nNote : This can be a good or a bad or dangerous assumption to make depending on the problem you are working on."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\n\n\nJacob Bernoulli (1655 - 1705)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-1",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-1",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nThe probability distribution (or probability mass function) of the Bernoulli distribution defines the probability of an event to occur given that there is only one other event that can occur (e.g. rain or no rain)\n\nClassically, we will give a value of 1 to one event (no rain) and 0 to the other (rain).\n\n\nFrom a mathematical perspective, it does not matter which event is given a 1 (or a 0). However, often it is common practice to choose how we give values based on the interpretation we make of the results."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-2",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-2",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nMathematically, the probability mass function of the Bernoulli distribution can be written as\n\\[\\begin{align*}\np \\quad & \\text{if} \\quad x =1\\\\\n(1-p) \\quad & \\text{if}\\quad  x =0\n\\end{align*}\\]\nwhere \\(p\\) is a shorthand for \\(P(x)\\) and \\(x\\) is one of two events."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-interlude",
    "href": "slides/02_5_Distribution/index.html#moment-interlude",
    "title": "Probability Distribution",
    "section": "Moment interlude",
    "text": "Moment interlude\nUsing probability distributions is practical because from them we can derive general information characterizing the each distribution.\n\nThese characteristics are know as moments of a distribution… And you know them :\n\n\n\nMean\nVariance\nSkewness\nKurtosis\n…"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution",
    "title": "Probability Distribution",
    "section": "Moments of the Bernoulli distribution",
    "text": "Moments of the Bernoulli distribution\n\nFor the sake of conciseness, in this course, we will discuss only the first two moments of distributions.\nMean\n\\[p\\]\nExample\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.14\\) in any given day, it means that, on average in a week (7 days) we should expect it will rain 1 day."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-1",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-1",
    "title": "Probability Distribution",
    "section": "Moments of the Bernoulli distribution",
    "text": "Moments of the Bernoulli distribution\n\nVariance\n\\[p(1-p)\\] Example\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.5\\) in any given day, it means that, across multiple weeks, some weeks might have no rain while some weeks it might rain all days because the variance is\n\\[p(1-p)=0.5\\times(1-0.5)=0.25\\quad\\text{and}\\quad \\sqrt{0.25} = 0.5\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-2",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-bernoulli-distribution-2",
    "title": "Probability Distribution",
    "section": "Moments of the Bernoulli distribution",
    "text": "Moments of the Bernoulli distribution\nIf you want to go deeper down and learn about the other moments of the Bernoulli distribution (as well as other aspect of the distribution), take a look at the Wikipedia page of the Bernoulli distribution\nhttps://en.wikipedia.org/wiki/Bernoulli_distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#lets-make-it-more-complicated",
    "href": "slides/02_5_Distribution/index.html#lets-make-it-more-complicated",
    "title": "Probability Distribution",
    "section": "Let’s make it more complicated",
    "text": "Let’s make it more complicated\nSo far, we focused on a situation where the two events to consider either occur or not.\n\nThere are many problems where interest lies in studying the likeliness of an event occurring over a known number of independent trials.\n\n\nExample\nHow many rainy day will there be during the five days of our summer school ?"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nPut differently, the binomial distribution is designed to approach questions where we are interested in finding the number of success (e.g. it rains !) out of a known set of independent trials (e.g. the five days of the summer school).\n\nThe binomial distribution is a generalisation of the Bernoulli distribution\n\n\nIt is a common distribution used when sampling is done with replacement\n\n\nLet’s take a look at the math of the Binomial distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution-1",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution-1",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nProbability mass function\n\\[\\binom{n}{k}p^k(1-p)^{n-k}\\]\nwhere\n\n\\(n\\) : Number of trails\n\\(k\\) : Number of success (an event occurs)\n\\(p\\) : Probability that an event occurs\n\nNote that \\(n \\ge k\\)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#mathematical-technicalities-interlude",
    "href": "slides/02_5_Distribution/index.html#mathematical-technicalities-interlude",
    "title": "Probability Distribution",
    "section": "Mathematical technicalities interlude",
    "text": "Mathematical technicalities interlude\n\n\\[\\binom{n}{k}\\]\n\n\n\n\\[\\frac{n!}{k!(n-k)!}\\]\n\n\n\n\n\\[\\frac{n\\times(n-1)\\times(n-2)\\times\\dots\\times 2\\times 1}{(k\\times(k-1)\\times(k-2)\\times\\dots\\times 2\\times 1)(n-k)\\times(n-k-1)\\times(n-k-2)\\times\\dots\\times 2\\times 1}\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution",
    "title": "Probability Distribution",
    "section": "Moment of the binomial distribution",
    "text": "Moment of the binomial distribution\n\nAgain, for conciseness, we will focus on the first moment (mean) and second moment (variance) of the binomial distribution.\n\n\n\nMean \\[np\\]\n\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.14\\) in any given day of the 5 days of the summer school, it means that on average we expect it will rain 0.7 days of the summer school (so 1 or no days)\n\\[np = 5 \\times 0.14 = 0.7\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution-1",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-binomial-distribution-1",
    "title": "Probability Distribution",
    "section": "Moment of the binomial distribution",
    "text": "Moment of the binomial distribution\n\nVariance\n\\[np(1-p)\\]\n\n\n\nExample\n\n\n\n\n\n\n\n\n\n\n\nIf the probability that it rains is \\(p=0.5\\) in any given day, it means that, across multiple weeks (7 days), roughly speaking some weeks might have 1 days of rain while others might have 5 because the variance is\n\\[np(1-p)=7 \\times 0.5\\times(1-0.5)=1.75\\quad\\text{and}\\quad \\sqrt{1.75} = 1.3229\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moments-of-the-binomial-distribution",
    "href": "slides/02_5_Distribution/index.html#moments-of-the-binomial-distribution",
    "title": "Probability Distribution",
    "section": "Moments of the binomial distribution",
    "text": "Moments of the binomial distribution\nIf you want to learn more about the other moments of the binomial distribution (as well as other aspect of the distribution), take a look at the Wikipedia page of the binomial distribution\nhttps://en.wikipedia.org/wiki/binomial_distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution-2",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution-2",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nThe binomial distribution is related to many other probability distribution\n\n\nBernoulli distribution (as we have seen)\n\n\n\n\nPoisson distribution (when there are an infinite number of trials while \\(np\\) converge to a finite value)\n\n\n\n\nNormal distribution…"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#jia-xian-triangle",
    "href": "slides/02_5_Distribution/index.html#jia-xian-triangle",
    "title": "Probability Distribution",
    "section": "Jia Xian triangle",
    "text": "Jia Xian triangle"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-1",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-1",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-2",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-2",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\n\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\n\n\n\nIf \\(n = 3\\)\nWhen \\(k=0\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{0}\\times0.5^0 \\times (1-0.5)^{(3-0)}=0.125\\]\n\n\n\n\nWhen \\(k=1\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{1}\\times0.5^1 \\times (1-0.5)^{(3-1)}=0.375\\]\n\n\n\n\nWhen \\(k=2\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{2}\\times0.5^1 \\times (1-0.5)^{(3-2)}=0.375\\]\n\n\n\n\nWhen \\(k=3\\)\n\n\n\\[\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{3}\\times0.5^1 \\times (1-0.5)^{(3-3)}=0.125\\]"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-3",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-3",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 3\\)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-4",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-4",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 10\\)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-5",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-5",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 50\\)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pascals-triangle-6",
    "href": "slides/02_5_Distribution/index.html#pascals-triangle-6",
    "title": "Probability Distribution",
    "section": "Pascal’s triangle",
    "text": "Pascal’s triangle\nThe Pascal’s triangle is directly related to the binomial distribution with \\(p=0.5\\).\nIf \\(n = 200\\)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-and-gaussian-distribution",
    "href": "slides/02_5_Distribution/index.html#binomial-and-gaussian-distribution",
    "title": "Probability Distribution",
    "section": "Binomial and Gaussian distribution",
    "text": "Binomial and Gaussian distribution\nIf the number trials (\\(n\\)) is large enough, it approximate to a Gaussian distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#gaussian-normal-distribution",
    "href": "slides/02_5_Distribution/index.html#gaussian-normal-distribution",
    "title": "Probability Distribution",
    "section": "Gaussian (Normal) distribution",
    "text": "Gaussian (Normal) distribution\n\n\nCarl Friedrich Gauss (1777-1855)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-1",
    "href": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-1",
    "title": "Probability Distribution",
    "section": "Gaussian (Normal) distribution",
    "text": "Gaussian (Normal) distribution\nUnlike the binomial distribution, the Gaussian distribution is a continuous distribution\n\nIt is the a very common distribution that is underlying many random natural phenomenon and it is the basis of statistical theory\n\n\nLet’s take a look at the mathematical formulation of the Gaussian distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-2",
    "href": "slides/02_5_Distribution/index.html#gaussian-normal-distribution-2",
    "title": "Probability Distribution",
    "section": "Gaussian (Normal) distribution",
    "text": "Gaussian (Normal) distribution\nProbability density function\n\\[\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}\\]\nwhere\n\n\\(x\\) : continuous variable of interest\n\\(\\mu\\) : The mean of the distribution\n\\(\\sigma\\) : The standard deviation of the distribution"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution",
    "title": "Probability Distribution",
    "section": "Moment of the Gaussian distribution",
    "text": "Moment of the Gaussian distribution\nMean\n\\[\\mu\\]\nExample\n\n\nLet’s say we measure the length of the right wing of individual of this species of (angry) bird, it is expected that the wing length will follow a Gaussian distribution with a mean of \\(\\mu\\). We will look at this in more details in the practical exercices later today."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution-1",
    "href": "slides/02_5_Distribution/index.html#moment-of-the-gaussian-distribution-1",
    "title": "Probability Distribution",
    "section": "Moment of the Gaussian distribution",
    "text": "Moment of the Gaussian distribution\nVariance\n\\[\\sigma^2\\]\nExample\n\n\nLet’s say we measure the length of the right wing of individual of this species of (angry) bird, it is expected that the wing length will follow a Gaussian distribution with a variance of \\(\\sigma^2\\). We will look at this in more details in the practical exercices later today."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#general-properties-of-distributions",
    "href": "slides/02_5_Distribution/index.html#general-properties-of-distributions",
    "title": "Probability Distribution",
    "section": "General properties of distributions",
    "text": "General properties of distributions\nIn R, there are 4 functions associated to every distribution. As an example, for the Gaussian distribution, they are\n\nrnorm\ndnorm\npnorm\nqnorm\n\n\nKnowing what these functions do will be very useful for this course"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#rnorm",
    "href": "slides/02_5_Distribution/index.html#rnorm",
    "title": "Probability Distribution",
    "section": "rnorm",
    "text": "rnorm\n\nThe r in rnorm is for random\n\n\n\nThis function allows us to randomly sample directly from the distribution of interest.\n\n\n\nExample\n\n\n\n\n\n\nLet’s assume that we look at the historical record and gather the minimum temperature measured on today’s date for the past 40 years here in Jouvence. Let’s assume that the random sample of 40 temperature measurements has an average of \\(4^{\\circ}C\\) and a standard deviation of \\(2^{\\circ}C\\). We can simulate these values as follow\n\n\nrnorm(40, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#rnorm40-mean-4-sd-2",
    "href": "slides/02_5_Distribution/index.html#rnorm40-mean-4-sd-2",
    "title": "Probability Distribution",
    "section": "rnorm(40, mean = 4, sd = 2)",
    "text": "rnorm(40, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#dnorm",
    "href": "slides/02_5_Distribution/index.html#dnorm",
    "title": "Probability Distribution",
    "section": "dnorm",
    "text": "dnorm\n\nThe d in dnorm is for density\n\n\n\nThis function gives the height of the distribution for a chosen value.\n\n\n\nExample\n\n\n\n\n\n\nIf we assume that the average temperature at this time of the year is \\(4^{\\circ}C\\) with a standard deviation of \\(2^{\\circ}C\\), we can calculate that the likeliness that a temperature of \\(6^{\\circ}C\\) to occur is\n\n\ndnorm(6, mean = 4, sd = 2)\n\n[1] 0.1209854"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#dnorm6-mean-4-sd-2",
    "href": "slides/02_5_Distribution/index.html#dnorm6-mean-4-sd-2",
    "title": "Probability Distribution",
    "section": "dnorm(6, mean = 4, sd = 2)",
    "text": "dnorm(6, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#qnorm",
    "href": "slides/02_5_Distribution/index.html#qnorm",
    "title": "Probability Distribution",
    "section": "qnorm",
    "text": "qnorm\n\nThe q in qnorm is for quantile\n\n\n\nThis function gives the value of the distribution given a certain density\n\n\n\nExample\n\n\n\n\n\n\nIf we assume that the average temperature at this time of the year is \\(4^{\\circ}C\\) with a standard deviation of \\(2^{\\circ}C\\), qnorm allows us to calculate the temperature expected to be obtained in the lowest quartile (1/4). It is calculate as\n\n\nqnorm(0.25, mean = 4, sd = 2)\n\n[1] 2.65102"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#qnorm0.25-mean4-sd2",
    "href": "slides/02_5_Distribution/index.html#qnorm0.25-mean4-sd2",
    "title": "Probability Distribution",
    "section": "qnorm(0.25, mean=4, sd=2)",
    "text": "qnorm(0.25, mean=4, sd=2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pnorm",
    "href": "slides/02_5_Distribution/index.html#pnorm",
    "title": "Probability Distribution",
    "section": "pnorm",
    "text": "pnorm\nThe p in pnorm is for probability distribution function\n\nThis function gives the integral (area under the curve) up to a specified value.\n\n\nThis is particularly useful because it informs us about the probability that an event is likely to occur (of course assuming a it follows a normal distribution)."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pnorm-1",
    "href": "slides/02_5_Distribution/index.html#pnorm-1",
    "title": "Probability Distribution",
    "section": "pnorm",
    "text": "pnorm\nExample\n\nIf we assume that the average temperature at this time of the year is \\(4^{\\circ}C\\) with a standard deviation of \\(2^{\\circ}C\\), pnorm will tell us that the probability to have a temperature lower or equal to \\(6^{\\circ}C\\). This is calculated as\n\npnorm(6, mean = 4, sd = 2)\n\n[1] 0.8413447"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#pnorm6-mean-4-sd-2",
    "href": "slides/02_5_Distribution/index.html#pnorm6-mean-4-sd-2",
    "title": "Probability Distribution",
    "section": "pnorm(6, mean = 4, sd = 2)",
    "text": "pnorm(6, mean = 4, sd = 2)"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-and-bernoulli-distribution",
    "href": "slides/02_5_Distribution/index.html#binomial-and-bernoulli-distribution",
    "title": "Probability Distribution",
    "section": "Binomial and Bernoulli distribution",
    "text": "Binomial and Bernoulli distribution\nAs I mentionned previsouly, these function are available in base R for a large number of distributions."
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#binomial-distribution-3",
    "href": "slides/02_5_Distribution/index.html#binomial-distribution-3",
    "title": "Probability Distribution",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\nrbinom\ndbinom\nqbinom\npbinom"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-3",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-3",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\nHowever, sometimes we must know a little bit of theory (as I have shown today) to use the right function.\n\nrbinom\ndbinom\nqbinom\npbinom"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#bernoulli-distribution-4",
    "href": "slides/02_5_Distribution/index.html#bernoulli-distribution-4",
    "title": "Probability Distribution",
    "section": "Bernoulli distribution",
    "text": "Bernoulli distribution\n\nrbinom\ndbinom\nqbinom\npbinom\n\nWith size = 1"
  },
  {
    "objectID": "slides/02_5_Distribution/index.html#other-distributions",
    "href": "slides/02_5_Distribution/index.html#other-distributions",
    "title": "Probability Distribution",
    "section": "Other distributions",
    "text": "Other distributions\nStatisticians and biologists have been very, very (!!) creative in proposing new probability distribution for specific problems\n\nIf you want to learn about the diversity of distributions that are out there, take a look at :\nhttps://en.wikipedia.org/wiki/List_of_probability_distributions\n\n\nMany of them have been implemented in R, either in base R or specialized packages\n\n\nIf you want to know if you favourite distribution has been implemented in R take a look at\nhttps://cran.r-project.org/web/views/Distributions.html"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far",
    "href": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far",
    "title": "Gaussian process",
    "section": "Hierarchical models so far",
    "text": "Hierarchical models so far\n\nAn already (very !) general formulation\n\nSo far, we have built hierarchical models that can be integrated into the following framework.\n\n\n\n\n\\[(\\mathbf{y}|\\mathbf{X},\\mathbf{Z}, \\boldsymbol{\\beta}, \\mathbf{b}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma_\\mathbf{y}^2\\mathbf{I})\\]\n\n\n\n\nwhere\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mu, \\mathbf{\\Sigma})\\]\n\n\n\n\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory variables)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix\n\n\n\n\\(\\mathbf{Z}\\) is designed matrix of “explanatory” variables with \\(n\\) rows (samples) and \\(q\\) columns\n\\(\\mu\\) is a vector defining the average importance of hierarchical parameters\n\\(\\mathbf{\\Sigma}\\) is a matrix defining the covariance structure of hierarchical parameters"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far-1",
    "href": "slides/09_Gaussian_process/index.html#hierarchical-models-so-far-1",
    "title": "Gaussian process",
    "section": "Hierarchical models so far",
    "text": "Hierarchical models so far\nAn already (very !) general formulation\nAnother way to write this generalized formulation is\n\\[\\mathbf{y}_i = \\mathbf{X}_{ij} \\boldsymbol{\\beta}_j + \\mathbf{Z}_{ik}\\mathbf{b}_{k} + \\boldsymbol{\\varepsilon}_{ij}\\] where\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mu, \\mathbf{\\Sigma})\\] and\n\\[\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\\]"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#even-more-complex-hierarchical-models",
    "href": "slides/09_Gaussian_process/index.html#even-more-complex-hierarchical-models",
    "title": "Gaussian process",
    "section": "Even more complex hierarchical models!?",
    "text": "Even more complex hierarchical models!?\nSo far we have seen many (!) types of hierarchical models, which got increasingly more complex in their structure.\n\nLet’s continue on that slippery slope…\n\n\nWould you know how to constraint (spatially, temporally, phylogenetically, etc.) such a model ?"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#even-more-complex-hierarchical-models-1",
    "href": "slides/09_Gaussian_process/index.html#even-more-complex-hierarchical-models-1",
    "title": "Gaussian process",
    "section": "Even more complex hierarchical models!",
    "text": "Even more complex hierarchical models!\nIf we get back to a model that has a single hierarchy on the intercept such that\n\\[\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{b},\\sigma^2)\\]\n\nwhere\n\\[\\mathbf{b} \\sim \\mathcal{N}(\\mu, \\sigma^2).\\]\n\n\nIf we want to account for a constraint on the previously presented model, we can rewrite the equation fo \\(\\mathbf{b}\\) as\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\mu, f(d)\\right)\\]\n\n\nwhere\n\n\\(f(d)\\) is a function of a (spatial, temporal, phylogenetic, …) distance matrix"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#even-more-complex-hierarchical-models-2",
    "href": "slides/09_Gaussian_process/index.html#even-more-complex-hierarchical-models-2",
    "title": "Gaussian process",
    "section": "Even more complex hierarchical models!",
    "text": "Even more complex hierarchical models!\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\mu, f(d)\\right)\\]\n\nWhat this equation means conceptually is that the variance associated to \\(\\mathbf{b}\\) is not a constant, it changes based on distance (across space, time, phylogeny, etc.). This is known as a Gaussian process."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#a-bit-of-history",
    "href": "slides/09_Gaussian_process/index.html#a-bit-of-history",
    "title": "Gaussian process",
    "section": "A bit of history",
    "text": "A bit of history\n\nIn statistics, Gaussian processes have a unique history. The development of this type of model is closely linked to the estimation of mineral deposits.\n\n\nSpatial Gaussian processes are also called geostatistical models, where the prefix geo refers to geology, not geography, as one may be led to believe.\n\n\nAs mining engineers are at the root of the development of Gaussian processes, the language associated with this type of model is influenced by this field."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#a-bit-of-history-1",
    "href": "slides/09_Gaussian_process/index.html#a-bit-of-history-1",
    "title": "Gaussian process",
    "section": "A bit of history",
    "text": "A bit of history\nGaussian processes have been developed in the 1950s by\n\n\n\n\n\n\n\n\nDaniel G. Krige (1919–2013)\n\n\n\n\n\n\n\n\nGeorges Mathéron (1930–2000)"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#assumption-with-gaussian-processes",
    "href": "slides/09_Gaussian_process/index.html#assumption-with-gaussian-processes",
    "title": "Gaussian process",
    "section": "Assumption with Gaussian processes",
    "text": "Assumption with Gaussian processes\n\nIn general, when defining a Gaussian process, we make the following assumptions:\n\n\n\nThe closer two samples are, the more similar they are.\n\n\n\n\nAfter a certain distance, it is no longer necessary to consider that a sample influences another.\n\n\n\nNote The distance of influence of a sample on another can be different depending on what is being studied, where it is being studied and when it is being studied"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd",
    "href": "slides/09_Gaussian_process/index.html#fd",
    "title": "Gaussian process",
    "section": "\\(f(d)\\)",
    "text": "\\(f(d)\\)\n\nSo, what does \\(f(d)\\) looks like exactly ?\n\n\nIn theory, \\(f(d)\\) can be anything…\n\n\nHowever, in practice, there are particularities of the functions that are defined by the assumptions we impose on our model.\n\n\nHere is a classic structure these variance function take"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary",
    "href": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - A bit of vocabulary",
    "text": "\\(f(d)\\) - A bit of vocabulary\n\nNugget effect"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-1",
    "href": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-1",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - A bit of vocabulary",
    "text": "\\(f(d)\\) - A bit of vocabulary\nRange"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-2",
    "href": "slides/09_Gaussian_process/index.html#fd---a-bit-of-vocabulary-2",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - A bit of vocabulary",
    "text": "\\(f(d)\\) - A bit of vocabulary\nSill"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fd---types-of-functions",
    "href": "slides/09_Gaussian_process/index.html#fd---types-of-functions",
    "title": "Gaussian process",
    "section": "\\(f(d)\\) - types of functions",
    "text": "\\(f(d)\\) - types of functions\n\nMany functions that have been proposed have this general shape\n\n\nLet’s first study the exponential function\n\\[C_0 + C_1 \\left(1-e^{-d/a}\\right)\\]\n\n\nwhere\n\n\\(C_0\\) is the nugget effect\n\n\n\n\n\\(C_1\\) is the sill\n\n\n\n\n\\(d\\) is the distance\n\n\n\n\n\\(a\\) is the range"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#exponential-function",
    "href": "slides/09_Gaussian_process/index.html#exponential-function",
    "title": "Gaussian process",
    "section": "Exponential function",
    "text": "Exponential function\nNugget : 2 – Sill : 5 – Range : 10"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-illustrative-example",
    "href": "slides/09_Gaussian_process/index.html#an-illustrative-example",
    "title": "Gaussian process",
    "section": "An illustrative example",
    "text": "An illustrative example\n\nTo present how Gaussian processes can be used, let’s study the distribution of Sylvilagus oviparus in Montréal.\n\n\nA few characteristics of Sylvilagus oviparus\n\nThey are found mainly in urban parks of Montréal and are very efficient at hiding in hollow trees and burrows.\n\n\n\n\nThey lay their eggs (often pastel-coloured) on the Sunday following the first full moon after the spring equinox.\n\n\n\n\nThey move well in an urban setting and are not affected by the level of urbanisation"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#a-typical-member-of-the-species",
    "href": "slides/09_Gaussian_process/index.html#a-typical-member-of-the-species",
    "title": "Gaussian process",
    "section": "A typical member of the species",
    "text": "A typical member of the species"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#distribution-of-s.-oviparus-in-montréal",
    "href": "slides/09_Gaussian_process/index.html#distribution-of-s.-oviparus-in-montréal",
    "title": "Gaussian process",
    "section": "Distribution of S. oviparus in Montréal",
    "text": "Distribution of S. oviparus in Montréal\n\nIn 2015, a survey was carried out to find Sylvilagus oviparus in Montréal’s park. Here are the data . . .\n\n\n\n\n\n\n\n\n\n\nWithin the censused park\n\nBlue parks : observed\nPink parks : not observed"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#model",
    "href": "slides/09_Gaussian_process/index.html#model",
    "title": "Gaussian process",
    "section": "Model",
    "text": "Model\n\nSince we have presence-absence data\n\\[P(y = 1) = \\frac{\\exp(b)}{1 - \\exp(b)}\\]\n\n\nwhere \\[b \\sim{\\cal N}\\left(0, C_0 +C_1\\left(1 - e^{\\frac{-d}{a}}\\right)\\right)\\]"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#fitting-the-model",
    "href": "slides/09_Gaussian_process/index.html#fitting-the-model",
    "title": "Gaussian process",
    "section": "Fitting the model",
    "text": "Fitting the model\nIf we estimate the parameters of the model presented in the previous slide we get a Gaussian process that looks like\n\n\nWhat can we learn from this model ?"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#kringing",
    "href": "slides/09_Gaussian_process/index.html#kringing",
    "title": "Gaussian process",
    "section": "Kringing",
    "text": "Kringing\nIf we want to interpolate across the region of interest, this is known as kriging.\n\nSimple kriging\nSo far, we have seen the most simplistic Gaussian process where there is not even any intercept that is estimated. In short, the model is constructed using only the covariance function.\n\n\nIn a linear regression perspective, this means that\n\\[\\mathbf{y}\\sim\\mathcal{N}(0, f(d))\\]"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#kringing-1",
    "href": "slides/09_Gaussian_process/index.html#kringing-1",
    "title": "Gaussian process",
    "section": "Kringing",
    "text": "Kringing\n\nOrdinary kriging\n\nIf we want to account for an intercept in the model such that\n\\[\\mathbf{y}\\sim\\mathcal{N}(\\beta_0, f(d))\\]\nThis is known as ordinary kriging.\n\n\nUniversal kriging\n\nIf we want to account for one or more explanatory variables in the model such that\n\\[\\mathbf{y}\\sim\\mathcal{N}(\\beta_0 + \\mathbf{X}\\beta, f(d))\\] This is known as Universal kriging."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#prediction-map",
    "href": "slides/09_Gaussian_process/index.html#prediction-map",
    "title": "Gaussian process",
    "section": "Prediction map",
    "text": "Prediction map"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#square-matrix",
    "href": "slides/nn_extra_slides/index.html#square-matrix",
    "title": "Extra stuff",
    "section": "Square matrix",
    "text": "Square matrix\nThe square matrix has as many rows at it has columns \\[\n\\mathbf{B} = \\begin{bmatrix}\n                B_{11} & B_{12} & \\dots & B_{1j} & \\dots & B_{1n}\\\\\n                B_{21} & B_{22} & \\dots & B_{2j} & \\dots & B_{2n}\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              B_{i1} & B_{i2} & \\dots & B_{ij} & \\dots & B_{in}\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            B_{n1} & B_{n2} & \\dots & B_{nj} & \\dots & B_{nn}\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#determinant-of-a-matrix",
    "href": "slides/nn_extra_slides/index.html#determinant-of-a-matrix",
    "title": "Extra stuff",
    "section": "Determinant of a matrix",
    "text": "Determinant of a matrix\nNot sure if it should be included or not"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "href": "slides/nn_extra_slides/index.html#eigenvectors-and-eigenvalues",
    "title": "Extra stuff",
    "section": "Eigenvectors and eigenvalues",
    "text": "Eigenvectors and eigenvalues\n\nRight eigenvector is :\n\\[\\mathbf{A}\\mathbf{w} = \\lambda\\mathbf{w}\\] Left eigenvector is :\n\\[\\mathbf{v}\\mathbf{A} = \\lambda\\mathbf{v}\\]\nRules\n\n\\(\\mathbf{A}\\) has to be a square matrix\nIf \\(\\mathbf{w}\\) is an eigenvector of \\(\\mathbf{A}\\), so is \\(c\\mathbf{w}\\) for any value of \\(c \\neq0\\)\nThe right eigenvector of \\(\\mathbf{A}^T\\) is the left eigenvector of \\(\\mathbf{A}\\)\nEigenvectors are linearly independent"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-definite-matrix",
    "title": "Extra stuff",
    "section": "Positive definite matrix",
    "text": "Positive definite matrix\nIt is reasonably common when you build a hierarchical model to get an error message that state :\n\nError: Matrix X is not positive definite\n\nor similarly\n\nError: Matrix X is not positive semi-definite\n\nWhat does this mean ? Any idea ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix",
    "title": "Extra stuff",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nNerdy mathematical definition\nPositive definite matrix\n\\(\\mathbf{M}\\) is a positive definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} &gt; 0\\)\nPositive semi-definite matrix\n\\(\\mathbf{M}\\) is a positive semi-definite matrix if, for any real vector \\(\\mathbf{z}\\), \\(\\mathbf{z}^t\\mathbf{M}\\mathbf{z} \\ge 0\\)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "href": "slides/nn_extra_slides/index.html#positive-semi-definite-matrix-1",
    "title": "Extra stuff",
    "section": "Positive (semi-)definite matrix",
    "text": "Positive (semi-)definite matrix\nChecking if a matrix is positive (semi-)definite\nThe properties of eigenvalues can be used to detect if a matrix is positive (semi-) definite.\nAll we have to do is look at the eigenvalue of a square matrix.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than 0, matrix \\(\\mathbf{M}\\) is positive definite.\nIf all eigenvalues of a matrix \\(\\mathbf{M}\\) larger than or equal ro 0, matrix \\(\\mathbf{M}\\) is positive semi-definite."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#dot-product",
    "href": "slides/nn_extra_slides/index.html#dot-product",
    "title": "Extra stuff",
    "section": "Dot product",
    "text": "Dot product\n\\[\\mathbf{v} \\cdot \\mathbf{x}= v_1x_1+v_2x_2+\\dots + v_nx_n\\]\n\n\\[\n            \\begin{bmatrix}\n                3 & 1\\\\\n            \\end{bmatrix}\n            \\cdot\n            \\begin{bmatrix}\n                2\\\\ 5\\\\\n            \\end{bmatrix} =\n                3 \\times  2 + 1 \\times 5 = 11\n\\]\n\nIn R\n\nv &lt;- c(3, 1)\nx &lt;- c(2,5)\nsum(v * x)\n\n[1] 11"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\\[\n\\begin{align*}\n        1 &= 3\\beta_1 + 5\\beta_2 - 4\\beta_3 \\\\\n        0 &= \\beta_1 - 2\\beta_2 + 3\\beta_3\\\\\n        1 &= 4\\beta_1 + 6\\beta_2 + 5\\beta_3\\\\\n    \\end{align*}\n\\] \\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-1",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-1",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we mathematically solve for \\(\\boldsymbol{\\beta}\\)?\n\\[\n    \\begin{align*}\n        \\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{X}^{-1}\\mathbf{X}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\mathbf{I}\\boldsymbol{\\beta}\\\\\n        \\mathbf{X}^{-1}\\mathbf{y} &= \\boldsymbol{\\beta}\\\\\n    \\end{align*}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-2",
    "href": "slides/nn_extra_slides/index.html#solving-systems-of-linear-equation-2",
    "title": "Extra stuff",
    "section": "Solving systems of linear equation",
    "text": "Solving systems of linear equation\n\n\\[\n\\mathbf{y} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\mathbf{X} \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\qquad \\boldsymbol{\\beta}\n\\] \\[\n        \\begin{bmatrix}\n            1\\\\ 0\\\\ 1\\\\\n        \\end{bmatrix}=\n        \\begin{bmatrix}\n            3 & 5 & -4\\\\\n            1 & -2 & 3\\\\\n            4 & 6 & 5 \\\\\n        \\end{bmatrix}\n        \\begin{bmatrix}\n            \\beta_1\\\\ \\beta_2\\\\ \\beta_3\\\\\n        \\end{bmatrix}\n\\]\nHow do we solve for \\(\\boldsymbol{\\beta}\\) in R?\n\nX &lt;- matrix(c(3, 1, 4, 5, -2, 6, -4, 3, 5), nrow = 3, ncol = 3)\ny &lt;- c(1, 0, 1)\n\n(beta &lt;- solve(X, y))\n\n[1]  0.20000000  0.05714286 -0.02857143"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior",
    "href": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior",
    "title": "Extra stuff",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nConjugate priors\nThese types of priors are convenient to use because\n\nThey are computationally faster to use\nThey can be interepreted as additional data\n\nWhy are they useful?\nThere is no need to write the likelihood down when using them. All that needs to be done is to sample them to obtain a parameter estimation."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior-1",
    "href": "slides/nn_extra_slides/index.html#a-few-words-about-the-prior-1",
    "title": "Extra stuff",
    "section": "A few words about the prior",
    "text": "A few words about the prior\nConjugate priors\nWhat does it mean to be of the same functional form?\nIt means that both distribution have th same mathematical structure.\n\n\nBinomial distribution \\[\\theta^a(1-\\theta)^b\\]\n\nBeta distribution \\[\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\\]\n\nhttps://en.wikipedia.org/wiki/Conjugate_prior"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#move-this-in-another-place",
    "href": "slides/nn_extra_slides/index.html#move-this-in-another-place",
    "title": "Extra stuff",
    "section": "Move this in another place",
    "text": "Move this in another place\nTechnically, we can sample all \\(\\boldsymbol{\\beta}_{f[l]}\\) independently, however, using multivariate Gaussian distribution, we can sample the \\(\\boldsymbol{\\beta}_{f}\\) for all levels of the factor in one go as\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\] where\n\n\\(\\boldsymbol{\\mu}_{f}\\) is a vector of \\(k\\) means, one for each level of the factor\n\\(\\mathbf{D}_f\\) is a \\(k\\times k\\) diagonal matrix with variance term on the diagonal"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean",
    "href": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean",
    "title": "Extra stuff",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nThe structure of matrix \\(\\mathbf{D}_f\\) can be considered in two different ways in\n\\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\]\nWritten in the general form as we did in the equation above, we assume that all variance on the diagonal are potentially different. Or in other words, the variance in each group is assumed to be different\n\n\n\\[\\mathbf{D}_f = \\begin{bmatrix}\n                \\sigma^2_{f[1]} & 0 & \\dots & 0 & \\dots & 0\\\\\n                0 & \\sigma^2_{f[2]} & \\dots & 0 & \\dots & 0\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              0 & 0 & \\dots & \\sigma^2_{f[l]} & \\dots & 0\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f[k]}\\\\\n        \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean-1",
    "href": "slides/nn_extra_slides/index.html#hierarchy-on-the-intercepts-mean-1",
    "title": "Extra stuff",
    "section": "Hierarchy on the intercept’s mean",
    "text": "Hierarchy on the intercept’s mean\n\nHowever, it can be assumed to be all the same variance regardless of the group considered\n\n\\[\\mathbf{D}_f = \\begin{bmatrix}\n                \\sigma^2_{f} & 0 & \\dots & 0 & \\dots & 0\\\\\n                0 & \\sigma^2_{f} & \\dots & 0 & \\dots & 0\\\\\n                \\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n              0 & 0 & \\dots & \\sigma^2_{f} & \\dots & 0\\\\\n            \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n            0 & 0 & \\dots & 0 & \\dots & \\sigma^2_{f}\\\\\n        \\end{bmatrix}\\]\n\nIn this case, \\(\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\mathbf{D}_f)\\) can be rewritten as \\[\\boldsymbol{\\beta}_{f} \\sim \\mathcal{MVN}(\\boldsymbol{\\mu}_{f}, \\sigma^2_{f}\\mathbf{I})\\] Note: This is essentially the same thing as a one-way analysis of variance."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-very-general-formulation",
    "href": "slides/nn_extra_slides/index.html#a-very-general-formulation",
    "title": "Extra stuff",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nAs discuss yesterday, a linear model can be writen as\n\\[(\\mathbf{y}|\\mathbf{X}, \\boldsymbol{\\beta}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma\\mathbf{y}^2\\mathbf{I})\\]\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory varaibles)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-very-general-formulation-1",
    "href": "slides/nn_extra_slides/index.html#a-very-general-formulation-1",
    "title": "Extra stuff",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{X},\\mathbf{Z}, \\boldsymbol{\\beta}, \\mathbf{b}, \\sigma_\\mathbf{y}^2)\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma_\\mathbf{y}^2\\mathbf{I})\\]\n\n\nwhere\n\n\\(\\mathbf{y}\\) is a vector quantifying a response variable of length \\(n\\)\n\\(\\mathbf{X}\\) is a matrix of explanatory variables with \\(n\\) rows (samples) and \\(p\\) columns (explanatory variables)\n\\(\\boldsymbol{\\beta}\\) is a vector \\(p\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{X}\\)\n\\(\\sigma_\\mathbf{y}^2\\) is a measure of variance of the error in the regression model\n\\(\\mathbf{I}\\) is an \\(n \\times n\\) identity matrix\n\n\n\n\\(\\mathbf{Z}\\) is another matrix of explanatory variables with \\(n\\) rows (samples) and \\(q\\) columns (explanatory variables)\n\\(\\mathbf{b}\\) is a vector \\(q\\) pararameters weighting the importance of each explanatory variables in \\(\\mathbf{Z}\\)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#a-very-general-formulation-2",
    "href": "slides/nn_extra_slides/index.html#a-very-general-formulation-2",
    "title": "Extra stuff",
    "section": "A (very !) general formulation",
    "text": "A (very !) general formulation\n\nA hierarchical model is a generalization of the linear model such that\n\\[(\\mathbf{y}|\\mathbf{b} )\\sim \\mathcal{MVN}(\\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b}, \\sigma^2\\mathbf{I})\\] What is also noticeable in this model is the conditional relationship between \\(\\mathbf{y}\\) and \\(\\mathbf{b}\\).\nSpecifically, in this formulation,\n\\[\\mathbf{b}\\sim \\mathcal{MVN}(\\mathbf{0}, \\mathbf{\\Sigma})\\] where \\(\\mathbf{\\Sigma}\\) is a covariance matrix.\nBased on this general formulation, we can now define all unconstrained hierarchical models."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nFor simplicity, let’s assume that we are monitoring the behaviour of the mallard every minutes and that we are recording whether it is\nOn land\n\n\n\n\n\nIn the water"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-1",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-1",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nUsing this information, we can draw diagram defining how the behaviour of the mallard changes at every time steps"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-2",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-2",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nIn a markov chain, we assume that we know how probable it is to go from one behaviour (land) to another (water)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-3",
    "href": "slides/nn_extra_slides/index.html#markov-chain-monte-carlo-mcmc-3",
    "title": "Extra stuff",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\n\nIn an MCMC, we assume that the likeliness of passing from one behaviour (land) to another (water) depends on a statistical distribution."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#addition-and-substraction",
    "href": "slides/nn_extra_slides/index.html#addition-and-substraction",
    "title": "Extra stuff",
    "section": "Addition and Substraction",
    "text": "Addition and Substraction\n\\[\\mathbf{C} = \\mathbf{A}\\pm \\mathbf{B}\\] \\[C_{ij} = A_{ij} \\pm B_{ij}\\]\n\n\\[\\begin{bmatrix}\n                3 & 5\\\\\n                1 & -2\\\\\n            \\end{bmatrix} +\n            \\begin{bmatrix}\n                2 & 1\\\\\n                4 & -2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                3+2 & 5+1\\\\\n                1+4 & -2-2\\\\\n            \\end{bmatrix} =\n            \\begin{bmatrix}\n                5 & 6\\\\\n                5 & -4\\\\\n            \\end{bmatrix}\\]\n\nIn R\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nB &lt;- matrix(c(2, 4, 1, -2), nrow = 2, ncol = 2)\nA + B\n\n     [,1] [,2]\n[1,]    5    6\n[2,]    5   -4"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#triangular-matrix",
    "href": "slides/nn_extra_slides/index.html#triangular-matrix",
    "title": "Extra stuff",
    "section": "Triangular matrix",
    "text": "Triangular matrix\n\n\nLower triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & 0 & 0\\\\\n        3 & 0 & 0 & 0\\\\\n        0 & 4 & 3 & 0\\\\\n        9 & -5 & 4 & 3\\\\\n\\end{bmatrix}\\]\n\nUpper triangular matrix\n\\[\\begin{bmatrix}\n        -10 & 0 & -5 & 0\\\\\n        0 & 0 & 5 & 6\\\\\n        0 & 0 & 3 & 3\\\\\n        0 & 0 & 0 & 3\\\\\n      \\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#symmetric-matrix",
    "href": "slides/nn_extra_slides/index.html#symmetric-matrix",
    "title": "Extra stuff",
    "section": "Symmetric matrix",
    "text": "Symmetric matrix\nThe values on the above and below the diagonal are match so that \\(A = A^t\\)\n\n\\[\n\\begin{bmatrix}\n                3 & 4 & -10\\\\\n                4 & 5 & 7\\\\\n                -10 & 7 & -6\\\\\n        \\end{bmatrix}\n\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#matrix-inversion",
    "href": "slides/nn_extra_slides/index.html#matrix-inversion",
    "title": "Extra stuff",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nIn matrix algebra, we cannot divide a matrix by another matrix, but we can multiple it by its inverse, which gets us to the same place. Classically, the inverse of matrix \\(\\mathbf{A}\\) is defined as \\(\\mathbf{A}^{-1}\\)\nAs such, \\[\\mathbf{A}\\cdot \\mathbf{A}^{-1} = \\mathbf{I}\\]\n\n\n\nIn R\n\n\nA &lt;- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\n(Ainv &lt;- solve(A))\n\n           [,1]       [,2]\n[1,] 0.18181818  0.4545455\n[2,] 0.09090909 -0.2727273\n\nA %*% Ainv\n\n              [,1] [,2]\n[1,]  1.000000e+00    0\n[2,] -2.775558e-17    1"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#matrix-inversion-1",
    "href": "slides/nn_extra_slides/index.html#matrix-inversion-1",
    "title": "Extra stuff",
    "section": "Matrix inversion",
    "text": "Matrix inversion\nInverting a diagonal matrix\n\\[D^{-1}=\n      \\begin{bmatrix}\n        1/d_1 & 0 & \\dots & 0\\\\\n        0 &  1/d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots &  1/d_n\\\\\n\\end{bmatrix}\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\n\nMany techniques have been proposed to estimate the parameters of a regression model.\n\n\nThe goal of this course is not to study these techniques but we will learn how to play with the estimated parameters because it will be very useful as we move along.\n\n\nThe most common way to build a regression model is\n\nreg &lt;- lm(b.exemplaris ~ humidity)\n\n\n\nSay we now want to build a model’s confidence interval from a linear regression\n\n\nHow would you do it ?\n\n\nLet’s look at the model’s results, maybe it will help us"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-1",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-1",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nModel’s results\n\n\n(summaryReg &lt;- summary(reg))\n\n\nCall:\nlm(formula = b.exemplaris ~ humidity)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.47988 -0.26475  0.00611  0.32590  1.36077 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.57389    0.04720   54.53   &lt;2e-16 ***\nhumidity     1.10086    0.07976   13.80   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4718 on 98 degrees of freedom\nMultiple R-squared:  0.6603,    Adjusted R-squared:  0.6569 \nF-statistic: 190.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n. . .\n\nLet’s say we want to construst the model’s confidence intervals by sampling multiple times (say 100 times!) the regression parameters, which we will assume follow Gaussian distribution. How would you do this?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-2",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-2",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nWe could sample the model parameters but how can we do this properly?\n. . .\nAny suggestions?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-3",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-3",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIf we look at the estimated regression model coefficient, we can learn a few things\n. . .\n\nsummaryReg$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 2.573889 0.04720304 54.52804 4.011925e-75\nhumidity    1.100865 0.07975800 13.80256 1.035796e-24\n\n\n. . .\nNotably, there are uncertainty around the parameters.\n. . .\nMaybe we can use this information to sample model parameters and reconstruct models across different iterations of parameters.\n. . .\nLet’s give it a shot !"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-4",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-4",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIf we assume that the parameters of our particular model follow a Gaussian distribution, we can state that\n. . .\n\\[\\beta_0 \\sim \\mathcal{N}(2.574, 0.047^2)\\] \\[\\beta_1 \\sim \\mathcal{N}(1.101, 0.080^2)\\]"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-5",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-5",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIn R, we can do this as follow\n\n# Object that include regression coefficients\nregCoef &lt;- summaryReg$coefficients\n\n# Sample regression parameters\nbeta_0 &lt;- rnorm(100, mean = regCoef[1,1], sd = regCoef[1,2])\nbeta_1 &lt;- rnorm(100, mean = regCoef[2,1], sd = regCoef[2,2])"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-6",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-6",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-7",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-7",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nBut is this the right way to do it ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-8",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-8",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nActually, even if the model’s confidence interval look about right, they are wrong !"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-9",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-9",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nThe approach presented in the previous slide works only if we assume that the parameters are completely independent from one another.\n. . .\nA situation that happens only in very specific circumstances.\n. . .\nSo… We need to find a way to account for the non-independencies between the parameters.\n. . .\nHow can we do this ? Any ideas ?"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-10",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-10",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\n\nAssuming the regression parameters are normally distributed is not a bad assumption.\n\n. . .\n\nHowever to consider a dependencies between the parameters we need to sample them from a multivariate normal distribution where the variance of each parameter and their dependency is defined by a covariance matrix estimated specifically for the data we model.\n\n. . .\n\nThe good news is that this covariance matrix is given by summary.lm function\n\n. . .\n\n\n(covReg &lt;- summaryReg$cov.unscaled)\n\n              (Intercept)      humidity\n(Intercept)  0.0100098513 -0.0005305969\nhumidity    -0.0005305969  0.0285782940"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-11",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-11",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nFor our specific model, mathematically, we assume that\n. . .\n\\[\\begin{bmatrix}\n  \\beta_0\\\\\n  \\beta_1\\\\\n\\end{bmatrix} \\sim \\mathcal{MVN} \\left( \\begin{bmatrix}\n  2.574\\\\\n  1.101\\\\\n\\end{bmatrix}, \\begin{bmatrix}\n  0.0100 & -0.0005 \\\\\n  -0.0005 & 0.0286 \\\\\n\\end{bmatrix} \\right)\\]\n. . .\nNote To present the multivariate normal distribution, we rely on matrix notation. This is our first introduction into matrix algebra. We will talk about this more into this course."
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-12",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-12",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters\nIn R, we can sample the parameters using a multivariate normal distribution using the following code\n\n# Object that include regression coefficients\nregCoef &lt;- summaryReg$coefficients\n\n# Sample regression parameters\nbeta &lt;- MASS::mvrnorm(100, regCoef[,1], Sigma = covReg)"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-13",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-13",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters"
  },
  {
    "objectID": "slides/nn_extra_slides/index.html#estimating-regression-parameters-14",
    "href": "slides/nn_extra_slides/index.html#estimating-regression-parameters-14",
    "title": "Extra stuff",
    "section": "Estimating regression parameters",
    "text": "Estimating regression parameters\nSampling model parameters - Comparison"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#hierarchical-model-structure-so-far",
    "href": "slides/08_Complex_hierarchical_model/index.html#hierarchical-model-structure-so-far",
    "title": "Complex hierarchical models",
    "section": "Hierarchical model structure so far",
    "text": "Hierarchical model structure so far\n\nMathematically, the basic structure of a hierarchical model is\n\n\n\\[\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{b} + \\boldsymbol{\\varepsilon}\\]\n\n\nwhere\n\n\n\n\\(\\mathbf{y}\\) : Vector of response variable\n\\(\\mathbf{X}\\) : Matrix of explanatory variables on which no hierarchies are accounted for\n\\(\\mathbf{Z}\\) : Matrix of explanatory variables on which hierarchies are accounted for\n\\(\\boldsymbol{\\beta}\\) : parameter estimated without a hierarchy\n\\(\\mathbf{b}\\) : parameter estimated with a hierarchy\n\\(\\boldsymbol{\\varepsilon}\\) : a vector that follows a Gaussian distribution such that \\({\\cal N}(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#complex-hierarchical-model",
    "href": "slides/08_Complex_hierarchical_model/index.html#complex-hierarchical-model",
    "title": "Complex hierarchical models",
    "section": "“Complex” hierarchical model",
    "text": "“Complex” hierarchical model\nBy “complex” we refer to hierarchical models for which more than one parameters are accounted for in a parameter hierarchy.\n\nAs we will see, there are a number of ways this can complexify the structure of a model in ways that are not always obvious."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies",
    "href": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies",
    "title": "Complex hierarchical models",
    "section": "Interacting hierarchies",
    "text": "Interacting hierarchies\n\nlme4 notation : y ~ (1 | f:g)\n\n\n\nThis model assumes that factors f and g interact to make a hierarchy.\n\n\n\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f\\times g },\\sigma^2\\mathbf{I})\\] or\n\\[y_i = b_{f[i]\\times g[i]} + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n\n\n\nwhere \\[\\mathbf{b}_{f\\times g} \\sim \\mathcal{N}\\left(\\mu_{f\\times g}, \\sigma^2_{f\\times g}\\right)\\]\n\n\n\n\nNote that a multi-factor hierarchy can be constructed by multiplying the levels of each factor to account for a more complexe hierarchy."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#interacting-hierarchies-1",
    "title": "Complex hierarchical models",
    "section": "Interacting hierarchies",
    "text": "Interacting hierarchies"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Multiple independent hierarchy",
    "text": "Multiple independent hierarchy\n\nlme4 notation : y ~ (1 | f) + (1 | g) or y ~ 1 + (1 | f) + (1 | g)\n\n\n\nThis model assumes there is a hierarchy that varies among two factors that are independent from one another.\n\n\n\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f} + \\mathbf{b}_{g},\\sigma^2\\mathbf{I})\\] or\n\\[y_i = b_{f[i]} + b_{g[i]} + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n\n\n\nwhere\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\begin{bmatrix}\n                                    \\mu_f\\\\\n                                    \\mu_g\\\\\n                                  \\end{bmatrix}\n                                ,\n                                \\begin{bmatrix}\n                                  \\sigma^2_f & 0\\\\\n                                  0& \\sigma^2_g\\\\\n                                \\end{bmatrix}\n                                \\right)\\]\n\n\n\n\nHere, we are dealing with a model that has two intercepts, which are sampled independently so that the \\(b\\)s will change for a sample \\(i\\) only when the the level of factor \\(f\\) and the level of factor \\(g\\) changes independently."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#multiple-independent-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Multiple independent hierarchy",
    "text": "Multiple independent hierarchy"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies",
    "href": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies",
    "title": "Complex hierarchical models",
    "section": "Nested hierarchies ?",
    "text": "Nested hierarchies ?\n\nlme4 notation : y ~ (1 | f/g) or y ~ (1 | f) + (1 | f:g)\n\n\n\nThis model assumes there is a hierarchy that varies among the levels of factor f and among the levels of factor g but only within the levels of factor f.\n\n\n\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\mathbf{b}_{f}+\\mathbf{b}_{f\\times g},\\sigma^2\\mathbf{I})\\] or\n\\[y_i = b_{f[i]}+b_{f[i]\\times g[i]} + \\varepsilon \\quad i = 1\\dots n\\]\n\n\n\n\nwhere\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\begin{bmatrix}\n                                    \\mu_f\\\\\n                                    \\mu_{f\\times g}\\\\\n                                  \\end{bmatrix},\n                                \\begin{bmatrix}\n                                  \\sigma^2_f & 0\\\\\n                                  0& \\sigma^2_{f\\times g}\\\\\n                                \\end{bmatrix}\n                                \\right)\\]\n\n\n\n\nHere, the model has two independent hierarchy, one changes for a sample \\(i\\) as a single intercept hierarchy and the other will change for a sample \\(i\\) only when the level of factor \\(g\\) is within the level of factor \\(f\\)."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#nested-hierarchies-1",
    "title": "Complex hierarchical models",
    "section": "Nested hierarchies ?",
    "text": "Nested hierarchies ?"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Uncorrelated intercept and slope hierarchy",
    "text": "Uncorrelated intercept and slope hierarchy\n\nlme4 notation : y ~ x + (x || f) or y ~ 1 + x + (1 | f) + (0 + x | g)\n\n\n\nThis model assumes there is a hierarchy that varies independently among the levels of factor f for the intercept and the slope.\n\n\n\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol\\beta_0+\\boldsymbol\\beta_1\\mathbf{x}+\\mathbf{b}_{0f}+\\mathbf{b}_{1f}\\mathbf{z},\\sigma^2\\mathbf{I})\\] or\n\\[y_i = \\beta_0 + \\beta_1x_i+b_{0f[i]}+b_{1f[i]}z_i + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n\n\n\nThis is because in this model\n\\[\\begin{bmatrix}\n    b_0\\\\\n    b_1\\\\\n\\end{bmatrix} \\sim \\mathcal{N}\\left(\\begin{bmatrix}\n                                    \\mu_{b_0f}\\\\\n                                    \\mu_{b_1f}\\\\\n                                  \\end{bmatrix},\n                                \\begin{bmatrix}\n                                  \\sigma^2_{b_0f} & 0\\\\\n                                  0& \\sigma^2_{b_1f}\\\\\n                                \\end{bmatrix}\n                                \\right)\\]\n\n\nNote : In this formulation \\(\\mathbf{x}=\\mathbf{z}\\). Similarly, \\(x_i=z_i\\)."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#uncorrelated-intercept-and-slope-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Uncorrelated intercept and slope hierarchy",
    "text": "Uncorrelated intercept and slope hierarchy"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#a-small-step-back",
    "href": "slides/08_Complex_hierarchical_model/index.html#a-small-step-back",
    "title": "Complex hierarchical models",
    "section": "A small step back",
    "text": "A small step back\n\n\nThe structure of the hierarchy discussed so far were “less complex” in the sense that\n\n\n\n\n\nA single variance parameter is estimated\n\n\\[\\sigma^2_{f}\\]\n\n\n\n\n\nMultiple independent variance parameters are estimated\n\n\\[\n\\begin{bmatrix}\n  \\sigma^2_f & 0\\\\\n  0& \\sigma^2_{g}\\\\\n\\end{bmatrix}\n\\]\n\n\n\n\nIn Bayesian, having uncorrelated variances allows us to sample variance parameters independently even with multiple factors, which is computationally more efficient.\n\n\n\n\nFrom this point on, we will look at more complex covariance structures where the hierarchical levels are not independent from each other."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Correlated intercept and slope hierarchy",
    "text": "Correlated intercept and slope hierarchy\n\n\nlme4 notation : y ~ x + (x | g) or y ~ 1 + x + (1 + x | g)\n\n\n\n\nThis model assumes the hierarchy between the intercept and the slope is correlated.\n\n\n\n\nMathematically, it can be translated to\n\\[\\mathbf{y} \\sim \\mathcal{MVN}(\\boldsymbol\\beta_0 + \\boldsymbol\\beta_1\\mathbf{x}+\\mathbf{b}_{0f} +\\mathbf{b}_{1f}\\mathbf{z},\\mathbf{\\Sigma})\\] or\n\\[y_i = \\beta_0 + \\beta_1x+b_{0f[i]} + b_{1f[i]}z_i + \\varepsilon \\quad\\forall\\quad i = 1\\dots n\\]\n\n\n\n\nIn this model\n\\[\\begin{bmatrix}\n    b_0\\\\\n    b_1\\\\\n\\end{bmatrix}\n\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n                                    \\mu_{b_0f}\\\\\n                                    \\mu_{b_1f}\\\\\n                                  \\end{bmatrix},\n                      \\begin{bmatrix}\n                        \\sigma^2_{b_0f} & \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} \\\\\n                        \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} & \\sigma^2_{b_1f}\n                      \\end{bmatrix}\n                \\right)\\]\n\n\nNote : In this formulation \\(\\mathbf{x}=\\mathbf{z}\\). Similarly, \\(x_i=z_i\\)."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Correlated intercept and slope hierarchy",
    "text": "Correlated intercept and slope hierarchy\n\n\nThe covariance structure \\[\\begin{bmatrix}\n    b_0\\\\\n    b_1\\\\\n\\end{bmatrix}\n\\sim \\mathcal{N}\\left(\\begin{bmatrix}\n                                    \\mu_{b_0f}\\\\\n                                    \\mu_{b_1f}\\\\\n                                  \\end{bmatrix},\n                      \\begin{bmatrix}\n                        \\sigma^2_{b_0f} & \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} \\\\\n                        \\rho_{b_0,b_1}\\sigma_{b_0f}\\sigma_{b_1f} & \\sigma^2_{b_1f}\n                      \\end{bmatrix}\n                \\right)\\]\nneeds to be discussed a bit more.\n\n\n\n\nNotation\n\n\n\n\nIn the covariance matrix,\n\n\n\n\n\n\\(\\rho_{b_0,b_1}\\) is the correlation between \\(b_0\\) and \\(b_1\\)\n\n\n\n\n\n\n\\(\\rho_{b_0,b_1}\\sigma_{b_0}\\sigma_{b_1}\\) is the covariance between \\(b_0\\) and \\(b_1\\)\n\n\n\n\n\nInterpretation\n\n\n\n\nA covariance matrix with non-zero covariance describes dependence between the \\(b\\)s, which can tell us both the strength of the relation between pairs of parameters and the variance structure."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-2",
    "href": "slides/08_Complex_hierarchical_model/index.html#correlated-intercept-and-slope-hierarchy-2",
    "title": "Complex hierarchical models",
    "section": "Correlated intercept and slope hierarchy",
    "text": "Correlated intercept and slope hierarchy"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#so-far-what-we-have-done",
    "href": "slides/08_Complex_hierarchical_model/index.html#so-far-what-we-have-done",
    "title": "Complex hierarchical models",
    "section": "So far what we have done",
    "text": "So far what we have done"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#what-we-can-do-now",
    "href": "slides/08_Complex_hierarchical_model/index.html#what-we-can-do-now",
    "title": "Complex hierarchical models",
    "section": "What we can do now !",
    "text": "What we can do now !"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy",
    "href": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy",
    "title": "Complex hierarchical models",
    "section": "Combining different types of hierarchy",
    "text": "Combining different types of hierarchy\nWith what we learned so far, it is possible to build more complex model by combining the building blocks we went over in the last few hours.\n\nThe best way to do this is to immerse ourselve into a particular problem.\n\n\nSo, in the next slides we will discuss about the pumpkinseed (Lepomis gibbosus)"
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy-1",
    "href": "slides/08_Complex_hierarchical_model/index.html#combining-different-types-of-hierarchy-1",
    "title": "Complex hierarchical models",
    "section": "Combining different types of hierarchy",
    "text": "Combining different types of hierarchy\nFictive context\nPumpkinseed (Lepomis gibbosus) growth has been studied in 15 lakes of Estrie during the famous ichthyology course given at Université de Sherbrooke in the past 30 years.\n\nDuring these ichthyology courses, fish are sampled, measured and tag in all 15 lakes and every year fish are recaptured by students and remeasured. So far, there has been 10 teaching assistants showing there own specific way to measure Pumpkinseed, with, albeit, uneven levels of precision.\n\n\nYour superviser thinks that Pumpkinseed growth is influenced by water temperature, which has also been sampled every time fish were measured.\n\n\nAfter 30 years of data gathering, 123 fish have been sampled during 17 consecutive years."
  },
  {
    "objectID": "slides/08_Complex_hierarchical_model/index.html#pumpkinseed-growth-example",
    "href": "slides/08_Complex_hierarchical_model/index.html#pumpkinseed-growth-example",
    "title": "Complex hierarchical models",
    "section": "Pumpkinseed growth example",
    "text": "Pumpkinseed growth example\nQuestion\n\nIs the growth of Pumpkinseed influenced by water temperature?\n\n\n\n\n\n\n\nTry to build the model that best answer this question\n\n\nTo have the most precise model, we need to account for lakes area and depth as well as control for the temperature variation in between lakes and for divergences in sampling measurements of each year (wink, wink… teachers assistant may have an influence here as well)."
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models",
    "href": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models",
    "title": "Gaussian process",
    "section": "An even more complex hierarchical models!",
    "text": "An even more complex hierarchical models!\nSo far we have seen many (!) types of hierarchical models, which got increasingly more complex in their structure.\n\nLet’s continue on that slippery slope…\n\n\nWould you know how to constraint (spatially, temporally, phylogenetically, etc.) such a model ?"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-1",
    "href": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-1",
    "title": "Gaussian process",
    "section": "An even more complex hierarchical models!",
    "text": "An even more complex hierarchical models!\nIf we get back to a model that has a single hierarchy on the intercept such that \\[\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{b},\\sigma^2)\\]\n\nwhere \\[\\mathbf{b} \\sim \\mathcal{N}(\\mu, \\sigma^2).\\]\n\n\nIf we want to account for a constraint on the previously presented model, we can rewrite the equation fo \\(\\mathbf{b}\\) as \\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\mu, f(d)\\right)\\]\n\n\nwhere\n\\(f(d)\\) is a function of a (spatial, temporal, phylogenetic, …) distance matrix"
  },
  {
    "objectID": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-2",
    "href": "slides/09_Gaussian_process/index.html#an-even-more-complex-hierarchical-models-2",
    "title": "Gaussian process",
    "section": "An even more complex hierarchical models!",
    "text": "An even more complex hierarchical models!\n\\[\\mathbf{b} \\sim \\mathcal{N}\\left(\\mu, f(d)\\right)\\]\n\nWhat this equation means conceptually is that the variance associated to \\(\\mathbf{b}\\) is not a constant, it changes based on distance (across space, time, phylogeny, etc.). This is known as a Gaussian process."
  }
]