{
  "hash": "1d8a2fe7ff495a1e164f00d074a56fe5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Convergence trick\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Guillaume Blanchet -- Andrew MacDonald\"\ndate: \"2024-05-01\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n\n::: {style=\"font-size: 0.9em\"}\n## Playing with the Gaussian distribution\n\nWhen estimating regression parameters, the Gaussian distribution is commonly used. Often what we need to do is figure out the mean and/or the variance of the Gaussian distribution that best fit the data given a particular model structure. \n\nHowever, for technical reasons, it is sometimes (actually, more often than we would care to advertize broadly !) very difficult to reach convergence for a particular parameter. Visually, a trace plot would look like this\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=2400}\n:::\n:::\n\n\n::: \n\n::: {style=\"font-size: 0.9em\"}\n## Playing with the Gaussian distribution\n\nEven if you run the model for many, many (many !) iterations, it never seems to converge.\n\nWhat should we do ?\n\n![](https://www.i2symbol.com/pictures/emojis/a/3/6/b/a36b215220fc0153c107ee9d022cb75e_384.png){fig-align=\"center\" width=20%}\n::: \n\n::: {style=\"font-size: 0.9em\"}\n## Playing with the Gaussian distribution\n\nThere is a very cool trick that can help us here. \n\nBefore we start to discuss this trick, it is important to know that sampling a standard Gaussian distribution ($\\mathcal{N}(0,1)$) is very straight forward computationally. So, the closer we get to a standard Gaussian distribution the better it is.\n\n### The convergence trick\n\nIf we think about it, the Gaussian distribution can be translated and scaled. If we can find a way to do this mathematically, we can incorporate this into our estimation procedure.\n\n:::: {style=\"text-align: center; font-size: 2em; color: red \"}\nAny ideas how to do this ?\n::::\n:::\n\n::: {style=\"font-size: 0.9em\"}\n## The convergence trick\n\n**Translation**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=1920}\n:::\n:::\n\n\n:::\n\n## The convergence trick\n\n**Translation**\n\nMathematically, translation is the equivalent of adding or subtracting a value from the mean of the distribution.\n\n. . .\n\nThis means that \n\n$$\\mathcal{N}(\\mu, \\sigma^2)$$\n\n. . .\n\nis exactly the same as \n\n$$\\mathcal{N}(0, \\sigma^2) + \\mu$$\n\n## The convergence trick\n\n**Scaling**\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## The convergence trick\n\n**Scaling**\n\nMathematically, scaling amounts to multiplying the Gaussian distribution by a positive number.\n\n. . .\n\nThis means that \n\n$$\\mathcal{N}(\\mu, \\sigma^2)$$\n\n. . .\n\nis exactly the same as \n\n$$\\mathcal{N}(\\mu, 1) \\times \\sigma^2$$\n\n## The convergence trick\n\nThe convergence trick amounts to sampling a standard Gaussian distribution and adjusting its mean and variance from **outside** the distribution\n\n$$\\mathcal{N}(0, 1) \\times \\sigma^2 + \\mu$$\nWhen implementing an MCMC in Stan (or any other such software), this trick allows for convergence to be much more efficient.\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n:::\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n\n\nTo do this, we need to work with a multivariate Gaussian distribution.\n:::\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n\nTo do this, we need to work with a multivariate Gaussian distribution.\n\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.\n:::\n\n## The convergence trick {auto-animate=\"true\"}\n\n:::{style=\"font-size: 0.95em\"}\nThe example I gave above is straight forward to visualize but is badly adapted to most problems because most regression models require that many parameters be sampled at once.\n\nTo do this, we need to work with a multivariate Gaussian distribution.\n\nThe good news is that the convergence trick works also with a multivariate Gaussian distribution. However, we need to rely on matrix algebra to translate and scale a multivariate Gaussian distribution properly.\n\nTo show how our convergence trick works for a multivariate Gaussian distribution, let's first visualize the two dimensional version of this distribution.\n:::\n\n## Bivariate Gaussian distribution\n\n$$\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  0\\\\\n\\end{bmatrix},\n\\begin{bmatrix}\n  2 & -1\\\\\n  -1 & 2\\\\\n\\end{bmatrix}\\right)$$\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\n#### Translation\n\nFor a multivariate distribution, a translation amounts to adding a **vector** of values to make the translation. \n\nMathematically, this means that\n\n$$\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right)=\\mathcal{MVN}\\left(\n\\begin{bmatrix}\n  0\\\\\n  \\vdots\\\\\n  0\\\\\n\\end{bmatrix},\n\\mathbf{\\Sigma}\\right) + \\begin{bmatrix}\n  \\mu_1\\\\\n  \\vdots\\\\\n  \\mu_n\\\\\n\\end{bmatrix}$$\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\n#### Scaling\n\nUnlike for the univariate Gaussian distribution, scalling for a multivariate distribution is a little trickier to perform... But mathematician and statistician have worked hard to figure out how to do this properly.\n\nHowever, we need to delve a little deeper into matrix algebra to understand how to scale a multivariate Gaussian distribution.\n\n# Matrix algebra interlude (part 2!)\n\n## Scaling a covariance matrix {auto-animate=\"true\"}\n\n::: {style=\"font-size: 0.8em\"}\nFirst recall that a covariance matrix $\\mathbf{\\Sigma}$ is a square matrix (i.e. it is an $n\\times n$ matrix).\n\nTo scale $\\mathbf{\\Sigma}$, we cannot only multiply it by a scalar or even by a single matrix, we need to use the following matrix multiplication\n\n$$\\mathbf{L}\\mathbf{\\Sigma}\\mathbf{L}^t$$\nwhere $\\mathbf{L}$ is a $p\\times n$ matrix of weight to be used for the scaling (a \"scaling\" matrix) and $\\mathbf{L}^t$ is its tranpose.\n\nThe technical reason why we **need** to use the equation above is to ensure that the resulting scaled covariance matrix also has an $n \\times n$ dimension. \n\nIf only\n$$\\mathbf{L}\\mathbf{\\Sigma}$$\nis used the dimension of the resulting matrix also would be $p \\times p$.\n:::\n\n## Square-root of a matrix\n\n::: {style=\"font-size: 0.8em\"}\nBecause in our problem weighting (or scaling) matrices is usually done with other covariance matrices, to apply the matrix scaling operation described previously, we need to find a way to square-root a matrix.\n\nThis where the genious of André-Louis Cholesky comes to the rescue.\n:::\n\n![](https://upload.wikimedia.org/wikipedia/commons/5/5f/Andre_Cholesky.jpg){fig-align=\"center\" width=20%}\n\n## Square-root of a matrix\n\n### Cholesky decomposition\n\n::: { style=\"font-size: 0.8em\"}\nAndré-Louis Cholesky discovered a matrix decomposition approach probably around 1902 (so when he was 27 years old!), although it was attributed to him a few years after his death.\n\nThe Cholesky decomposition allows to decompose a square matrix in a triangular matrix, which, when multiplied by its transposed will allow us to recover the initial matrix. \n\nIn coloquial terms, the Cholesky decomposition is the equivalent of a square root for matrices.\n\nIn math terms the Cholesky decomposition is defined as \n$$\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t$$\n:::\n\n## Square-root of a matrix\n\n### Cholesky decomposition\n\n#### Example\n$$\\mathbf{A} = \\mathbf{L}\\mathbf{L}^t$$\n\n$$\n\t\t\\begin{bmatrix}\n\t\t\t1 & 1 & 1\\\\\n\t\t\t1 & 5 & 5\\\\\n\t\t\t1 & 5 & 14\\\\\n\t\t\\end{bmatrix}=\n\t\t\\begin{bmatrix}\n\t\t\t1 & 0 & 0\\\\\n\t\t\t1 & 2 & 0\\\\\n\t\t\t1 & 2 & 3 \\\\\n\t\t\\end{bmatrix}\n\t\t\\begin{bmatrix}\n\t\t\t1 & 1 & 1\\\\\n\t\t\t0 & 2 & 2\\\\\n\t\t\t0 & 0 & 3 \\\\\n\t\t\\end{bmatrix}\n$$\n\n# End of matrix algebra interlude\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\n#### Scaling\n\n::: { style=\"font-size: 0.7em\"}\nTo scale the following multivariate Gaussian distribution\n$$\\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{\\Sigma}\\right),$$ \n\nThe following steps need to be applied \n\n1. Apply the Cholesky decomposition on the scaling matrix, here $\\mathbf{\\Sigma}$\n$$\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^t$$\n2. Multiply the $\\mathbf{L}$ matrix to a standard variance multivariate Gaussian distribution\n\n$$\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\boldsymbol{\\mu},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t.$$\n\nRecall, that $\\mathbf{I}$ is the identity matrix.\n:::\n\n## The convergence trick {auto-animate=\"true\"}\n\n### Multivariate Gaussian distribution\n\nIf we apply translation and scaling together on a multivariate Gaussian distribution, we get\n\n$$\\mathbf{L}\\cdot \\mathcal{MVN}\\left(\\mathbf{0},\\mathbf{I}\\right)\\cdot\\mathbf{L}^t + \\boldsymbol{\\mu}$$\nWhen implementing in Stan some of the models we will discuss in this course, this convergence trick becomes very practical because it can lead a model to convergence much faster than without using this trick.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}