{
  "hash": "999a573b0061e5c302ac8cb5eb0a2e2c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear models\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Guillaume Blanchet -- Andrew MacDonald\"\ndate: \"2025-05-06\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n\n## Basic regression model\n\n::: {style=\"font-size: 0.8em\"}\nHierarchical models are a generalized version of the classic regression models you have seen in your undergraduate courses.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nIn its simplest form, a regression model is usually presented as \n:::\n\n. . .\n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n$$\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nIt is known as a **simple linear model**, where :\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$y_i$ is the value of a response variable for observation $i$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$x_i$ is the value of an explanatory variable for observation $i$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$\\beta_0$ is the model intercept\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$\\beta_1$ is the model slope\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$\\varepsilon$ is the error term\n:::\n\n## Basic regression model\n\nThe cool thing about the simple linear model is that it can be studied visually quite easily.\n\n. . .\n\nFor example if we are interested in knowing how a newly discovered plant species (*Bidonia exemplaris*) reacts to humidity, we can relate the biomass of *B. exemplaris* sampled at 100 sites with the soil humidity content and readily  visual the data and the trend.\n\n## Basic regression model\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Regression parameters\n\nGenerally, the **slope** and the **intercept** are the regression parameters we focus on when studying the simple linear model, but there is another one that is very important to consider, especially for this course.\n\n. . .\n\n**Any ideas which one it is ?**\n\n![](https://www.i2symbol.com/pictures/emojis/3/9/7/2/3972d0a5cdf7dffdaf58a912331839c7_384.png){fig-align=\"center\" width=35%}\n\n## Regression parameters\n\n::: {style=\"font-size: 0.75em\"}\nIf we go back to the mathematical description of the model\n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i} + \\varepsilon\n$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nwe can see that in the simple linear regression the error term ($\\varepsilon$) has actually a very precise definition:\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\n$$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$$\n:::\n\n::: {style=\"font-size: 0.75em\"}\nwhere $\\sigma^2$ is an estimated variance.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nIn words, it means that the error in a simple linear regression follows a Gaussian distribution with a variance that is estimated.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nFor most of the course, we will play with the variance parameter $\\sigma^2$ in a bunch of different ways. \n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nBut before we do this, we need to understand a bit more about how this parameter influence the model.\n::: \n\n\n## Regression parameters\n\nA first way to do this is to think about the simple linear regression in a slightly different way. Specifically, based on what we learned in the previous slide the simple linear regression can be rewritten as \n$$\ny \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_{i}, \\sigma^2)\n$$\n\n. . .\n\nAs we will see later in this course, this writting style will become particularly useful.\n\n## Regression parameters\n\n**Variance of the model ($\\sigma^2$)**\n\n::: {style=\"font-size: 0.75em\"}\nIn essence, $\\sigma^2$ tells us about what the model could not account for.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nFor example, let's compare the biomass of *Bidonia exemplaris* with that of *Ilovea chicktighii*, another species (a carnivorous plant) \n:::\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=1536}\n:::\n:::\n\n\n\n## Regression parameters\n**Variance of the model ($\\sigma^2$)**\n\nBy regressing humidity on the biomass of both plants, we can obtain the estimated parameters for each regression (which are all available using `summary.lm`)\n\n. . .\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Regression model\nregBexemplaris <- lm(b.exemplaris ~ humidity)\nregIchicktighii <- lm(i.chicktighii ~ humidity)\n\n# Summary\nsummaryRegBexemplaris <- summary(regBexemplaris)\nsummaryRegIchicktighii <- summary(regIchicktighii)\n```\n:::\n\n\n\n## Regression parameters\n**Variance of the model ($\\sigma^2$)**\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\nFor *Bidonia exemplaris*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimated coefficients\nsummaryRegBexemplaris$coefficients[,1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate Std. Error\n(Intercept) 4.015999 0.05235200\nhumidity    1.313897 0.08845811\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimated variance\nsummaryRegBexemplaris$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5232624\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nFor *Ilovea chicktighii*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Estimated coefficients\nsummaryRegIchicktighii$coefficients[,1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            Estimate  Std. Error\n(Intercept) 3.991785 0.008928294\nhumidity    1.271250 0.015085956\n```\n\n\n:::\n\n```{.r .cell-code}\n# Estimated variance\nsummaryRegIchicktighii$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.089239\n```\n\n\n:::\n:::\n\n\n:::\n::::\n\n## Limits of the simple linear regression \n\nThere are two major pitfalls of the simple linear model for problems in the life sciences\n\n. . .\n\n1.  One explanatory is almost never enough to approach biological questions nowadays.\n\n. . .\n\n2.  The simple linear model assumes that the error follows a Gaussian distribution.\n\n## Multiple linear regression\n\n::: {style=\"font-size: 0.85em\"}\nSimple linear regression can be extended to account for multiple explanatory variables to study more complexe problems. This type of regression model is known as a **multiple linear regression**.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.85em\"}\nMathematically, a multiple linear regression can be defined as \n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n$$\n:::\n\n. . . \n\n::: {style=\"font-size: 0.85em\"}\nTheoretically, estimating the parameters of a multiple regression model is done the same ways as for simple linear regression. However, technically, matrix algebra is quite practical to use in this context and especially for this course.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.85em\"}\nIn this respect, let's take a bit of time to get acquinted with different basic (and maybe not so basic!) knowledge of matrix algebra.\n:::\n\n# Matrix algebra interlude\n\n## A general way to write matrices\n$$\n\\mathbf{A} = \\begin{bmatrix}\n\t\t\t\tA_{11} & A_{12} & \\dots & A_{1j} & \\dots & A_{1n}\\\\\n\t\t\t\tA_{21} & A_{22} & \\dots & A_{2j} & \\dots & A_{2n}\\\\\n\t\t\t\t\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots\\\\\n\t\t\t  A_{i1} & A_{i2} & \\dots & A_{ij} & \\dots & A_{in}\\\\\n\t\t    \\vdots & \\vdots & & \\vdots & \\ddots & \\vdots\\\\\n\t\t    A_{m1} & A_{m2} & \\dots & A_{mj} & \\dots & A_{mn}\\\\\n\t\t\\end{bmatrix}\n$$\n$$A = \\left[a_{ij}\\right]=\\left[a_{ij}\\right]_{m\\times n}$$\n\n# Basic matrix operations\n\n## The transpose of a matrix\n\n::::: { style=\"font-size: 0.85em\"}\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n$$A = \\begin{bmatrix}\n          5 & -6 & 4 & -4\\\\\n        \\end{bmatrix}\n$$\n\n$$B = \\begin{bmatrix}\n          -8\\\\\n          9\\\\\n          -2\\\\\n        \\end{bmatrix}\n$$\n\n$$C = \\begin{bmatrix}\n          -4 & 1\\\\\n          2 & -5\\\\\n        \\end{bmatrix}\n$$\n:::\n\n::: {.column width=\"50%\"}\n\n$$A^t=\\begin{bmatrix}\n          5\\\\\n          -6\\\\\n          4\\\\\n          -4\\\\\n        \\end{bmatrix}\n$$\n    \n \n$$B^t =\\begin{bmatrix}\n          -8 & 9 & -2\\\\\n        \\end{bmatrix}\n$$\n    \n\n$$C^t =\\begin{bmatrix}\n          -4 & 2\\\\\n          1 & -5\\\\\n        \\end{bmatrix}\n$$\n:::\n::::\n:::::\n\n## The transpose of a matrix\n\nIn R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    3    5\n[2,]    1   -2\n```\n\n\n:::\n\n```{.r .cell-code}\nt(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]    3    1\n[2,]    5   -2\n```\n\n\n:::\n:::\n\n\n\n## Multiplying a matrix by a scalar\n\t\n$$\\mathbf{B} = c\\mathbf{A}$$\n$$B_{ij} = cA_{ij}$$\n\t\n::: {style=\"color: blue;\"}\n$$\n\t\t\t0.3 \\begin{bmatrix}\n\t\t\t\t3 & 5\\\\\n\t\t\t\t1 & -2\\\\\n\t\t\t\\end{bmatrix} =  \n\t\t\t\\begin{bmatrix}\n\t\t\t\t0.9 & 1.5\\\\\n\t\t\t\t0.3 & -0.6\\\\\n\t\t\t\\end{bmatrix}\n$$\n:::\t\t\n\nIn R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nc <- 0.3\nc*A \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1] [,2]\n[1,]  0.9  1.5\n[2,]  0.3 -0.6\n```\n\n\n:::\n:::\n\n\n\n## Matrix multiplications (not divisions!)\n\t\n$$\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}$$\n\n$$C_{ik} = \\sum^{n}_{j=1}A_{ij}B_{jk}$$\n\n**Rules**\n\t\nAssociative: $\\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}$ \n\t\nDistributive: $\\mathbf{A}(\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B}+\\mathbf{A}\\mathbf{C}$\n\t\nNot commutative: $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$\n\n\n## Inner product\n$$(\\mathbf{Ax})_i=\\sum_{j=1}^{n}A_{ij}x_j$$\n\n:::{style=\"color: blue;\"}\n$$\n  \\begin{bmatrix}\n    3 & 5\\\\\n    1 & -2\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n    2\\\\ 5\\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}\n    (3, 5) \\cdot (2, 5)\\\\\n    (1, -2) \\cdot (2, 5) \\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}\n    3 \\times 2 + 5 \\times 5\\\\\n    1 \\times 2 -2 \\times 5\\\\\n  \\end{bmatrix} = \n  \\begin{bmatrix}\n    31\\\\\n    -8\\\\\n  \\end{bmatrix}\n$$\n:::\n\nIn R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA <- matrix(c(3, 1, 5, -2), nrow = 2, ncol = 2)\nx <- matrix(c(2,5), nrow = 2, ncol = 1)\nA %*% x\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     [,1]\n[1,]   31\n[2,]   -8\n```\n\n\n:::\n:::\n\n\n\n# Some important matrices\n\n## Identity matrix\n\nThe identity matrix is a square matrix where all values of its diagonal are 0 except  the diagonal values which are all 1s.\n\n::: { style=\"font-size: 0.9em\"}\n$$\n\\mathbf{I}=\\begin{bmatrix}\n\t\t\t\t1 & 0 & 0\\\\\n\t\t\t\t0 & 1 & 0\\\\\n\t\t\t\t0 & 0 & 1\\\\\n\t\t\\end{bmatrix}\n$$\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\nThe identity matrix is important because \n\n$$\\mathbf{A} \\cdot \\mathbf{I}_n = \\mathbf{A}$$\nor \n\n$$\\mathbf{I}_m \\cdot \\mathbf{A} = \\mathbf{A}$$\n:::\n\n## Diagonal matrix\n\nThe diagonal matrix is a square matrix where all values of its diagonal are 0 except the ones on the diagonal.\n\n:::: {style=\"font-size: 0.8em\"}\n$$D=\n      \\begin{bmatrix}\n        d_1 & 0 & \\dots & 0\\\\\n        0 & d_2 & \\dots & 0\\\\\n        \\vdots & \\vdots & \\ddots & \\vdots\\\\\n        0 & 0 & \\dots & d_n\\\\\n\\end{bmatrix}$$\n\nAn example\n\n::: {style=\"color: blue;\"}\n$$\n\\begin{bmatrix}\n\t\t\t\t-1 & 0 & 0\\\\\n\t\t\t\t0 & 0 & 0\\\\\n\t\t\t\t0 & 0 & 6\\\\\n\t\t\\end{bmatrix}\n$$\n:::\n::::\n\n# End of matrix algebra interlude\n\n## Multiple linear regression\n\n. . .\n\nIn this course, we will rely heavily on multiple linear regression and expand on it by studying how some of the parameters (the $\\beta$s) can depend on other other data and parameters. \n\n. . .\n\nAs we saw earlier, a classic way to write multiple linear regression is \n\n$$\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\varepsilon\n$$\n\n. . .\n\nHowever, we can rewrite this using matrix notation has\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\n\n## Multiple linear regression\n**Matrix notation**\n\n$$\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n$$\n\nUsing the matrix notation, we assume that \n\n. . .\n\n\n::: { style=\"font-size: 0.9em\"}\n- $\\mathbf{y}$ is a vector of length $n$ (samples)\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\n- $\\mathbf{X}$ is a matrix with $n$ samples (rows) and representing $p$ explanatory variables (columns)\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\n- $\\boldsymbol{\\beta}$ is a vector of $p$ regression coefficients\n:::\n\n. . .\n\n::: { style=\"font-size: 0.9em\"}\n- $\\boldsymbol{\\varepsilon}$ is a vector of residuals of length $n$\n:::\n\n## Error in linear models\n\nAs previously mentioned, in (simple and multiple!) linear regression the error term ($\\varepsilon$) has actually a very precise definition:\n\n$$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$$ where $\\sigma^2$ is an estimated variance\n\n. . .\n\nwhich means that the error in a linear regression follows a Gaussian distribution with an estimated variance.\n\n. . .\n\n**Note** The model can be written using matrix notation as  \n$$\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{X}\\boldsymbol{\\beta}, \\sigma^2)$$ where $\\sigma^2$\n\n## Generalized linear models\n\n. . .\n\nIf for some reason we do not want our model to have a Gaussian error, *generalized* linear models (GLMs) have been proposed. In essence, GLMs use *link functions* to adapt models for them to be used on non-Gaussian data.\n\n. . .\n\nMathematically, the generic way to write generalized linear model is\n\n$$\n\\widehat{y}_i = g^{-1}(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip})\n$$\n\n. . .\n\nor in matrix notation\n\n$$\n\\widehat{\\mathbf{y}} = g^{-1}(\\mathbf{X}\\boldsymbol{\\beta})\n$$\n\nwhere $g$ is the link function and $g^{-1}$ the inverse link function.\n\n## Generalized linear models\n**Link functions**\n\n::: {style=\"font-size: 0.75em\"}\nThere are many types of link functions and they are usually directly associated to the underlying data the analysis is carried out on.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nArguably the most common link function in ecology is \n:::\n\n. . .\n\n*logit link function*\n\n::: {style=\"font-size: 0.75em\"}\nIt is commonly used for modelling binary (0-1) data.\n\n$$\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\frac{\\widehat{\\mathbf{y}}}{1 - \\widehat{\\mathbf{y}}}\\right)\n$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nThe inverse logit link function is \n\n$$\n\\widehat{\\mathbf{y}} = \\frac{\\exp(\\mathbf{X}\\boldsymbol{\\beta})}{1 + \\exp(\\mathbf{X}\\boldsymbol{\\beta})} = \\frac{1}{1 + \\exp(-\\mathbf{X}\\boldsymbol{\\beta})}\n$$\n:::\n\n## Generalized linear models\n**Link functions**\n\nAnother commonly used link function is\n\n. . .\n\n*log link function*\n\nIt is commonly used for modelling count data.\n\n$$\n\\mathbf{X}\\boldsymbol{\\beta} = \\ln\\left(\\widehat{\\mathbf{y}}\\right)\n$$\n\n. . .\n\nThe inverse logit link function is \n\n$$\n\\widehat{\\mathbf{y}} = \\exp(\\mathbf{X}\\boldsymbol{\\beta})\n$$\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}