{
  "hash": "a3a8ec67e825100769ada84eee0007c9",
  "result": {
    "markdown": "---\ntitle: \"Probability Distribution\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Guillaume Blanchet -- Andrew MacDonald\"\ndate: \"2024-04-29\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n## Probabilities\n\nTo understand **distributions**, we first need to have a basic understanding of *probabilities*.\n\n## A bit of history\n\n::: {style=\"font-size: 0.8em\"}\nUnlike many other fields of science, the first contributors in the study of probability were not scholars, they were gamblers !\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nFor example, the emperor Claudius (10 BC -- 54 AD), who was an avid gambler (he had a carriage built to allow him and his party to gamble while travelling) wrote a treatise on randomness and probability.\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/7/71/Claudius_crop.jpg){fig-align='center' width=20%}\n:::\n:::\n\n\n::: {style=\"font-size: 0.5em\"}\n[Lanciani (1892) Gambling and Cheating in Ancient Rome. *The North American Review* 155:97-105](https://www.jstor.org/stable/25102412)\n:::\n\n## A bit of history\n\n::: {style=\"font-size: 0.85em\"}\nIf you want a fun book to read about probabilities, its history and the difficulty of working with probabilities, I strongly recommend\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/en/3/35/The_Drunkard%27s_Walk.jpg){fig-align='center' width=30%}\n:::\n:::\n\n\n## The basics of probabilities\n\nA probability [**ALWAYS**]{style=\"color: red;\"} ranges between 0 and 1\n\n. . .\n\nA probability of [0]{style=\"font-size: 1.5em\"} means that an event is impossible\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n*Example*: The probability that a dog and a cat naturally reproduce is [0]{style=\"font-size: 1.5em\"}\n:::\n\n. . .\n\nA probability of [1]{style=\"font-size: 1.5em\"} means that an event is certain\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n*Example*: The probability that you are in this summer school as we speak is [1]{style=\"font-size: 1.5em\"}\n:::\n\n## The basics of probabilities\n\n**Notation**\n\n::: {style=\"font-size: 0.7em\"}\nA classic way to write the probability of an event is to use the notation $P$.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n***Example***\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\n::: {style=\"font-size: 0.7em\"}\nThe probability that it rains as you walk outside after the lecture is written as\n\n$$P(r)$$ where $r$ is the event you are interested in. Here, $r$ is whether it rains as you walk outside after the lecture today\n:::\n\n## Probabilities and events\n\n::: {style=\"font-size: 0.7em\"}\nWhen dealing with discrete (countable) events, it is very practical to know the number of events that can occur.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\nIn the simplest case, we can either measure the probability of an event to occur or not.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n***Example***\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\n::: {style=\"font-size: 0.65em\"}\nIt can either rain or not. Mathematically, the probability that it rains is written as\n\n$$P(r)$$ and the probability that it does not rain would be written as\n\n$$1 - P(r)$$\n:::\n\n## Probabilities and events\n\n::: {style=\"font-size: 0.7em\"}\nUsually, these basic notions of probabilities are presented using coin flipping. When a coin is flip, it is usually assumed that\n\n$$P(r)=0.5$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\nHowever, in probability theory, P(r) can have any value ranging between 0 and 1.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n***Example***\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\n::: {style=\"font-size: 0.7em\"}\nWhat do you think is the probability that it will rain at the end of the lecture?\n:::\n\n## Probabilities and events\n\nAt this point we can note that when we add the probabilites of all possible events, the sum will always be 1\n\n. . .\n\n***Example***\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\n$$P(r) + (1-P(r)) = 1$$\n\n. . .\n\nThis is true only if the events are independent from each other\n\n## Independent !?\n\n::: {style=\"font-size: 0.7em\"}\nEvents that are *independent* from each other means that the if an event occurs it is in no way related to the occurrence of another event.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n***Example***\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\n::: {style=\"font-size: 0.7em\"}\nIf we assume that weather events like a rainy day are independent from one another, it means that if it rains today it is unrelated to the weather of yesterday or tomorrow.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n**Note** : This can be a good or a bad or dangerous assumption to make depending on the problem you are working on.\n:::\n\n## Bernoulli distribution\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/19/Jakob_Bernoulli.jpg/800px-Jakob_Bernoulli.jpg){fig-align='center' width=40%}\n:::\n:::\n\n\n::: r-stack\nJacob Bernoulli (1655 - 1705)\n:::\n\n## Bernoulli distribution\n\nThe probability distribution (or probability mass function) of the Bernoulli distribution defines the probability of an event to occur given that there is only one other event that can occur (e.g. rain or no rain)\n\n. . .\n\nClassically, we will give a value of 1 to one event (no rain) and 0 to the other (rain).\n\n. . .\n\nFrom a mathematical perspective, it does not matter which event is given a 1 (or a 0). However, often it is common practice to choose how we give values based on the interpretation we make of the results.\n\n## Bernoulli distribution\n\nMathematically, the probability mass function of the Bernoulli distribution can be written as\n\n$$\\begin{align*}\np \\quad & \\text{if} \\quad x =1\\\\\n(1-p) \\quad & \\text{if}\\quad  x =0\n\\end{align*}$$\n\nwhere $p$ is a shorthand for $P(x)$ and $x$ is one of two events.\n\n## Moment interlude\n\nUsing probability distributions is practical because from them we can derive general information characterizing the each distribution.\n\n. . .\n\nThese characteristics are know as [moments]{style=\"color: blue;\"} of a distribution... And you know them :\n\n. . .\n\n-   Mean\n-   Variance\n-   Skewness\n-   Kurtosis\n-   ...\n\n## Moments of the Bernoulli distribution\n\n::: {style=\"font-size: 0.8em\"}\nFor the sake of conciseness, in this course, we will discuss only the first two moments of distributions.\n\n**Mean**\n\n$$p$$\n\n***Example***\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\nIf the probability that it rains is $p=0.14$ in any given day, it means that, on average in a week (7 days) we should expect it will rain 1 day.\n:::\n\n## Moments of the Bernoulli distribution\n\n::: {style=\"font-size: 0.8em\"}\n**Variance**\n\n$$p(1-p)$$ ***Example***\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\nIf the probability that it rains is $p=0.5$ in any given day, it means that, across multiple weeks, some weeks might have no rain while some weeks it might rain all days because the variance is\n\n$$p(1-p)=0.5\\times(1-0.5)=0.25\\quad\\text{and}\\quad \\sqrt{0.25} = 0.5$$\n:::\n\n## Moments of the Bernoulli distribution\n\nIf you want to go deeper down and learn about the other moments of the Bernoulli distribution (as well as other aspect of the distribution), take a look at the Wikipedia page of the Bernoulli distribution\n\n<https://en.wikipedia.org/wiki/Bernoulli_distribution>\n\n## Let's make it more complicated\n\nSo far, we focused on a situation where the two events to consider either occur or not.\n\n. . .\n\nThere are many problems were interest lies in studying the likeliness of an event occurring over a known number of independent trials.\n\n. . .\n\n***Example***\n\nHow many rainy day will there be during the five days of our summer school ?\n\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){width=\"15%\"} ![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){width=\"15%\"} ![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){width=\"15%\"}\n\n## Binomial distribution\n\nPut differently, the binomial distribution is designed to approach questions where we are interested in finding the number of success (e.g. it rains !) out of a known set of independent trials (e.g. the five days of the summer school).\n\n. . .\n\nThe binomial distribution is a generalisation of the Bernoulli distribution\n\n. . .\n\nIt is a common distribution used when sampling is done with replacement\n\n. . .\n\nLet's take a look at the math of the Binomial distribution\n\n## Binomial distribution\n\n**Probability mass function**\n\n$$\\binom{n}{k}p^k(1-p)^{n-k}$$\n\nwhere\n\n-   $n$ : Number of trails\n-   $k$ : Number of success (an event occurs)\n-   $p$ : Probability that an event occurs\n\nNote that $n \\ge k$\n\n## Mathematical technicalities interlude\n\n::: {style=\"font-size: 0.8em\"}\n$$\\binom{n}{k}$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$$\\frac{n!}{k!(n-k)!}$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\n$$\\frac{n\\times(n-1)\\times(n-2)\\times\\dots\\times 2\\times 1}{(k\\times(k-1)\\times(k-2)\\times\\dots\\times 2\\times 1)(n-k)\\times(n-k-1)\\times(n-k-2)\\times\\dots\\times 2\\times 1}$$\n:::\n\n## Moment of the binomial distribution\n\n::: {style=\"font-size: 0.75em\"}\nAgain, for conciseness, we will focus on the first moment (mean) and second moment (variance) of the binomial distribution.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\n**Mean** $$np$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\n**Example**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\n::: {style=\"font-size: 0.75em\"}\nIf the probability that it rains is $p=0.14$ in any given day of the 5 days of the summer school, it means that on average we expect it will rain 0.7 days of the summer school (so 1 or no days)\n\n$$np = 5 \\times 0.14 = 0.7$$\n:::\n\n## Moment of the binomial distribution\n\n::: {style=\"font-size: 0.75em\"}\n**Variance**\n\n$$np(1-p)$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\n**Example**\n:::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/telegram/386/cloud-with-rain_1f327-fe0f.webp){fig-align='center' width=15%}\n:::\n:::\n\n\n::: {style=\"font-size: 0.75em\"}\nIf the probability that it rains is $p=0.5$ in any given day, it means that, across multiple weeks (7 days), roughly speaking some weeks might have 1 days of rain while others might have 5 because the variance is\n\n$$np(1-p)=7 \\times 0.5\\times(1-0.5)=1.75\\quad\\text{and}\\quad \\sqrt{1.75} = 1.3229$$\n:::\n\n## Moments of the binomial distribution\n\nIf you want to learn more about the other moments of the binomial distribution (as well as other aspect of the distribution), take a look at the Wikipedia page of the binomial distribution\n\n<https://en.wikipedia.org/wiki/binomial_distribution>\n\n## Binomial distribution\n\nThe binomial distribution is related to many other probability distribution\n\n. . .\n\n-   Bernoulli distribution (as we have seen)\n\n. . .\n\n-   Poisson distribution (when there are an infinite number of trials while $np$ converge to a finite value)\n\n. . .\n\n-   Normal distribution...\n\n. . .\n\n![](https://www.i2symbol.com/pictures/emojis/6/b/8/f/6b8f4b33b349292e182a03813323165e_384.png){fig-align=\"center\" width=\"20%\"}\n\n## Jia Xian triangle\n\n![](https://upload.wikimedia.org/wikipedia/commons/e/ea/Yanghui_triangle.gif){fig-align=\"center\"}\n\n## Pascal's triangle\n\n![](https://upload.wikimedia.org/wikipedia/commons/6/66/TrianguloPascal.jpg){fig-align=\"center\"}\n\n## Pascal's triangle\n\n![](https://upload.wikimedia.org/wikipedia/commons/0/0d/PascalTriangleAnimated2.gif){fig-align=\"center\" width=\"50%\"}\n\n## Pascal's triangle\n\n::: {style=\"font-size: 0.7em\"}\nThe Pascal's triangle is directly related to the binomial distribution with $p=0.5$.\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n**If** $n = 3$\n\n*When* $k=0$\n:::\n\n::: {style=\"font-size: 0.525em\"}\n$$\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{0}\\times0.5^0 \\times (1-0.5)^{(3-0)}=0.125$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n*When* $k=1$\n:::\n\n::: {style=\"font-size: 0.525em\"}\n$$\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{1}\\times0.5^1 \\times (1-0.5)^{(3-1)}=0.375$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n*When* $k=2$\n:::\n\n::: {style=\"font-size: 0.525em\"}\n$$\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{2}\\times0.5^1 \\times (1-0.5)^{(3-2)}=0.375$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.7em\"}\n*When* $k=3$\n:::\n\n::: {style=\"font-size: 0.525em\"}\n$$\\binom{n}{k}p^k(1-p)^{n-k}=\\binom{3}{3}\\times0.5^1 \\times (1-0.5)^{(3-3)}=0.125$$\n:::\n\n## Pascal's triangle\n\nThe Pascal's triangle is directly related to the binomial distribution with $p=0.5$.\n\n**If** $n = 3$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Pascal's triangle\n\nThe Pascal's triangle is directly related to the binomial distribution with $p=0.5$.\n\n**If** $n = 10$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Pascal's triangle\n\nThe Pascal's triangle is directly related to the binomial distribution with $p=0.5$.\n\n**If** $n = 50$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Pascal's triangle\n\nThe Pascal's triangle is directly related to the binomial distribution with $p=0.5$.\n\n**If** $n = 200$\n\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Binomial and Gaussian distribution\n\nIf the number trials ($n$) is large enough, it approximate to a Gaussian distribution\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n## Gaussian (Normal) distribution\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Carl_Friedrich_Gauss_1840_by_Jensen.jpg/804px-Carl_Friedrich_Gauss_1840_by_Jensen.jpg){fig-align='center' width=35%}\n:::\n:::\n\n\n::: r-stack\nCarl Friedrich Gauss (1777-1855)\n:::\n\n## Gaussian (Normal) distribution\n\nUnlike the binomial distribution, the Gaussian distribution is a continuous distribution\n\n. . .\n\nIt is the a very common distribution that is underlying many random natural phenomenon and it is the basis of statistical theory\n\n. . .\n\nLet's take a look at the mathematical formulation of the Gaussian distribution\n\n## Gaussian (Normal) distribution\n\n**Probability density function**\n\n$$\\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}\\right)^2}$$\n\nwhere\n\n-   $x$ : continuous variable of interest\n-   $\\mu$ : The mean of the distribution\n-   $\\sigma$ : The standard deviation of the distribution\n\n## Moment of the Gaussian distribution\n\n**Mean**\n\n$$\\mu$$\n\n***Example***\n\n![](https://www.lesaffaires.com/uploads/images/normal/eb7bfc01b807ccc40a87b4fc539966d5.jpg){fig-align=\"center\" width=\"80%\"}\n\n::: {style=\"font-size: 0.8em\"}\nLet's say we measure the length of the left wing of individual of this species of (angry) birds, it is expected that the wing length will follow a Gaussian distribution with a mean of $\\mu$. We will look at this in more details in the practical exercices later today.\n:::\n\n## Moment of the Gaussian distribution\n\n**Variance**\n\n$$\\sigma^2$$\n\n***Example***\n\n![](https://www.lesaffaires.com/uploads/images/normal/eb7bfc01b807ccc40a87b4fc539966d5.jpg){fig-align=\"center\" width=\"80%\"}\n\n::: {style=\"font-size: 0.8em\"}\nLet's say we measure the length of the left wing of individual of this species of (angry) birds, it is expected that the wing length will follow a Gaussian distribution with a variance of $\\sigma^2$. We will look at this in more details in the practical exercices later today.\n:::\n\n## General properties of distributions\n\nIn R, there are 4 functions associated to every distribution. As an example, for the Gaussian distribution, they are\n\n-   `rnorm`\n-   `dnorm`\n-   `pnorm`\n-   `qnorm`\n\n. . .\n\nKnowing what these functions do will be very useful for this course\n\n## `rnorm`\n\n::: {style=\"font-size: 0.8em\"}\nThe `r` in `rnorm` is for [random]{style=\"color: blue;\"}\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nThis function allows us to randomly sample directly from the distribution of interest.\n:::\n\n. . .\n\n**Example**\n\n![](https://em-content.zobj.net/source/twitter/348/thermometer_1f321-fe0f.png){fig-align=\"center\" width=\"8%\"}\n\n::: {style=\"font-size: 0.8em\"}\nLet's assume that we look at the historical record and gather the minimum temperature measured on today's date for the past 40 years. This data can be assumed to be a random sample of 40 temperature measurements with an average of $4^{\\circ}C$ and a standard deviation of $2^{\\circ}C$. We can simulate these values as follow\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrnorm(40, mean = 4, sd = 2)\n```\n:::\n\n\n## `rnorm(40, mean = 4, sd = 2)`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n## `dnorm`\n\n::: {style=\"font-size: 0.8em\"}\nThe `d` in `dnorm` is for [density]{style=\"color: blue;\"}\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nThis function gives the height of the distribution for a chosen value.\n:::\n\n. . .\n\n**Example**\n\n![](https://em-content.zobj.net/source/twitter/348/thermometer_1f321-fe0f.png){fig-align=\"center\" width=\"8%\"}\n\n::: {style=\"font-size: 0.8em\"}\nIf we assume that the average temperature at this time of the year is $4^{\\circ}C$ with a standard deviation of $2^{\\circ}C$, we can calculate that the likeliness that a temperature of $6^{\\circ}C$ to occur is\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndnorm(6, mean = 4, sd = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1209854\n```\n:::\n:::\n\n\n## `dnorm(6, mean = 4, sd = 2)`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n## `qnorm`\n\n::: {style=\"font-size: 0.8em\"}\nThe `q` in `qnorm` is for [quantile]{style=\"color: blue;\"}\n:::\n\n. . .\n\n::: {style=\"font-size: 0.8em\"}\nThis function gives the value of the distribution given a certain density\n:::\n\n. . .\n\n**Example**\n\n![](https://em-content.zobj.net/source/twitter/348/thermometer_1f321-fe0f.png){fig-align=\"center\" width=\"8%\"}\n\n::: {style=\"font-size: 0.8em\"}\nIf we assume that the average temperature at this time of the year is $4^{\\circ}C$ with a standard deviation of $2^{\\circ}C$, `qnorm` allows us to calculate the temperature expected to be obtained at the lowest quartile (1/4). It is calculate as\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqnorm(0.25, mean = 4, sd = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.65102\n```\n:::\n:::\n\n\n## `qnorm(0.25, mean=4, sd=2)`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-24-1.png){width=960}\n:::\n:::\n\n\n## `pnorm`\n\nThe `p` in `pnorm` is for [probability distribution function]{style=\"color: blue;\"}\n\n. . .\n\nThis function gives the integral (area under the curve) up to a specified value.\n\n. . .\n\nThis is particularly useful because it informs us about the probability that an event is likely to occur (of course assuming a it follows a normal distribution).\n\n## `pnorm`\n\n**Example**\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](https://em-content.zobj.net/source/twitter/348/thermometer_1f321-fe0f.png){fig-align='center' width=5%}\n:::\n:::\n\n\nIf we assume that the average temperature at this time of the year is $4^{\\circ}C$ with a standard deviation of $2^{\\circ}C$, `pnorm` will tell us that the probability to have a temperature lower or equal to $6^{\\circ}C$. This is calculated as\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(6, mean = 4, sd = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8413447\n```\n:::\n:::\n\n\n## `pnorm(6, mean = 4, sd = 2)`\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-27-1.png){width=960}\n:::\n:::\n\n\n## Binomial and Bernoulli distribution\n\nAs I mentionned previsouly, these function are available in base R for a large number of distributions.\n\n## Binomial distribution\n\n-   `rbinom`\n-   `dbinom`\n-   `qbinom`\n-   `pbinom`\n\n## Bernoulli distribution\n\nHowever, sometimes we must know a little bit of theory (as I have show today) to use the right function.\n\n-   `rbinom`\n-   `dbinom`\n-   `qbinom`\n-   `pbinom`\n\n![](https://www.i2symbol.com/pictures/emojis/c/6/0/e/c60e666a9af7bcd1a7b887437b3520c3_384.png){fig-align=\"center\" width=\"20%\"}\n\n## Bernoulli distribution\n\n-   `rbinom`\n-   `dbinom`\n-   `qbinom`\n-   `pbinom`\n\nWith `size = 1`\n\n## Other distributions\n\nStatisticians and biologists have been very, very (!!) creative in proposing new probability distribution for specific problems\n\n. . .\n\nIf you want to learn about the diversity of distribution that is out there, take a look at :\n\n<https://en.wikipedia.org/wiki/List_of_probability_distributions>\n\n. . .\n\nMany of them have been implemented in R, either in base R or specialized packages\n\n. . .\n\nIf you want to know if you favourite distribution has been implemented in R take a look at\n\n<https://cran.r-project.org/web/views/Distributions.html>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}