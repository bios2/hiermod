{
  "hash": "0d6e37025438b28d949914c5bf2be9e1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian modelling\"\ntitle-slide-attributes: \n  data-background-image: ../img/bg.jpg\n  data-background-size: full\nauthor: \"Guillaume Blanchet -- Andrew MacDonald\"\ndate: \"2025-05-06\"\nexecute:\n  echo: true\nformat: \n  revealjs:\n    theme: [default]\n    logo: ../img/UdeS_logo_h_rgbHR.png\n    transition: slide\n    background-transition: fade\n---\n\n\n\n## Frequentist \n. . . \n\n::: {style=\"font-size: 0.8em\"}\nIn introductory statistics course, it is common to rely on the frequentist paradigm when inferring results from data.\n::: \n\n. . . \n\n::: {style=\"font-size: 0.8em\"}\nFrequentists want to find the best model parameter(s) for the data at hand.\n::: \n\n. . . \n\n::: {style=\"font-size: 0.8em\"}\n$$\\text{Likelihood}\\hspace{1.5cm}P(\\text{Data}|\\text{Model})$$\n::: \n\n. . . \n\n::: {style=\"font-size: 0.8em\"}\nThey are interested in **maximizing** the Likelihood\n\nThey need **data**\n::: \n\n. . . \n\n### Estimating model parameters\n\n::: {style=\"font-size: 0.8em\"}\n- Minimizing the sums of squares\n- Simulated annealing\n- Nelder-Mead Simplex\n- ...\n::: \n\n\n## Bayesian\n\n. . . \n\n::: {style=\"font-size: 0.75em\"}\nBayesians want to find how good the model parameter(s) are given some data\n:::\n\n. . . \n\n::: {style=\"font-size: 0.75em\"}\n$$\\text{Posterior}\\hspace{1.5cm}P(\\text{Model}|\\text{Data})$$\n:::\n\n. . . \n\n::: {style=\"font-size: 0.75em\"}\nThey are interested in the **posterior** distribution\n:::\n\n. . . \n\n::: {style=\"font-size: 0.75em\"}\nThey need **data** and **prior** information\n:::\n\n. . . \n\n::: {style=\"font-size: 0.75em\"}\nThe general framework used in Bayesian modelling is \n:::\n\n::: {style=\"font-size: 0.75em\"}\n$$\\underbrace{P(\\text{Model}|\\text{Data})}_\\text{Posterior}\\propto \\underbrace{P(\\text{Data}|\\text{Model})}_\\text{Likelihood}\\underbrace{P(\\text{Model})}_\\text{Prior}$$\n:::\n\n. . . \n\n#### Estimating model parameters\n\n::: {style=\"font-size: 0.75em\"}\n- Markov Chain Monte Carlo\n- Hamiltonian Monte Carlo\n- ...\n:::\n\n## Our way of thinking is Bayesian\n\n[![](../img/Fantastic_Beast.png){width=1000 height=500}](https://youtu.be/uoEnGiG9aWA?t=34)\n\n## A few words about the prior\n\n. . .\n\n**Definition of prior probability**\n\n. . .\n\n::: {style=\"font-size: 0.75em\"}\nThe **prior probability** informes us about the probability of the model being true *before* considering any available data\n:::\n. . .\n\n**Types of priors**\n\n. . .\n\n*\"Uninformative\"*\n\n::: {style=\"font-size: 0.75em\"}\nThese priors are meant to bring very little information about the model\n:::\n\n. . .\n\n\n*Informative*\n\n::: {style=\"font-size: 0.75em\"}\nThese priors bring information about the model that is available\n:::\n\n## A few words about the prior\n\n**\"Uninformative\" priors**\n\n. . .\n\n:::::{style=\"font-size: 0.75em\"}\n**Example** If we have no idea of how elevation influence sugar maple\n:::::\n\n. . .\n\n*Gaussian distribution*\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n:::::{style=\"font-size: 0.75em\"}\n$$f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n:::::\n::: \n::: {.column width=\"50%\"}\n:::::{style=\"font-size: 0.75em\"}\n$\\mu = 0$\n\n$\\sigma = \\text{Large say 100}$\n:::::\n:::\n::::\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n## A few words about the prior\n\n**Informative priors**\n\n. . . \n\n:::::{style=\"font-size: 0.6em\"}\n*Example* If we know that \n  \n  - There are less sugar maples the higher we go\n  - The influence of elevation on sugar maple cannot be more than two folds\n\n*Uniform distribution*\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n$$f(x)=\\left\\{\n  \\begin{array}{cl}\n    \\frac{1}{b-a} & \\text{for } x\\in [a,b]\\\\\n    0 &\\text{otherwise}\\\\\n  \\end{array}\n\\right.$$\n:::\n::: {.column width=\"50%\"}\n  \\item $a > -2$\n  \n  \\item $b < 0$\n:::\n::::\n:::::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Estimating Bayesian model\n\n. . .\n\n:::{style=\"font-size: 0.75em;\"}\nAs I hinted earlier, there are a number of ways to estimate the parameters of a Bayesian model. A common way to estimate Bayesian models is to rely on Markov Chain Monte Carlo (MCMC) or variants of it, including Hamiltonian Monte Carlo (HMC), which is used in Stan \n:::\n. . .\n\n**Typical reasons to favour MCMC**\n\n. . .\n\n:::{style=\"font-size: 0.75em;\"}\n- It is flexible\n- It can be applied to complex models such as models with multiple levels of hierarchy\n:::\n. . .\n\n**Why should we learn about MCMC ?**\n\n. . .\n\n:::{style=\"font-size: 0.75em;\"}\nThe goal of this course is **not** to learn the intricacies of MCMC or HMC, but since we will play a lot with Stan, it is important to learn at least conceptually how MCMC and HMC work.\n:::\n\n## Markov Chain Monte Carlo (MCMC)\n\n:::{style=\"font-size: 0.75em;\"}\nHistorically, the developments of MCMC have been intimately linked with the arrival of computers. Specifically, the first developments and applications of MCMC were made during the Los Alamos projects.\n\nTo explain what is an MCMC, let's imagine that we are interested in understanding how the mallard (*Anas platyrhynchos*) grows from hatchling to adult.\n:::\n\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Anas_platyrhynchos_male_female_quadrat.jpg/1024px-Anas_platyrhynchos_male_female_quadrat.jpg){fig-align=\"center\" width=10%}\n\n## Markov Chain Monte Carlo (MCMC)\n**A simplistic statistical example**\n\n. . .\n\n::: {style=\"font-size: 0.75em;\"}\nLet's say that we are interested in modelling how male Mallard weight changes as they grow from hatching to adult. Here are some data obtained from a local Mallard duck farm.\n:::\n. . .\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Markov Chain Monte Carlo (MCMC)\n**A simplistic statistical example**\n\n. . .\n\n::: {style=\"font-size: 0.65em;\"}\nA growth model can be constructed using a simple linear regression\n\n$$y = \\beta_0 + \\beta x + \\varepsilon$$\nFrom this model we can infer that the average weight of a Mallard duck when it hatches is 107 grams (intercept), and the average daily growth of the Mallard is 13.5 grams (slope).\n:::\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Markov Chain Monte Carlo (MCMC)\n**A simplistic statistical example**\n\n::: {style=\"font-size: 0.85em;\"}\nEstimating the intercept and slope parameters of the simple linear regression can be done with an MCMC. This amount to sampling \n:::\n\n::: {style=\"font-size: 0.85em;\"}\n$\\beta_0$ as\n:::\n\n. . .\n\n::: {style=\"font-size: 0.85em;\"}\n$$\\beta_0 \\sim \\mathcal{D}(\\text{mean}, \\text{variance}, \\text{skewness}, \\text{kurtosis}, \\dots)$$\n:::\n\n. . .\n\n::: {style=\"font-size: 0.85em;\"}\nand $\\beta$ as \n:::\n\n. . .\n\n\n::: {style=\"font-size: 0.85em;\"}\n$$\\beta \\sim \\mathcal{D}(\\text{mean}, \\text{variance}, \\text{skewness}, \\text{kurtosis}, \\dots)$$\n:::\n\n\n::: {style=\"font-size: 0.85em;\"}\nIn doing so, we are not focusing on finding the 'best' parameter values. Rather, we are focused on finding the **distribution** of best parameter values.\n:::\n\n## Markov Chain Monte Carlo (MCMC)\n\n. . .\n\nWhen using an MCMC, we are interested in sampling distributions to estimate the parameters of the model.\n\n. . . \n\nTechnically, MCMC and HMC rely on different approaches to assess the structure of the distributions. \n\n## Markov Chain Monte Carlo (MCMC)\n\nMCMC relies on using many random samples to assess the structure of the distribution.\n\n![](../img/Drunken_walk.png){fig-align=\"center\"}\n\n## Hamiltonian Monte Carlo\n\nHamiltonial Monte Carlo relies on Hamiltonian dynamics to assess the structure of the distribution.\n\n![](https://www.westcoasttraveller.com/wp-content/uploads/2021/03/24581102_web1_210322-WCT-WestcoastSkateparks_1-800x533.jpg){fig-align=\"center\" width=80%}\n\n## Sampling the parameters\n\n. . .\n\n:::{style=\"font-size: 0.9em;\"}\nIn MCMC and HMC, a lot of iterations need to be carried out to assess the distribution of parameters. But how many is enough ?\n:::\n\n. . .\n\n:::{style=\"font-size: 0.9em;\"}\nHere is a rough procedure to follow:\n\n1. Perform a pilot run with a reduced number of iterations (e.g. 10) and measure the time it takes\n2. Decide on a number of steps to use to obtain a result in a reasonable amount of time\n3. Run the algorithm again !\n4. Study the chain visually\n:::\n\n## Studying convergence\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/trace-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Studying convergence\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/thining1 -1.png){width=960}\n:::\n:::\n\n\n\n\n## Studying convergence\n\n:::{style=\"font-size: 0.7em;\"}\nIf we ran the same MCMC as above but instead for 50000 steps, we obtain\n:::\n\n. . .\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/thining2-1.png){width=960}\n:::\n:::\n\n\n\n. . .\n\n:::{style=\"font-size: 0.7em;\"}\n**Note** It is also possible to *thin* (record only the $n^{\\text{th}}$ iteration), but it is better to keep all iterations when possible. Actually, *thinning* should only be carried out when there are too many iterations for you to keep them and manipulate them in the memory of your computer.\n:::\n\n## Studying convergence\n**Burn-in**\n\nBurn-in is throwing away some iterations at the beginning of the MCMC run \n\n. . .\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/burnin1 -1.png){width=960}\n:::\n:::\n\n\n\n## Studying convergence\n**Burn-in**\n\nAfter burn-in, we obtain\n\n. . . \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-revealjs/burnin2 -1.png){width=960}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}