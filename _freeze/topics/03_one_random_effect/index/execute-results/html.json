{
  "hash": "2775831bd6e6bfd6044ac60d5d342dac",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models with one level of hierarchy\"\ndescription: |\n  Some of these things are somewhat like the others.\nexecute:\n  freeze: true\ncomments:\n  hypothesis: true\nformat:\n  html:\n    code-tools: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n:::{.callout-tip}\n## Bayesian workflow\n\n1. Visualize your data\n2. Decide on your model structure\n3. Simulate from the model to understand it\n4. Fit the model to the data\n5. Plot model predictions to evaluate the fit / draw conclusions\n:::\n\nToday's goal is to look at a couple of different model structures that we saw yesterday. \n\n## Load packages and data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nsuppressPackageStartupMessages(library(dplyr))\nlibrary(ggplot2)\nlibrary(tidyr)\n# library(cmdstanr)\nsuppressPackageStartupMessages(library(rstan))\nrstan_options(\"auto_write\" = TRUE)\noptions(mc.cores = parallel::detectCores())\nlibrary(tidybayes)\n```\n:::\n\n\n\n\n## Gaussian random intercepts: Penguin body mass\n\n**Are populations of penguins on different islands different in their body mass?**\n\nThe Palmer penguins are found on three different islands. Let's look at the distribution of body mass of each species on each island.\n\n### Plot the data\n\nFirst a bit of data cleaning and preparation.\nFirst we select our variables. \nFor this section we'll drop NA values in the predictor^[dropping NA values is not always the best idea! I'm doing it here to create a focused example. Bayes gives us other options for working with missing values, including modelling them directly.].\nWe'll create a new variable out of the union of the species and island names^[again, this is slightly contrived for the sake of making a clean example! normally you'd most likely treat these two variables separately.]\nWe'll also change the units of body mass to kilograms.\nYou can always transform a variable to more sensible units! \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_mass_island <- penguins |> \n  select(species, island, body_mass_g) |> \n  tidyr::drop_na(body_mass_g) |> \n  tidyr::unite(sp_island, species, island) |> \n  ## center mass and change the units\n  mutate(mass_kg = (body_mass_g)/1000)\n\nknitr::kable(head(penguin_mass_island))\n```\n\n::: {.cell-output-display}\n\n\n|sp_island        | body_mass_g| mass_kg|\n|:----------------|-----------:|-------:|\n|Adelie_Torgersen |        3750|   3.750|\n|Adelie_Torgersen |        3800|   3.800|\n|Adelie_Torgersen |        3250|   3.250|\n|Adelie_Torgersen |        3450|   3.450|\n|Adelie_Torgersen |        3650|   3.650|\n|Adelie_Torgersen |        3625|   3.625|\n\n\n:::\n:::\n\n\n\nLet's visualize the distribution of body sizes for these species+island combinations:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_mass_island |> \n  ggplot(aes(y = sp_island,\n             x = mass_kg,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\") + \n  labs(x = \"Mass in kg\", y = \"Species_Island\")\n```\n\n::: {.cell-output-display}\n![Observations of the mass of penguins on five different species-island combinations.](index_files/figure-html/gauss-inter-plot-1.png){width=672}\n:::\n:::\n\n\n\nIt's always good to ask some questions about the dataset. \nHere is a simple one: are the sample sizes equal among the species-island combinations?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_mass_island |> \n  count(sp_island) |> \n  knitr::kable()\n```\n\n::: {.cell-output-display}\n\n\n|sp_island        |   n|\n|:----------------|---:|\n|Adelie_Biscoe    |  44|\n|Adelie_Dream     |  56|\n|Adelie_Torgersen |  51|\n|Chinstrap_Dream  |  68|\n|Gentoo_Biscoe    | 123|\n\n\n:::\n:::\n\n\n\n### Decide on a model structure\n\nWe'll begin by fitting a model that assumes that body size for each of these five groups is completely independent:\n\n$$\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5)\n\\end{align}\n$$\n\nHere the subscript $i$ is just counting the row of the dataset, and $\\text{group}[i]$ means the group (species+island) to which row number $i$ belongs.\n\n### Simulate to understand this model {#sec-fixed-simulation}\n\nHere's a little trick to get group indexes (numbers) from a character vector:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroup_names <- unique(penguin_mass_island$sp_island)\ngroup_numbers <- seq_along(group_names)\nnames(group_numbers) <- group_names\n\ngroup_numbers\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAdelie_Torgersen    Adelie_Biscoe     Adelie_Dream    Gentoo_Biscoe \n               1                2                3                4 \n Chinstrap_Dream \n               5 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npenguin_groupid <- penguin_mass_island |> \n  mutate(group_id = group_numbers[sp_island])\n\npenguin_groupid\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 342 × 4\n   sp_island        body_mass_g mass_kg group_id\n   <chr>                  <int>   <dbl>    <int>\n 1 Adelie_Torgersen        3750    3.75        1\n 2 Adelie_Torgersen        3800    3.8         1\n 3 Adelie_Torgersen        3250    3.25        1\n 4 Adelie_Torgersen        3450    3.45        1\n 5 Adelie_Torgersen        3650    3.65        1\n 6 Adelie_Torgersen        3625    3.62        1\n 7 Adelie_Torgersen        4675    4.68        1\n 8 Adelie_Torgersen        3475    3.48        1\n 9 Adelie_Torgersen        4250    4.25        1\n10 Adelie_Torgersen        3300    3.3         1\n# ℹ 332 more rows\n```\n\n\n:::\n:::\n\n\n\nAs you can see, we're set up now with the names and the indexes we need. \n\nNow we can simulate data and plot it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngroup <- length(group_numbers)\noverall_mean <- rnorm(1, mean = 5, sd = 2)\ngroup_diffs <- rnorm(n = ngroup, mean = 0, sd = 1)\nsigma_obs <- rexp(1, .5)\n\npenguin_pred_obs <- penguin_groupid |> \n  mutate(fake_mass_avg = overall_mean + group_diffs[group_id],\n         fake_mass_obs = rnorm(length(fake_mass_avg), \n                               mean = fake_mass_avg, \n                               sd = sigma_obs))\n\npenguin_pred_obs |> \n  ggplot(aes(y = sp_island,\n             x = fake_mass_obs,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n:::{.callout-tip}\n### EXERCISE\nRun the above code a few times, trying different prior values. \n:::\n\n### Write it in Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_groups <- stan_model(file = here::here(\"topics/03_one_random_effect/fixed_groups.stan\"),\n                           model_name = \"fixed_groups\")\n\nfixed_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` stan\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  int<lower=0> Ngroup;\n  array[N] int<lower=0, upper=Ngroup> group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma);\n  b_group ~ std_normal();\n  b_avg ~ normal(5, 2);\n  sigma ~ exponential(.5);\n}\ngenerated quantities {\n\n  vector[Ngroup] group_averages;\n\n  for (k in 1:Ngroup){\n    group_averages[k] = b_avg + b_group[k];\n  }\n\n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n\n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(group_averages[k], sigma);\n  }\n} \n```\n\n\n:::\n:::\n\n\n\n### Fit the model\n\nFitting the model requires arranging the data first, and creating a list which we then feed into Stan.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npeng_group_list <- with(penguin_groupid, \n         list(\n           N = length(mass_kg),\n           y = mass_kg,\n           Ngroup = max(group_id),\n           group_id = group_id\n         ))\n\nfixed_groups_samples <- sampling(fixed_groups,\n  data = peng_group_list,\n  refresh = 0L\n)\n```\n:::\n\n\n\n### Plot predictions to evaluate results\n\nLet's begin by plotting the averages for each group.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_groups_samples |> \n  gather_rvars(group_averages[group_id]) |> \n  mutate(sp_island = names(group_numbers)[group_id]) |> \n  ggplot(aes(y = sp_island, dist = .value)) + \n  stat_pointinterval() + \n  geom_jitter(data = penguin_mass_island,\n              aes(y = sp_island,\n                  x = mass_kg,\n                  colour = sp_island), \n              pch = 21, inherit.aes = FALSE,\n              alpha = 0.8, height = 0.1, width = 0) + \n  scale_colour_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nSome things to notice about the code above: \n\n* I'm using my named vector `group_numbers` to re-create the column `sp_island`. This is my technique for making sure I always use the correct label, but you can do this any way you want.\n* We use `tidybayes::stat_pointinterval()` to summarize the posterior distribution.\n* we're adding points from the original data (`penguin_mass_island`) with `geom_jitter()`. We're adding noise vertically to make the visualization better, but not adding any horizontal noise.\n\n:::{.callout-tip}\n### EXERCISE: plot posterior predictions of _observations_\n\nRepeat the exercise above using the value of `one_obs_per_group`. \nWhy are the results different? What additional error is included in these predictions?\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n### SOLUTION\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_groups_samples |> \n  tidybayes::gather_rvars(one_obs_per_group[group_id]) |> \n  mutate(sp_island = group_names[group_id]) |> \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval(colour = \"black\") + \n  geom_jitter(\n    aes(y = sp_island,\n        x = mass_kg,\n        colour = sp_island), \n    inherit.aes = FALSE,\n    alpha = .2, data = penguin_groupid, height = .2, width = 0) + \n  scale_colour_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n:::\n\n### Make it hierarchical\n\n#### Math\n\n\n:::{.column-screen}\n\n::::{.columns}\n\n::: {.column width=\"2.5%\"}\n:::\n\n::: {.column width=\"45%\"}\n$$\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, 1) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5)\n\\end{align}\n$$\n:::\n\n::: {.column width=\"5%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n$$\n\\begin{align}\n\\text{Body mass}_i &\\sim \\text{Normal}(\\mu_i, \\sigma_{\\text{obs}}) \\\\\n\\mu_i &= \\bar\\beta + \\beta_{\\text{group}[i]} \\\\\n\\bar\\beta &\\sim \\text{Normal}(5, 2) \\\\\n\\beta_{\\text{group}} &\\sim \\text{Normal}(0, \\sigma_{\\text{sp}}) \\\\\n\\sigma_{\\text{obs}} &\\sim \\text{Exponential}(.5) \\\\\n\\sigma_{\\text{sp}} &\\sim \\text{Exponential}(1)\n\\end{align}\n$$\n\n:::\n\n::: {.column width=\"2.5%\"}\n\n:::\n\n::::\n\n:::\n\n#### Simulation of a hierarchical model\n\n:::{.callout-tip}\n### EXERCISE \nSimulate from the model above. Base your approach on the [code for simulation the non-hierarchical version](#sec-fixed-simulation). \nRemember to simulate one additional number: the standard deviation of group differences\n:::\n\n:::{.callout-note collapse=\"true\"}\n### SOLUTION\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngroup <- length(group_numbers)\noverall_mean <- rnorm(1, mean = 5, sd = 2)\nsigma_group <- rexp(1, .1)\ngroup_diffs <- rnorm(n = ngroup, mean = 0, sd = sigma_group)\nsigma_obs <- rexp(1, .5)\n\npenguin_pred_obs <- penguin_groupid |> \n  mutate(fake_mass_avg = overall_mean + group_diffs[group_id],\n         fake_mass_obs = rnorm(length(fake_mass_avg), \n                               mean = fake_mass_avg, \n                               sd = sigma_obs))\n\npenguin_pred_obs |> \n  ggplot(aes(y = sp_island,\n             x = fake_mass_obs,\n             colour = sp_island)) + \n  geom_jitter(alpha = 0.8, height = 0.1, width = 0) + \n  scale_color_brewer(palette = \"Dark2\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n:::\n\n#### Stan\n\n\nBelow I'm comparing the two Stan programs side-by-side. Compare them to the models above! \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups <- stan_model(\n  file = \"topics/03_one_random_effect/hierarchical_groups.stan\")\n```\n:::\n\n\n\n:::{.column-screen}\n\n::::{.columns}\n\n::: {.column width=\"2.5%\"}\n:::\n\n::: {.column width=\"45%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixed_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` stan\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  int<lower=0> Ngroup;\n  array[N] int<lower=0, upper=Ngroup> group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real<lower=0> sigma;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma);\n  b_group ~ std_normal();\n  b_avg ~ normal(5, 2);\n  sigma ~ exponential(.5);\n}\ngenerated quantities {\n\n  vector[Ngroup] group_averages;\n\n  for (k in 1:Ngroup){\n    group_averages[k] = b_avg + b_group[k];\n  }\n\n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n\n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(group_averages[k], sigma);\n  }\n} \n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: {.column width=\"5%\"}\n\n:::\n\n::: {.column width=\"45%\"}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` stan\nS4 class stanmodel 'anon_model' coded as follows:\ndata {\n  int<lower=0> N;\n  vector[N] y;\n  int<lower=0> Ngroup;\n  array[N] int<lower=0, upper=Ngroup> group_id;\n}\nparameters {\n  real b_avg;\n  vector[Ngroup] b_group;\n  real<lower=0> sigma_obs;\n  real<lower=0> sigma_grp;\n}\nmodel {\n  y ~ normal(b_avg + b_group[group_id], sigma_obs);\n  b_group ~ normal(0, sigma_grp);\n  b_avg ~ normal(5, 2);\n  sigma_obs ~ exponential(.5);\n  sigma_grp ~ exponential(1);\n}\ngenerated quantities {\n\n  vector[Ngroup] group_averages;\n\n  for (k in 1:Ngroup){\n    group_averages[k] = b_avg + b_group[k];\n  }\n\n  // predict making one new observation per group\n  vector[Ngroup] one_obs_per_group;\n\n  for (k in 1:Ngroup) {\n    one_obs_per_group[k] = normal_rng(group_averages[k], sigma_obs);\n  }\n\n  // difference for a new group\n  real new_b_group = normal_rng(0, sigma_grp);\n\n  // observations from that new group\n  real one_obs_new_group = normal_rng(b_avg + new_b_group, sigma_obs);\n\n} \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: {.column width=\"2.5%\"}\n\n:::\n\n::::\n\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples <- sampling(hierarchical_groups,\n  data = peng_group_list, refresh = 0)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n                      mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff\nb_avg                 4.02    0.01 0.36  3.27  3.82  4.01  4.21  4.82   633\nb_group[1]           -0.31    0.01 0.36 -1.11 -0.51 -0.30 -0.10  0.44   643\nb_group[2]           -0.30    0.01 0.37 -1.13 -0.50 -0.30 -0.10  0.43   636\nb_group[3]           -0.32    0.01 0.36 -1.13 -0.52 -0.33 -0.12  0.42   652\nb_group[4]            1.06    0.01 0.36  0.24  0.86  1.06  1.26  1.80   641\nb_group[5]           -0.28    0.01 0.36 -1.10 -0.48 -0.28 -0.08  0.46   648\nsigma_obs             0.46    0.00 0.02  0.43  0.45  0.46  0.48  0.50  1764\nsigma_grp             0.77    0.01 0.32  0.38  0.54  0.69  0.90  1.61  1230\ngroup_averages[1]     3.71    0.00 0.06  3.58  3.67  3.71  3.75  3.84  4643\ngroup_averages[2]     3.71    0.00 0.07  3.58  3.67  3.71  3.76  3.85  4439\ngroup_averages[3]     3.69    0.00 0.06  3.57  3.65  3.69  3.73  3.81  4485\ngroup_averages[4]     5.07    0.00 0.04  4.99  5.04  5.07  5.10  5.16  4836\ngroup_averages[5]     3.74    0.00 0.05  3.63  3.70  3.74  3.77  3.84  4800\none_obs_per_group[1]  3.70    0.01 0.46  2.79  3.40  3.70  4.00  4.64  3958\none_obs_per_group[2]  3.71    0.01 0.47  2.77  3.40  3.70  4.03  4.59  3963\none_obs_per_group[3]  3.71    0.01 0.46  2.80  3.40  3.70  4.01  4.65  3940\none_obs_per_group[4]  5.07    0.01 0.47  4.15  4.76  5.07  5.38  5.99  4055\none_obs_per_group[5]  3.74    0.01 0.47  2.81  3.43  3.73  4.06  4.65  3707\nnew_b_group          -0.02    0.01 0.85 -1.72 -0.49 -0.01  0.44  1.68  3832\none_obs_new_group     4.00    0.02 1.04  1.94  3.39  4.02  4.61  6.13  2571\nlp__                 88.46    0.07 2.19 83.23 87.18 88.83 90.07 91.63  1094\n                     Rhat\nb_avg                1.01\nb_group[1]           1.01\nb_group[2]           1.01\nb_group[3]           1.00\nb_group[4]           1.01\nb_group[5]           1.01\nsigma_obs            1.00\nsigma_grp            1.00\ngroup_averages[1]    1.00\ngroup_averages[2]    1.00\ngroup_averages[3]    1.00\ngroup_averages[4]    1.00\ngroup_averages[5]    1.00\none_obs_per_group[1] 1.00\none_obs_per_group[2] 1.00\none_obs_per_group[3] 1.00\none_obs_per_group[4] 1.00\none_obs_per_group[5] 1.00\nnew_b_group          1.00\none_obs_new_group    1.00\nlp__                 1.00\n\nSamples were drawn using NUTS(diag_e) at Wed May  7 06:57:20 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples |> \n  tidybayes::gather_rvars(b_group[group_id],\n                          new_b_group) |> \n  mutate(sp_island = group_names[group_id],\n         sp_island = if_else(is.na(sp_island),\n                             true = \"New Group\",\n                             false = sp_island)) |> \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhierarchical_groups_samples |> \n  tidybayes::gather_rvars(one_obs_per_group[group_id],\n                          one_obs_new_group) |> \n  mutate(sp_island = group_names[group_id],\n         sp_island = if_else(is.na(sp_island),\n                             true = \"New Group\",\n                             false = sp_island)) |> \n  ggplot(aes(y = sp_island,\n             dist = .value,\n             colour = sp_island)) + \n  stat_pointinterval() + \n  geom_point(aes(y = sp_island,\n             x = mass_kg,\n             colour = sp_island), \n             inherit.aes = FALSE,\n             alpha = .7,\n             data = penguin_groupid,\n             position = position_jitter(width = 0, height = .2))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n### Exercises\n\n1. Try leaving out a group and refitting the hierarchical model. Are the predictions for the missing group accurate?\n1. There are other categorical predictors in the dataset. Try including `year` as a part of the group-creating factor (i.e. in the call to `unite()` above). What changes?\n1. Modify the `generated quantities` block to simulate a fake observation for EVERY row of the dataset. This opens the possibility of using `bayesplot` to make predictions. Look back at the code from Day 1 and create a posterior predictive check for both models. (e.g. using `ppc_dens_overlay`)\n1. We could perhaps have used `sex` as a grouping factor, but `sex` has missing values in it! Why is this a problem for this kind of model? What would it take to address that? (Discussion only; missing values are unfortunately outside the scope of the class!)\n\n## Observation-level random effects: Mite abundance\n\n### What is the question? \n\nLet's write a model to answer the question:  \n\n**How does the total abundance of the mite community change as water content increases?**  \n\n### Express this in Math\n\nHere's a partially complete model for species richness over time\n\n$$\n\\begin{align}\n\\text{S}_i &\\sim \\text{Poisson}(e^a) \\\\\na &= \\bar\\beta + \\beta_{\\text{water}} \\cdot \\text{water}_i \\\\\n\\bar\\beta &\\sim \\text{Normal}(?, ?) \\\\\n\\beta_{\\text{water}} &\\sim \\text{Normal}(?, ?) \\\\\n\\end{align}\n$$\n\n:::{.callout-tip}\n### EXERCISE \nSimulate from this model, and look at your simulations to decide on a reasonable prior for the data.\n:::\n\n:::{.callout-note collapse=\"true\"}\n### SOLUTION\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 30\nwater <- seq(from = -5, to = 5, length.out = n)\n\nb0 <- rnorm(1, mean = log(17), sd = .3)\nb1 <- rnorm(1, mean = 0, sd = .2)\n\nS <- rpois(n, lambda = exp(b0 + b1*water))\nplot(water, S)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n:::\n\n### Data preparation & visualization\n\nFirst we need to load and prepare the data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(mite, package = \"vegan\")\ndata(\"mite.env\", package = \"vegan\")\n\n# combine data and environment\n\nmite_data_long <- mite |> \n  tibble::rownames_to_column(var = \"site_id\") |> \n  bind_cols(mite.env) |> \n  pivot_longer(Brachy:Trimalc2,\n               names_to = \"spp\", values_to = \"abd\")\n```\n:::\n\n\n\n\nFirst let's transform the mite dataset into a dataframe of total community abundance (N) per site. \nWe'll also standardize the water content while we're at it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmite_community_abd <- mite_data_long |> \n  group_by(site_id, WatrCont) |> \n  summarize(N = sum(abd)) |>\n  ungroup() |> \n  mutate(water_c = (WatrCont - mean(WatrCont))/100)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`summarise()` has grouped output by 'site_id'. You can override using the\n`.groups` argument.\n```\n\n\n:::\n\n```{.r .cell-code}\nknitr::kable(head(mite_community_abd))\n```\n\n::: {.cell-output-display}\n\n\n|site_id | WatrCont|   N|    water_c|\n|:-------|--------:|---:|----------:|\n|1       |   350.15| 140| -0.6048571|\n|10      |   220.73| 166| -1.8990571|\n|11      |   134.13| 216| -2.7650571|\n|12      |   405.91| 213| -0.0472571|\n|13      |   243.70| 177| -1.6693571|\n|14      |   239.51| 269| -1.7112571|\n\n\n:::\n:::\n\n\n\nWe get a nice histogram of community abundance, and a clear negative relationship with water volume:\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nmite_community_abd |> \n  ggplot(aes(x = N)) + \n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmite_community_abd |> \n  ggplot(aes(x = water_c, y = N)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-21-2.png){width=672}\n:::\n:::\n\n\n\n### Write the model in Stan and estimate it\n\nThis model will have a structure very similar to previous ones we've seen. \nThe main difference is the use of the `poisson_log` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson_regression <- stan_model(\n  file = \"topics/03_one_random_effect/poisson_regression.stan\",\n  model_name = \"poisson_regression\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrecompiling to avoid crashing R session\n```\n\n\n:::\n\n```{.r .cell-code}\npoisson_regression\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` stan\nS4 class stanmodel 'poisson_regression' coded as follows:\ndata {\n  int<lower=0> N;\n  vector[N] water;\n  array[N] int y;\n  // for prediction\n  int<lower=0> Npred;\n  vector[Npred] water_pred;\n}\nparameters {\n  real b_avg;\n  real b_water;\n}\nmodel {\n  y ~ poisson_log(b_avg + b_water * water);\n  b_water ~ normal(0, .2);\n  b_avg ~ normal(0, .3);\n}\ngenerated quantities {\n  array[N] int fake_obs;\n  for (i in 1:N){\n    fake_obs[i] = poisson_log_rng(b_avg + b_water * water[i]);\n  }\n\n  // confidence interval for the line\n  vector[Npred] line_avg;\n  line_avg = exp(b_avg + b_water * water_pred);\n\n  // prediction interval for the line\n  array[Npred] int line_obs;\n  for (j in 1:Npred){\n    line_obs[j] = poisson_rng(line_avg[j]);\n  }\n} \n```\n\n\n:::\n:::\n\n\n\n:::{.callout-note}\n### Built in Stan functions\n\nStan has a lot of built-in functions to facilitate modelling. \nFor example the log link function is so common that there is a specialized likelihood function just for that, `poisson_log`. And there's an even more efficient version called `poisson_log_glm`. I didn't use it here because I wanted to code to closely match the equations. You can read about these functions (and much more!) in the amazing [Stan manual](https://mc-stan.org/docs/functions-reference/unbounded_discrete_distributions.html#poisson)\n:::\n\nUsing a now-familiar workflow, we feed the data into the model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwater_for_pred <- seq(from = -3, to = 4.5, length.out = 15)\n\nabd_data_list <- list(N = length(mite_community_abd$N),\n              water = mite_community_abd$water_c,\n              y = mite_community_abd$N,\n              Npred = 15,\n              water_pred = water_for_pred)\n\npoisson_regression_sample <- sampling(\n  poisson_regression,\n  data = abd_data_list,\n  refresh = 0)\n```\n:::\n\n\n\n### Plot the model to see if it fits well\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\npoisson_regression_sample |> \n  tidybayes::gather_rvars(line_obs[i]) |> \n  mutate(water = water_for_pred) |> \n  ggplot(aes(x = water, dist = .value)) + \n  stat_lineribbon() + \n  geom_point(aes(x = water_c, y = N), \n             data = mite_community_abd, \n             inherit.aes = FALSE) + \n  scale_fill_brewer(palette = \"Greens\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfake_obs_S <- rstan::extract(poisson_regression_sample, pars = \"fake_obs\")\n\nbayesplot::ppc_dens_overlay(y = mite_community_abd$N,\n                            yrep = head(fake_obs_S$fake_obs, 50))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-2.png){width=672}\n:::\n:::\n\n\n\nRemember, on the left we are plotting the _*Prediction interval*_ here: it's showing the distribution of probable observations according to the model. Notice that the the model predicts much narrower variation than we really find!   \n\nOn the right hand side we have the posterior predictive check, which once again shows that the model is overconfident and predicts a range of observations that are far too narrow.  \n\n\n:::{.callout-tip\n### EXERCISE\n\n1) Discuss with your neighbours: would you trust this model? Would you publish it? The technical name for this phenomenon is \"overdisperson\". Have you checked for this in previous count models you've done?\n\n2) Add a random effect for _every individual observation_ in the model. Begin by writing the mathematical notation for this new model! \n\n3) fit the model and re-create the two figures above. What do you notice? \n\n* Which model is more trustworthy?\n* look at the slope in the new model. Is it different?\n\n:::\n\n:::{.callout-note collapse=\"true\"}\n### SOLUTION\n\n\n#### Mathematical notation\n\n$$\n\\begin{align}\n\\text{S}_i &\\sim \\text{Poisson}(e^a) \\\\\na &= \\bar\\beta + \\beta_{\\text{water}} \\cdot \\text{water}_i + \\text{site}_i\\\\\n\\bar\\beta &\\sim \\text{Normal}(?, ?) \\\\\n\\beta_{\\text{water}} &\\sim \\text{Normal}(?, ?) \\\\\n\\text{site} &\\sim \\text{Normal}(?, \\sigma) \\\\\n\\sigma &\\sim \\text{Exponential}(?)\n\\end{align}\n$$\n\n#### Stan code \n\nWrite the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson_regression_overdisp <- stan_model(\n  file = \"topics/03_one_random_effect/poisson_regression_overdisp.stan\",\n  model_name = \"poisson_regression_overdisp\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nrecompiling to avoid crashing R session\n```\n\n\n:::\n\n```{.r .cell-code}\npoisson_regression_overdisp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n``` stan\nS4 class stanmodel 'poisson_regression_overdisp' coded as follows:\ndata {\n  int<lower=0> N;\n  vector[N] water;\n  array[N] int y;\n  // for prediction\n  int<lower=0> Npred;\n  vector[Npred] water_pred;\n}\nparameters {\n  real b_avg;\n  real b_water;\n  real<lower=0> sigma_site;\n  vector[N] site_intercepts;\n}\nmodel {\n  b_avg ~ normal(0, .3);\n  b_water ~ normal(0, .2);\n  site_intercepts ~ normal(0, sigma_site);\n  sigma_site ~ exponential(.5);\n  y ~ poisson_log(b_avg + b_water * water + site_intercepts);\n}\ngenerated quantities {\n  array[N] int fake_obs;\n  for (i in 1:N){\n    fake_obs[i] = poisson_log_rng(b_avg + b_water * water[i] + site_intercepts[i]);\n  }\n\n  // confidence interval for the line\n\n  // prediction interval for the line\n  vector[Npred] new_site_intercepts;\n  array[Npred] int line_obs;\n  for (j in 1:Npred){\n    new_site_intercepts[j] = normal_rng(0, sigma_site);\n  }\n\n  vector[Npred] line_avg;\n  line_avg = exp(b_avg + b_water * water_pred + new_site_intercepts);\n\n  for(k in 1:Npred){\n    line_obs[k] = poisson_rng(line_avg[k]);\n  }\n} \n```\n\n\n:::\n:::\n\n\n\nSample it\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson_regression_overdisp_sample <- sampling(\n  poisson_regression_overdisp, \n  data = abd_data_list, refresh = 0\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The largest R-hat is 1.08, indicating chains have not mixed.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#r-hat\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n```\n\n\n:::\n:::\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nfake_obs_overdisp_S <- rstan::extract(poisson_regression_overdisp_sample, pars = \"fake_obs\")\n\nbayesplot::ppc_dens_overlay(y = mite_community_abd$N,\n                            yrep = head(fake_obs_overdisp_S$fake_obs, 50))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\npoisson_regression_overdisp_sample |> \n  tidybayes::gather_rvars(line_obs[i]) |> \n  mutate(water = water_for_pred) |> \n  ggplot(aes(x = water, dist = .value)) + \n  stat_lineribbon() + \n  geom_point(aes(x = water_c, y = N), \n             data = mite_community_abd, \n             inherit.aes = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-2.png){width=672}\n:::\n:::\n\n\n\n:::\n\n:::{.callout-note}\n## Bonus exercises\n\nCompare the slope estimates between the two poisson models above (possibly in the same figure). What happened here?\n\nAnother great way to model overdispersion is via the [Negative Binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) distribution. Look at the Stan documentation for [neg_binomial_2_log](https://mc-stan.org/docs/functions-reference/neg-binom-2-log.html) and adapt your model to use it (don't forget to drop the random effect when you do!).\n\nEXPERIMENTAL: replace the normal distribution for the random observation-level effect with the student_t distribution, to help with that one extreme datapoint! how does the slope change?\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}