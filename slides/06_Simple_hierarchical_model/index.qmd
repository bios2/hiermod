---
title: "'Simple' hierarchical models"
title-slide-attributes: 
  data-background-image: ../img/bg.jpg
  data-background-size: full
author: "Guillaume Blanchet -- Andrew MacDonald"
date: "2025-05-07"
execute:
  echo: true
format: 
  revealjs:
    theme: [default]
    logo: ../img/UdeS_logo_h_rgbHR.png
    transition: slide
    background-transition: fade
---

## Hierarchical models

. . . 

With a hierarchical model, our interest lies not in finding a single value for a parameter of interest but rather to estimate the ***distribution*** of a parameter of interest and, specifically, the [variance]{style="color: blue"} of this distribution. 

. . . 

If we want to estimate the variance of a distribution, we need to gather some samples from this distribution. 

. . . 

In the context of a hierarchical model, the "samples" are the levels of a factor, which will be used to estimate the variance of the distribution. 


## "Simple" hierarchical model

. . . 

Here, we use the term "simple" in a rather loose way to discuss hierarchical models where the hierarchy is on **one** parameter without any constrains, whether they are spatial, temporal, phylogenetic or others. 

. . . 

Futhermore, for most of this lecture, we will focus on models with a Gaussian error term to develop the underlying theory. 

. . . 

When we will have done this, it will be reasonably straight forward to move to non-Gaussian hierarchical model.


## The "`|`"

. . . 

::: {style="font-size: 0.8em"}
Most of you have probably already used the packages `lme4`, `brms` or `glmmTMB` to build hierarchical models and so you have used the `|` to include a hierarchy in your model.
:::

. . . 

::: {style="font-size: 0.8em"}
But do you know what the underlying mathematical structure of the model you built look like ? Does it really answer the question you were asking ?
:::

![](https://www.i2symbol.com/pictures/emojis/2/1/4/4/21445641aeaca6c4436579b3fa30772b_384.png){fig-align="center" width=20%}

. . .

::: {style="font-size: 0.8em"}
Let's look at different `lme4` models to learn about some basic (and not so basic!) hierarchical models.
:::

## A bit of notation

::: {style="font-size: 0.8em"}
Mathematically, the basic structure of a hierarchical model is
::: 

::: {style="font-size: 0.8em"}
$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\mathbf{b} + \boldsymbol{\varepsilon}$$
::: 
::: {style="font-size: 0.8em"}
where
::: 

::: {style="font-size: 0.8em"}
- $\mathbf{y}$ : Vector of response variable
- $\mathbf{X}$ : Matrix of explanatory variables on which **no** hierarchies are accounted for
- $\mathbf{Z}$ : Matrix of explanatory variables on which hierarchies are accounted for
- $\boldsymbol{\beta}$ : parameter estimated without a hierarchy
- $\mathbf{b}$ : parameter estimated with a hierarchy
- $\boldsymbol{\varepsilon}$ : a vector that follows a Gaussian distribution such that ${\cal N}(0, \sigma^2)$
::: 

## A bit of notation

. . .

::: {style="font-size: 0.9em"}
Before we get into writing math, we need to define a bit of notation in addition of the one we have used so far.
:::

. . .

::: {style="font-size: 0.9em"}
Specifically, when defining a hierarchy in a model, it is common to do this using at least one factor. Mathematically, we will define the different level of a factor in a model by a subscript. 

We will use square brackets to define the sample.
:::

. . .

**Example**

::: {style="font-size: 0.9em"}
$$\mathbf{Z}_{f[i]}$$
This means that, within $\mathbf{Z}$, we focus on the $i^{\text{th}}$ sample within factor $f$. 
:::

. . .

::: {style="font-size: 0.9em"}
**Note** The $i^{\text{th}}$ sample of factor $f$ maybe associated to any level of factor $f$.
:::

## Hierarchy on the intercept

. . .

::: {style="font-size: 0.9em"}
`lme4` notation used `y ~ (1 | f)` or `y ~ 1 + (1 | f)`
:::

. . .

::: {style="font-size: 0.9em"}
This model assumes there is a hierarchy solely on the intercept.
:::

. . .

::: {style="font-size: 0.9em"}
Mathematically, it can be translated to 

$$\mathbf{y} \sim \mathcal{MVN}(\mathbf{b}_{f},\sigma^2\mathbf{I})$$
:::

. . .

::: {style="font-size: 0.9em"}
or 

$$y_i = b_{{f[i]}} + \varepsilon \quad \forall\quad i = 1\dots n$$
:::

. . .

::: {style="font-size: 0.9em"}
where

$$\mathbf{b}_f \sim \mathcal{N}(0, \sigma^2_f)$$
:::

## Hierarchy on the intercept

```{r, echo=FALSE, fig.width=8,  fig.height=8, fig.align='center'}
zones=matrix(c(1,2), ncol=2, byrow=TRUE)
layout(zones, widths=c(1/5,4/5), heights=c(1/5,4/5))

val <- seq(-1, 3, length = 200)
marginal <- dnorm(val, mean = 1, sd = 0.5)

par(mar = c(0.1, 0.1, 0.1, 0.1))
plot(-marginal,val,
     type = "n",
     ylim = c(-3,3),
     axes = FALSE,
     xlab = "",
     ylab = "") 

polygon(x = c(-marginal,-marginal[1]),
        y = c(val, val[1]),
        col = rgb(0,0,1, 0.5),border = NA)

lines(-marginal,val, lwd = 3)

plot(0,0,
     type = "n",
     xlim = c(0,5),
     ylim = c(-3,3),
     bty="L",
     xaxt = "n",
     yaxt = "n",
     xlab = "",
     ylab = "") 

set.seed(42)
abline(h = rnorm(30, mean = 1, sd = 0.5),
       lwd = 3,
       col = "blue")
```

## Hierarchy on the slope

::: {style="font-size: 0.8em"}
`lme4` notation : `y ~ 1 + (x | f)`
:::

. . .

::: {style="font-size: 0.8em"}
This model assumes there is a hierarchy on the parameter associated to variable `x`.
:::

. . .

::: {style="font-size: 0.8em"}
Mathematically, it can be translated to 

$$\mathbf{y} \sim \mathcal{MVN}(\beta_0 + \mathbf{z}\mathbf{b}_{f},\sigma^2\mathbf{I})$$
:::

. . .

::: {style="font-size: 0.8em"}
or 

$$y_i = \beta_0 + b_{f[i]}z_i + \varepsilon \quad\forall\quad i = 1\dots n$$
:::

. . .

::: {style="font-size: 0.8em"}
where 

$\mathbf{z}$ is an explanatory variable, $z_i$ the $i^{\text{th}}$ sample of $\mathbf{z}$ and 

$$\mathbf{b}_f \sim \mathcal{N}(0, \sigma^2_f)$$
:::

## Hierarchy on the slopes

```{r, echo=FALSE, fig.width=8,  fig.height=8, fig.align='center'}
plot(0,0,
     type = "n",
     xlim = c(0,5),
     ylim = c(0,5),
     bty="L",
     xaxt = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     xaxs = "i",
     yaxs = "i") 

set.seed(42)
for(i in 1:30){
abline(a = 0, b = rnorm(1, mean = 1, sd = 0.2),
       lwd = 3,
       col = "blue")
}

marginalRot <- spdep::Rotation(cbind(-marginal,val), angle = 3.9)

par(xpd = TRUE)

polygon(x = c(marginalRot[,1] + 1.5,marginalRot[1,1] + 1.5),
        y = c(marginalRot[,2] + 2.5, marginalRot[1,2] + 2.5),
        col = rgb(0,0,1, 0.5),border = NA)

polygon(x = c(marginalRot[,1] + 2.5,marginalRot[1,1] + 2.5),
        y = c(marginalRot[,2] + 3.5, marginalRot[1,2] + 3.5),
        col = rgb(0,0,1, 0.5),border = NA)

polygon(x = c(marginalRot[,1] + 3.5,marginalRot[1,1] + 3.5),
        y = c(marginalRot[,2] + 4.5, marginalRot[1,2] + 4.5),
        col = rgb(0,0,1, 0.5),border = NA)

lines(marginalRot[,1]+1.5, marginalRot[,2] + 2.5, lwd = 3)
lines(marginalRot[,1]+2.5, marginalRot[,2] + 3.5, lwd = 3)
lines(marginalRot[,1]+3.5, marginalRot[,2] + 4.5, lwd = 3)
par(xpd = FALSE)
```

## Hierarchy on intercept and slope

. . .

Mathematically speaking, what are the differences between having a hierarchy on the intercept and a hierarchy on the slope ? Any idea ?

![](https://www.i2symbol.com/pictures/emojis/4/c/e/b/4ceb092d154efb14f6913cb5e332f6da_384.png){fig-align="center" width=50%}

## Hierarchy on intercept and slope

**Answer : Very little !**

. . .

Actually, if we return to the way $\mathbf{b}$ is defined we see that in both case it is defined as 

$$\mathbf{b}_f \sim \mathcal{N}(0, \sigma^2_f)$$
with the sole difference is that $\mathbf{b}$ is linked to an explanatory variable when the hierarchy is on the slope, while when the hierarchy is on the intercept it is not linked to any explanatory variable.

. . .

Well... Actually... When a hierarchy is applied on the intercept it is technically associated to a **constant** explanatory variable.

## How many levels ?

. . .

A common question that often gets asked is : 

. . .

"How many level is enough ?"

. . .

This is a simple questions that sadly does not have a simple answer.

![](https://www.i2symbol.com/pictures/emojis/7/e/1/8/7e1820e72993db112424b5a92e1d40d7_384.png){fig-align="center" width=35%}

## How many levels ?

In these types of models we are interested in estimating the variance parameter $\sigma^2_f$ in 

$$\mathbf{b}_f \sim \mathcal{N}(0, \sigma^2_f)$$
to get the best estimation of $\mathbf{b}$. 

. . .

So, another way to ask this question is: "What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?" 

. . .

However, in the context of how we defined hierarchical models, a sample is a single level of a factor.

## How many levels ?

What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?

. . .

### Is 3 enough ?
:::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, fig.width=8,  fig.height=7, fig.align='center'}
val <- seq(1, 5, length = 200)
marginal <- dnorm(val, mean = 3, sd = 0.5)

par(mar = c(3, 0.1, 0.1, 0.1))
plot(val, marginal,
     type = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     frame.plot = FALSE,
     cex.axis = 2) 


set.seed(42)
sampl <- rnorm(3, mean = 3, sd = 0.5)
abline(v = sampl,
       lwd = 3,
       col = "blue")

polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0,0,1, 0.5),border = NA)

lines(val,marginal, lwd = 8)
```
:::
::: {.column width="50%"}
True variance : 0.25

Estimated variance : `r round(var(sampl), 3)`
:::
::::


## How many levels ?

What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?

### Maybe 5 ?
:::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, fig.width=8,  fig.height=7, fig.align='center'}
val <- seq(1, 5, length = 200)
marginal <- dnorm(val, mean = 3, sd = 0.5)

par(mar = c(3, 0.1, 0.1, 0.1))
plot(val, marginal,
     type = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     frame.plot = FALSE,
     cex.axis = 2) 

set.seed(42)
sampl <- rnorm(5, mean = 3, sd = 0.5)
abline(v = sampl,
       lwd = 3,
       col = "blue")

polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0,0,1, 0.5),border = NA)

lines(val,marginal, lwd = 8)
```
:::
::: {.column width="50%"}
True variance : 0.25

Estimated variance : `r round(var(sampl), 3)`
:::
::::

## How many levels ?

What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?

### Or 10 ?
:::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, fig.width=8,  fig.height=7, fig.align='center'}
val <- seq(1, 5, length = 200)
marginal <- dnorm(val, mean = 3, sd = 0.5)

par(mar = c(3, 0.1, 0.1, 0.1))
plot(val, marginal,
     type = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     frame.plot = FALSE,
     cex.axis = 2) 

set.seed(42)
sampl <- rnorm(10, mean = 3, sd = 0.5)
abline(v = sampl,
       lwd = 3,
       col = "blue")

polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0,0,1, 0.5),border = NA)

lines(val,marginal, lwd = 8)
```
:::
::: {.column width="50%"}
True variance : 0.25

Estimated variance : `r round(var(sampl), 3)`
:::
::::


## How many levels ?

What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?

### Or 50 ?
:::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, fig.width=8,  fig.height=7, fig.align='center'}
val <- seq(1, 5, length = 200)
marginal <- dnorm(val, mean = 3, sd = 0.5)

par(mar = c(3, 0.1, 0.1, 0.1))
plot(val, marginal,
     type = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     frame.plot = FALSE,
     cex.axis = 2) 

set.seed(42)
sampl <- rnorm(50, mean = 3, sd = 0.5)
abline(v = sampl,
       lwd = 3,
       col = "blue")

polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0,0,1, 0.5),border = NA)

lines(val,marginal, lwd = 8)
```
:::
::: {.column width="50%"}
True variance : 0.25

Estimated variance : `r round(var(sampl), 3)`
:::
::::


## How many levels ?

What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?

### Or 100 ?
:::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, fig.width=8,  fig.height=7, fig.align='center'}
val <- seq(1, 5, length = 200)
marginal <- dnorm(val, mean = 3, sd = 0.5)

par(mar = c(3, 0.1, 0.1, 0.1))
plot(val, marginal,
     type = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     frame.plot = FALSE,
     cex.axis = 2) 

set.seed(42)
sampl <- rnorm(100, mean = 3, sd = 0.5)
abline(v = sampl,
       lwd = 3,
       col = "blue")

polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0,0,1, 0.5),border = NA)

lines(val,marginal, lwd = 8)
```
:::
::: {.column width="50%"}
True variance : 0.25

Estimated variance : `r round(var(sampl), 3)`
:::
::::


## How many levels ?

What is the minimum number of samples needed to properly estimate the variance of a Gaussian distribution?

### Or 1000 ?
:::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, fig.width=8,  fig.height=7, fig.align='center'}
val <- seq(1, 5, length = 200)
marginal <- dnorm(val, mean = 3, sd = 0.5)

par(mar = c(3, 0.1, 0.1, 0.1))
plot(val, marginal,
     type = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     frame.plot = FALSE,
     cex.axis = 2) 

set.seed(42)
sampl <- rnorm(1000, mean = 3, sd = 0.5)
abline(v = sampl,
       lwd = 3,
       col = "blue")

polygon(x = c(val, val[1]),
        y = c(marginal,marginal[1]),
        col = rgb(0,0,1, 0.5),border = NA)

lines(val,marginal, lwd = 8)
```
:::
::: {.column width="50%"}
True variance : 0.25

Estimated variance : `r round(var(sampl), 3)`
:::
::::

## How many levels ?

There is a consensus among researchers working intimately with hierarchical models that when the interest is to properly estimate the variance parameter $\sigma^2$, 5 or 6 levels is the extreme minimum.

. . .

::: {style="font-size: 0.75em"}
In the book *Richly Parameterized Linear Models: Additive, Time Series, and Spatial Models Using Random Effects*, James S. Hodges (2016) makes this very thoughtful statement : 

:::: {style="color: blue"}
"Treating factors with small numbers of levels as random will in the best case lead to very small and/or imprecise estimates of random effects; in the worst case it will lead to various numerical difficulties such as lack of convergence, zero variance estimates, etc."
::::
:::

## How many levels ?

So, what to do if the number of level is not high enough for your comfort ?

. . .

You can still use the hierarchy in your model but focus on the mean of the levels instead of the variance.

. . .

How does this translate mathematically with what we have seen so far ?

![](https://www.i2symbol.com/pictures/emojis/4/c/e/b/4ceb092d154efb14f6913cb5e332f6da_384.png){fig-align="center" width=30%}

## Hierarchy on the intercept's mean

::: {style="font-size: 0.85em"}
$$\mathbf{y} \sim \mathcal{MVN}(\boldsymbol{\beta}_{f},\sigma_\mathbf{y}^2\mathbf{I})$$
or 

$$y_i = \beta_{f[i]} + \varepsilon \quad \forall\quad i = 1\dots n$$
:::

. . .

::: {style="font-size: 0.85em"}
In this model, we assume that $\boldsymbol{\beta}_{f}$ is distributed as 

$$\boldsymbol{\beta}_{f} \sim \mathcal{N}(\mu_{f}, \sigma^2_{f})$$
:::

. . .

::: {style="font-size: 0.85em"}
This means that all samples **among** the levels of factor $f$ are used to estimate $\boldsymbol{\beta}_{f}$. 
:::

. . .

::: {style="font-size: 0.85em"}
By developping our model this way, we focus on estimating the mean of groups in the hierarchy instead of only the variance. 
:::

## Hierarchy on the intercept's mean

Let's take a deeper look at 
$$\boldsymbol{\beta}_{f} \sim \mathcal{N}(\mu_{f}, \sigma^2_{f})$$

. . .

When we study this way of sampling $\boldsymbol{\beta}_{f}$, although our interest is more on $\mu_{f}$, we also have to acount for the variance term $\sigma^2_{f}$.

. . .

**Note**: This is essentially the same thing as a one-way analysis of variance.

## Hierarchy on the intercept's mean

```{r, echo=FALSE, fig.width=8,  fig.height=8, fig.align='center'}
zones=matrix(c(1,2), ncol=2, byrow=TRUE)
layout(zones, widths=c(1/5,4/5), heights=c(1/5,4/5))

val <- seq(-1, 3, length = 200)
marginal <- dnorm(val, mean = 1, sd = 0.5)

val1 <- seq(-2, 2, length = 200)
marginal1 <- dnorm(val1, mean = 0, sd = 0.25)

val2 <- seq(-3, 3, length = 200)
marginal2 <- dnorm(val2, mean = -1, sd = 0.75)

par(mar = c(0.1, 0.1, 0.1, 0.1))
plot(-marginal1,val1,
     type = "n",
     ylim = c(-3,3),
     axes = FALSE,
     xlab = "",
     ylab = "") 

polygon(x = c(-marginal,-marginal[1]),
        y = c(val, val[1]),
        col = rgb(0,0,1, 0.5),border = NA)

polygon(x = c(-marginal1,-marginal1[1]),
        y = c(val1, val1[1]) ,
        col = rgb(0,1,0, 0.5),border = NA)

polygon(x = c(-marginal2,-marginal2[1]),
        y = c(val2, val2[1]) ,
        col = rgb(1,0.65,0, 0.5),border = NA)

lines(-marginal,val, lwd = 3)
lines(-marginal1,val1, lwd = 3)
lines(-marginal2,val2, lwd = 3)

plot(0,0,
     type = "n",
     xlim = c(0,5),
     ylim = c(-3,3),
     bty="L",
     xaxt = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",) 

set.seed(42)

points(runif(200, 0, 5),
       rnorm(200, mean = 1, sd = 0.5),
       pch = 19, col = "blue") 

points(runif(200, 0, 5),
       rnorm(200, mean = 0, sd = 0.25),
       pch = 19, col = "green") 

points(runif(200, 0, 5),
       rnorm(200, mean = -1, sd = 0.75),
       pch = 19, col = "orange") 

abline(h = c(1, 0, -1),
       lwd = 8,
       col = c("blue", "darkgreen", "orange"))
```

## Hierarchy on the slope's mean

. . .

::: {style="font-size: 0.75em"}
Developping a hierarchy on the slope's mean translate mathematically in a very similar way as it does for the intercept. 
:::

. . .

::: {style="font-size: 0.75em"}
$$\mathbf{y} \sim \mathcal{MVN}(\boldsymbol{\beta}_0+\mathbf{X}\boldsymbol{\beta}_{f},\sigma_\mathbf{y}^2\mathbf{I})$$
or 

$$y_i = \beta_0 + \beta_{f[i]}x_i + \varepsilon \quad \forall\quad i = 1\dots n$$
:::

. . .

::: {style="font-size: 0.75em"}
In this model, we assume that $\boldsymbol{\beta}_{f}$ is distributed as 

$$\boldsymbol{\beta}_{f} \sim \mathcal{N}(\mu_{f}, \sigma^2_{f})$$
:::

. . .

::: {style="font-size: 0.75em"}
This means that all the samples **among** the levels of factor $f$ are used to estimate $\boldsymbol{\beta}_{f}$

By developping our model this way, we focus on estimating the average slope for each group in the hierarchy instead of only the variance. 
:::

## Hierarchy on the slope's mean

```{r, echo=FALSE, fig.width=8,  fig.height=8, fig.align='center'}

val <- seq(-1, 3, length = 200)
marginal <- dnorm(val, mean = 1, sd = 0.5)

val1 <- seq(-2, 2, length = 200)
marginal1 <- dnorm(val1, mean = 0.5, sd = 0.25)

val2 <- seq(-3, 3, length = 200)
marginal2 <- dnorm(val2, mean = 0.25, sd = 0.75)

plot(0,0,
     type = "n",
     xlim = c(0,5),
     ylim = c(0,5),
     bty="L",
     xaxt = "n",
     yaxt = "n",
     xlab = "",
     ylab = "",
     xaxs = "i",
     yaxs = "i") 

x <- runif(200, 0, 5)
points(x,
       rnorm(200, mean = x, sd = 0.5),
       pch = 19, col = "blue") 

points(x,
       rnorm(200, mean = 1.25 * x, sd = 0.25),
       pch = 19, col = "green") 

points(x,
       rnorm(200, mean = 0.5 * x, sd = 0.75),
       pch = 19, col = "orange") 

abline(a = 0, b = 1,
       lwd = 8,
       col = "blue")

abline(a = 0, b = 0.5,
       lwd = 8,
       col = "orange")

abline(a = 0, b = 1.25,
       lwd = 8,
       col = "darkgreen")
```

## Tracking the estimated parameters

. . .

::: {style="font-size: 0.8em;"}
As can be seen, it is important in hierarchical models to track the different parameters that are estimated to ensure we can make proper inferences with our model.
:::

. . .

::: {style="font-size: 0.8em;"}
However, we need to be careful because the notation used can play tricks on us. This is especially true when using matrix notation. 
:::

. . .

::: {style="font-size: 0.8em;"}
For example, in 

$$\mathbf{b}_{f}\sim \mathcal{MVN}(\mathbf{0}, \mathbf{\Sigma})$$
the number of levels are not explicitly defined and it is not clear if $\mathbf{\Sigma}$ includes the same variance value on the diagonal or different ones and whether the off diagonal elements are 0 or not. 
:::

. . .

::: {style="font-size: 0.8em;"}
In any case, make sure to keep track of the estimated parameters so that you can better understand the limits of the model you are building and using. 
:::

## Choosing the right model

. . . 

Although these different models are mathematically quite similar, they approach very different biological questions. 

. . . 

A comparison of the different figures caricaturizing how each model works should give a good insight of what each model can do.

. . . 

It is thus important to make sure you design your biological question well so that deciding on which model to use is reasonably straight forward.

